from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer
from fastapi.responses import FileResponse, Response
from fastapi.staticfiles import StaticFiles
import uvicorn
from datetime import datetime, date, timedelta
from typing import List, Optional, Dict, Any
import pandas as pd
import xlsxwriter
import json
import os
import time
import logging
from collections import defaultdict
import re
import html
import threading
import copy
from contextlib import contextmanager
import pickle
from pydantic import BaseModel, Field, EmailStr, validator
from enum import Enum
from pathlib import Path
from dotenv import load_dotenv
import secrets
import sqlite3

from passlib.context import CryptContext

# Optional imports for system monitoring
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("‚ö†Ô∏è psutil not available - system monitoring features limited")

# Load environment variables from .env file
load_dotenv()


# ÔøΩ KATEGORI 8 - CONFIGURATION MANAGEMENT ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞
class Environment(str, Enum):
    """Environment types"""

    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    TESTING = "testing"


class DatabaseConfig(BaseModel):
    """Database configuration model"""

    host: str = Field(default="localhost")
    port: int = Field(default=5432)
    username: str = Field(default="postgres")
    password: str = Field(default="")
    database: str = Field(default="personel_takip")
    pool_size: int = Field(default=5, ge=1, le=50)
    max_overflow: int = Field(default=10, ge=0, le=100)

    @property
    def url(self) -> str:
        if self.password:
            return f"postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}"
        return f"postgresql://{self.username}@{self.host}:{self.port}/{self.database}"


class SecurityConfig(BaseModel):
    """Security configuration model"""

    secret_key: str = Field(default="your-secret-key-here")
    algorithm: str = Field(default="HS256")
    access_token_expire_minutes: int = Field(default=30, ge=1, le=10080)  # Max 1 week
    refresh_token_expire_days: int = Field(default=7, ge=1, le=30)  # Max 1 month
    password_min_length: int = Field(default=8, ge=6, le=50)
    max_login_attempts: int = Field(default=5, ge=1, le=20)
    lockout_duration_minutes: int = Field(default=15, ge=1, le=1440)


class ServerConfig(BaseModel):
    """Server configuration model"""

    host: str = Field(default="127.0.0.1")
    port: int = Field(default=8000, ge=1, le=65535)
    workers: int = Field(default=1, ge=1, le=10)
    reload: bool = Field(default=False)
    log_level: str = Field(default="info")
    timeout_keep_alive: int = Field(default=5, ge=1, le=300)
    max_request_size: int = Field(default=16777216)  # 16MB


class ApplicationSettings(BaseModel):
    """Main application settings with environment variable support"""

    # Environment
    environment: Environment = Field(default=Environment.DEVELOPMENT)
    debug: bool = Field(default=True)

    # Application
    app_name: str = Field(default="Personel Takip API")
    app_version: str = Field(default="1.0.0")
    description: str = Field(default="Personel ve g√ºnl√ºk kayƒ±t takip sistemi")

    # Database
    database: DatabaseConfig = Field(default_factory=DatabaseConfig)

    # Security
    security: SecurityConfig = Field(default_factory=SecurityConfig)

    # Server
    server: ServerConfig = Field(default_factory=ServerConfig)

    # Rate Limiting
    rate_limit_requests: int = Field(default=100, ge=1, le=10000)
    rate_limit_window_minutes: int = Field(default=1, ge=1, le=60)

    # Export Settings
    export_path: str = Field(default="./exports")
    max_export_records: int = Field(default=10000, ge=1, le=100000)

    # Logging
    log_file: str = Field(default="personel_takip.log")
    log_level: str = Field(default="INFO")
    log_rotation_days: int = Field(default=7, ge=1, le=365)
    log_max_size_mb: int = Field(default=50, ge=1, le=1000)

    # Performance
    cache_ttl_minutes: int = Field(default=5, ge=1, le=1440)
    background_task_timeout: int = Field(default=300, ge=30, le=3600)

    # Monitoring
    enable_metrics: bool = Field(default=True)
    metrics_endpoint_enabled: bool = Field(default=True)


class ConfigManager:
    """Centralized configuration management"""

    def __init__(self):
        self.settings = ApplicationSettings()
        self._config_file = Path("config.json")
        self._load_config_file()
        # Apply environment overrides after loading config file
        self._apply_env_overrides()

    def _load_config_file(self):
        """Load configuration from JSON file if exists"""
        if self._config_file.exists():
            try:
                with open(self._config_file, "r", encoding="utf-8") as f:
                    config_data = json.load(f)
                    # Override settings with file data
                    for key, value in config_data.items():
                        if hasattr(self.settings, key):
                            setattr(self.settings, key, value)
                logger.info(f"Configuration loaded from {self._config_file}")
            except Exception as e:
                logger.warning(f"Failed to load config file: {e}")

    def _apply_env_overrides(self):
        """Apply overrides from environment variables (.env already loaded)
        Supported format: PERSONEL_TAKIP_<SECTION>__<FIELD>=value (double underscore for nesting)
        Example: PERSONEL_TAKIP_SERVER__PORT=8002
        """
        prefix = "PERSONEL_TAKIP_"
        for key, raw_val in os.environ.items():
            if not key.startswith(prefix):
                continue
            path = key[len(prefix) :]
            parts = path.split("__")
            # Map to settings attributes
            try:
                target = self.settings
                for i, part in enumerate(parts):
                    attr = part.lower()
                    if i == len(parts) - 1:
                        # Last part: set value with type coercion based on existing attribute
                        if not hasattr(target, attr):
                            # Unknown field; skip quietly
                            break
                        current_val = getattr(target, attr)
                        coerced = self._coerce_type(raw_val, type(current_val))
                        setattr(target, attr, coerced)
                    else:
                        if not hasattr(target, attr):
                            # Unknown section; skip
                            break
                        target = getattr(target, attr)
            except Exception as e:
                logger.debug(f"Env override skipped for {key}: {e}")

    @staticmethod
    def _coerce_type(value: str, target_type):
        """Coerce string env values into the existing field type."""
        try:
            if target_type is bool:
                return str(value).strip().lower() in ("1", "true", "yes", "on")
            if target_type is int:
                return int(value)
            if target_type is float:
                return float(value)
            if isinstance(target_type, Environment) or target_type == Environment:
                # environment enum
                v = str(value).strip().lower()
                return (
                    Environment(v)
                    if v in [e.value for e in Environment]
                    else Environment.DEVELOPMENT
                )
            # Fallback to string
            return value
        except Exception:
            return value

    def save_config(self) -> bool:
        """Save current configuration to file"""
        try:
            config_data = self.settings.dict()
            with open(self._config_file, "w", encoding="utf-8") as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"Configuration saved to {self._config_file}")
            return True
        except Exception as e:
            logger.error(f"Failed to save config file: {e}")
            return False

    def get_database_url(self) -> str:
        """Get database connection URL"""
        return self.settings.database.url

    def get_cors_origins(self) -> List[str]:
        """Get CORS allowed origins based on environment"""
        if self.settings.environment == Environment.PRODUCTION:
            return ["https://your-production-domain.com", "https://api.your-domain.com"]
        elif self.settings.environment == Environment.STAGING:
            return ["https://staging.your-domain.com", "https://test.your-domain.com"]
        else:
            return [
                "http://localhost:3000",
                "http://localhost:5173",
                "http://localhost:8080",
                "http://localhost:8001",
                "http://127.0.0.1:3000",
                "http://127.0.0.1:5173",
                "http://127.0.0.1:8080",
                "http://127.0.0.1:8001",
                # Allow file:// pages (Origin: null) in local dev
                "null",
            ]

    def is_production(self) -> bool:
        """Check if running in production environment"""
        return self.settings.environment == Environment.PRODUCTION

    def is_debug_mode(self) -> bool:
        """Check if debug mode is enabled"""
        return self.settings.debug and not self.is_production()

    def get_export_settings(self) -> Dict[str, Any]:
        """Get export-related settings"""
        return {
            "path": self.settings.export_path,
            "max_records": self.settings.max_export_records,
        }

    def validate_config(self) -> List[str]:
        """Validate configuration and return any issues"""
        issues = []

        # Check required settings for production
        if self.is_production():
            if self.settings.security.secret_key == "your-secret-key-here":
                issues.append("Production requires a secure secret key")

            if self.settings.debug:
                issues.append("Debug mode should be disabled in production")

            if not self.settings.database.password:
                issues.append("Production database should have a password")

        # Check export path
        try:
            export_path = Path(self.settings.export_path)
            export_path.mkdir(exist_ok=True)
        except Exception as e:
            issues.append(f"Export path is not accessible: {e}")

        return issues

    def get_config_summary(self) -> Dict[str, Any]:
        """Get configuration summary for monitoring"""
        return {
            "environment": self.settings.environment,
            "debug": self.settings.debug,
            "app_version": self.settings.app_version,
            "database_host": self.settings.database.host,
            "server_host": self.settings.server.host,
            "server_port": self.settings.server.port,
            "rate_limit": f"{self.settings.rate_limit_requests}/{self.settings.rate_limit_window_minutes}min",
            "cache_ttl": f"{self.settings.cache_ttl_minutes}min",
            "log_level": self.settings.log_level,
            "metrics_enabled": self.settings.enable_metrics,
        }

    def get_environment(self) -> str:
        """Get current environment as string"""
        return self.settings.environment.value


# Global configuration manager instance
config_manager = ConfigManager()
settings = config_manager.settings


# Update CORS with dynamic origins
def get_cors_origins():
    return config_manager.get_cors_origins()


# Environment-specific logging setup
def setup_logging():
    """Setup logging based on configuration"""
    log_level = getattr(logging, settings.log_level.upper(), logging.INFO)

    # Clear existing handlers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Setup new handlers
    handlers = []

    # File handler with rotation
    if settings.log_file:
        from logging.handlers import RotatingFileHandler

        file_handler = RotatingFileHandler(
            settings.log_file,
            maxBytes=settings.log_max_size_mb * 1024 * 1024,
            backupCount=settings.log_rotation_days,
        )
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        handlers.append(file_handler)

    # Console handler (only in development)
    if not config_manager.is_production():
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        handlers.append(console_handler)

    logging.basicConfig(level=log_level, handlers=handlers, force=True)


# Initialize logging with configuration
setup_logging()

# Logger instance'ƒ±
logger = logging.getLogger("PersonelTakipAPI")

# Request/Response monitoring i√ßin
request_stats = defaultdict(list)  # {endpoint: [response_time, response_time, ...]}


class StructuredLogger:
    """JSON formatƒ±nda structured logging"""

    @staticmethod
    def log_request(
        method: str, endpoint: str, user_id: str = None, params: dict = None
    ):
        """API request'i logla"""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "type": "request",
            "method": method,
            "endpoint": endpoint,
            "user_id": user_id,
            "params": params or {},
        }
        logger.info(f"REQUEST: {json.dumps(log_data, ensure_ascii=False)}")

    @staticmethod
    def log_response(
        endpoint: str, status_code: int, response_time: float, user_id: str = None
    ):
        """API response'u logla"""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "type": "response",
            "endpoint": endpoint,
            "status_code": status_code,
            "response_time_ms": round(response_time * 1000, 2),
            "user_id": user_id,
        }
        logger.info(f"RESPONSE: {json.dumps(log_data, ensure_ascii=False)}")

        # Performance monitoring i√ßin stats'e ekle
        request_stats[endpoint].append(response_time)

        # Son 100 request'i tut (memory y√∂netimi)
        if len(request_stats[endpoint]) > 100:
            request_stats[endpoint] = request_stats[endpoint][-100:]

    @staticmethod
    def log_error(
        error_type: str,
        message: str,
        endpoint: str = None,
        user_id: str = None,
        stack_trace: str = None,
    ):
        """Hata logla"""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "type": "error",
            "error_type": error_type,
            "message": message,
            "endpoint": endpoint,
            "user_id": user_id,
            "stack_trace": stack_trace,
        }
        logger.error(f"ERROR: {json.dumps(log_data, ensure_ascii=False)}")

    @staticmethod
    def log_business_event(
        event_type: str, description: str, data: dict = None, user_id: str = None
    ):
        """Business event'i logla (personel ekleme, silme vs.)"""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "type": "business_event",
            "event_type": event_type,
            "description": description,
            "data": data or {},
            "user_id": user_id,
        }
        logger.info(f"BUSINESS_EVENT: {json.dumps(log_data, ensure_ascii=False)}")


# Structured logger instance
slogger = StructuredLogger()

# Global documentation manager - middleware'den √∂nce tanƒ±mlanmalƒ±
doc_manager = None  # Initialize after DocumentationManager class definition

# Ge√ßici veri - Database yerine (sonra SQLAlchemy ile deƒüi≈ütireceƒüiz)
# VERƒ∞ Lƒ∞STELERƒ∞ - Global tanƒ±mlamalar (DataSyncManager'dan √∂nce olmalƒ±)
targets_data = [
    {
        "id": 1,
        "personnel_id": 1,
        "target_type": "uye_adedi",
        "target_value": 70,
        "start_date": "2025-08-01",
        "end_date": "2025-08-31",
    },
    {
        "id": 2,
        "personnel_id": 2,
        "target_type": "uye_adedi",
        "target_value": 50,
        "start_date": "2025-08-01",
        "end_date": "2025-08-15",
    },
]

personnel_data = [
    {
        "id": 1,
        "name": "Hakan",
        "username": "hakan.user",
        "email": "hakan@company.com",
        "reference": "Ahmet Demir",
        "hire_date": "2024-01-15",
        "team": "As Ekip",
        "promotion_date": "2024-06-01",
        "department": "IT",
        "position": "Developer",
        "phone": "555-0001",
        "daily_targets": {
            "uye_adedi": 150,
            "whatsapp_adedi": 80,
            "cihaz_adedi": 50,
            "whatsapp_cevapsiz": 10,
        },
        "status": "active",
    },
    {
        "id": 2,
        "name": "ORHAN",
        "username": "orhan.user",
        "email": "orhan@company.com",
        "reference": "Mehmet Yƒ±lmaz",
        "hire_date": "2023-06-10",
        "team": "Paf Ekip",
        "promotion_date": "",
        "department": "Sales",
        "position": "Sales Manager",
        "phone": "555-0002",
        "daily_targets": {
            "uye_adedi": 120,
            "whatsapp_adedi": 60,
            "cihaz_adedi": 40,
            "whatsapp_cevapsiz": 8,
        },
        "status": "active",
    },
]

daily_records_data = [
    {
        "id": 1,
        "date": "2025-08-01",
        "personnel_id": 1,
        "call_number": "CALL-2025-001",
        "score": 85,
        "notes": "M√º≈üteri ile iyi ileti≈üim kurdu, √ß√∂z√ºm odaklƒ± yakla≈üƒ±m sergiledi.",
    },
    {
        "id": 2,
        "date": "2025-08-02",
        "personnel_id": 1,
        "call_number": "CALL-2025-002",
        "score": 75,
        "notes": "Teknik konularda biraz eksik, ancak sabƒ±rlƒ± yakla≈üƒ±m.",
    },
    {
        "id": 3,
        "date": "2025-08-01",
        "personnel_id": 2,
        "call_number": "CALL-2025-003",
        "score": 90,
        "notes": "M√ºkemmel performans, m√º≈üteri √ßok memnun kaldƒ±.",
    },
]

# Performans verileri
performance_data = [
    {
        "id": 1,
        "date": "2025-08-02",
        "personnel_id": 1,
        "member_count": 55,
        "whatsapp_count": 0,
        "device_count": 0,
        "unanswered_count": 0,
        "knowledge_duel_result": 0,
        "reward_penalty": "",
        "notes": "Test performans kaydƒ±",
    },
    {
        "id": 2,
        "date": "2025-08-08",
        "personnel_id": 2,
        "member_count": 20,
        "whatsapp_count": 0,
        "device_count": 0,
        "unanswered_count": 0,
        "knowledge_duel_result": 0,
        "reward_penalty": "",
        "notes": "Test performans kaydƒ±",
    },
]

# Eƒüitim-Geribildirim-Uyarƒ±-Kesinti verileri
training_feedback_data = [
    {
        "id": 1,
        "date": "2025-08-08",
        "personnel_id": 1,
        "warning_interruption_type": "uyari",  # "uyari" veya "kesinti"
        "warning_interruption_subject": "Ge√ß kalma uyarƒ±sƒ±",
        "feedback_count": 2,
        "feedback_subject": "M√º≈üteri memnuniyeti artƒ±rma √∂nerileri",
        "general_training_count": 1,
        "general_training_subject": "Satƒ±≈ü teknikleri eƒüitimi",
        "personal_training_count": 0,
        "personal_training_subject": "",
        "notes": "Personel geli≈üim s√ºreci takibi",
    },
    {
        "id": 2,
        "date": "2025-08-07",
        "personnel_id": 2,
        "warning_interruption_type": "geribildirim",  # Sadece geribildirim
        "warning_interruption_subject": "",
        "feedback_count": 1,
        "feedback_subject": "ƒ∞leti≈üim becerilerini geli≈ütirme",
        "general_training_count": 0,
        "general_training_subject": "",
        "personal_training_count": 1,
        "personal_training_subject": "Birebir coaching seansƒ±",
        "notes": "Performans iyile≈ütirme √ßalƒ±≈ümasƒ±",
    },
]

# Mesai sonrasƒ± √ßalƒ±≈üma verileri (after-hours)
after_hours_data = []  # {id, date, personnel_id, call_count, talk_duration, member_count, notes?}

# üîÑ KATEGORI 7 - VERƒ∞ SENKRONƒ∞ZASYON ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞
# Thread-safe data access i√ßin lock'lar
data_lock = (
    threading.RLock()
)  # Recursive lock - aynƒ± thread birden fazla acquire edebilir
backup_lock = threading.Lock()

# Basit JSON kalƒ±cƒ±lƒ±k (in-memory veri i√ßin)
# Persist path can be overridden via env PERSONEL_TAKIP_DATA_DIR (e.g., mount a volume)
DATA_DIR = os.getenv(
    "PERSONEL_TAKIP_DATA_DIR", os.path.join(os.path.dirname(__file__), "data")
)
try:
    os.makedirs(DATA_DIR, exist_ok=True)
except Exception:
    # Fall back to backend directory if custom path is not writable
    DATA_DIR = os.path.dirname(__file__)
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
    except Exception:
        pass

DATA_STORE_PATH = os.path.join(DATA_DIR, "data_store.json")


def persist_data_to_disk():
    """Verileri JSON dosyasƒ±na yazar (best-effort)."""
    global \
        personnel_data, \
        daily_records_data, \
        targets_data, \
        performance_data, \
        training_feedback_data, \
        after_hours_data
    payload = {
        "personnel": personnel_data,
        "daily_records": daily_records_data,
        "targets": targets_data,
        "performance": performance_data,
        "training_feedback": training_feedback_data,
        "after_hours": after_hours_data,
        "saved_at": datetime.now().isoformat(),
    }
    try:
        with open(DATA_STORE_PATH, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
    except Exception as e:
        slogger.log_error("PersistError", f"Persist failed: {e}")


def load_data_from_disk():
    """JSON dosyasƒ±ndan verileri y√ºkler; yoksa sessizce ge√ßer."""
    global \
        personnel_data, \
        daily_records_data, \
        targets_data, \
        performance_data, \
        training_feedback_data, \
        after_hours_data
    if not os.path.exists(DATA_STORE_PATH):
        return False
    try:
        with open(DATA_STORE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        # Basit ≈üema baƒülama
        personnel_data = data.get("personnel", personnel_data) or []
        daily_records_data = data.get("daily_records", daily_records_data) or []
        targets_data = data.get("targets", targets_data) or []
        performance_data = data.get("performance", performance_data) or []
        training_feedback_data = (
            data.get("training_feedback", training_feedback_data) or []
        )
        after_hours_data = data.get("after_hours", after_hours_data) or []
        return True
    except Exception as e:
        slogger.log_error("PersistError", f"Load failed: {e}")
        return False


class DataSyncManager:
    """Veri senkronizasyon ve tutarlƒ±lƒ±k y√∂neticisi"""

    def __init__(self):
        self.transaction_active = threading.local()  # Thread-local transaction state
        self.backup_interval = 300  # 5 dakika
        self.last_backup = time.time()
        self.data_version = 1  # Data versioning i√ßin

    @contextmanager
    def transaction(self, operation_name: str = "unknown"):
        """Transaction context manager - atomik i≈ülemler i√ßin"""
        global \
            personnel_data, \
            daily_records_data, \
            targets_data, \
            performance_data, \
            training_feedback_data

        if (
            hasattr(self.transaction_active, "active")
            and self.transaction_active.active
        ):
            # Nested transaction - sadece log
            slogger.log_business_event(
                event_type="nested_transaction",
                description=f"Nested transaction started: {operation_name}",
            )
            yield
            return

        # Ana transaction ba≈ülat
        self.transaction_active.active = True
        transaction_id = f"tx_{int(time.time() * 1000)}_{operation_name}"

        # Backup data for rollback - mevcut veri durumunu kaydet
        backup_data = {
            "personnel": copy.deepcopy(personnel_data),
            "daily_records": copy.deepcopy(daily_records_data),
            "targets": copy.deepcopy(targets_data),
            "performance": copy.deepcopy(performance_data),
            "training_feedback": copy.deepcopy(training_feedback_data),
            "version": self.data_version,
        }

        slogger.log_business_event(
            event_type="transaction_start",
            description=f"Transaction started: {transaction_id}",
            data={"operation": operation_name},
        )

        try:
            with data_lock:  # Thread-safe data access
                yield transaction_id

            # Transaction ba≈üarƒ±lƒ± - version artƒ±r
            self.data_version += 1
            # Best-effort: Persist data to disk after successful transaction
            try:
                persist_data_to_disk()
            except Exception as pe:
                slogger.log_error(
                    error_type="PersistError",
                    message=f"Failed to persist data to disk: {str(pe)}",
                )

            slogger.log_business_event(
                event_type="transaction_commit",
                description=f"Transaction committed: {transaction_id}",
                data={"new_version": self.data_version},
            )

        except Exception as e:
            # Rollback i≈ülemi
            slogger.log_error(
                error_type="TransactionRollback",
                message=f"Transaction rollback: {transaction_id}",
                stack_trace=str(e),
            )

            # Data'yƒ± geri y√ºkle
            personnel_data.clear()
            personnel_data.extend(backup_data["personnel"])
            daily_records_data.clear()
            daily_records_data.extend(backup_data["daily_records"])
            targets_data.clear()
            targets_data.extend(backup_data["targets"])
            performance_data.clear()
            performance_data.extend(backup_data["performance"])
            training_feedback_data.clear()
            training_feedback_data.extend(backup_data["training_feedback"])

            # Index'leri yeniden olu≈ütur
            rebuild_all_indexes()

            self.data_version = backup_data["version"]

            raise e
        finally:
            self.transaction_active.active = False

    def validate_data_integrity(self) -> dict:
        """Veri b√ºt√ºnl√ºƒü√º kontrol√º"""
        global \
            personnel_data, \
            daily_records_data, \
            targets_data, \
            performance_data, \
            training_feedback_data
        issues = []

        with data_lock:
            # 1. Personnel ID consistency
            personnel_ids = {p["id"] for p in personnel_data}

            # 2. Daily records'ta ge√ßersiz personnel_id kontrol√º
            for record in daily_records_data:
                if record["personnel_id"] not in personnel_ids:
                    issues.append(
                        f"Daily record {record['id']} has invalid personnel_id: {record['personnel_id']}"
                    )

            # 3. Targets'ta ge√ßersiz personnel_id kontrol√º
            for target in targets_data:
                if target["personnel_id"] not in personnel_ids:
                    issues.append(
                        f"Target {target['id']} has invalid personnel_id: {target['personnel_id']}"
                    )

            # 4. Training feedback'te ge√ßersiz personnel_id kontrol√º
            for record in training_feedback_data:
                if record["personnel_id"] not in personnel_ids:
                    issues.append(
                        f"Training feedback record {record['id']} has invalid personnel_id: {record['personnel_id']}"
                    )

            # 5. Duplicate ID kontrol√º
            personnel_id_counts = {}
            for p in personnel_data:
                pid = p["id"]
                personnel_id_counts[pid] = personnel_id_counts.get(pid, 0) + 1
                if personnel_id_counts[pid] > 1:
                    issues.append(f"Duplicate personnel ID: {pid}")

            # 6. Index consistency kontrol√º
            if len(personnel_index) != len(personnel_data):
                issues.append(
                    f"Personnel index size mismatch: index={len(personnel_index)}, data={len(personnel_data)}"
                )

            if len(daily_records_index) != len(daily_records_data):
                issues.append(
                    f"Daily records index size mismatch: index={len(daily_records_index)}, data={len(daily_records_data)}"
                )

            if len(targets_index) != len(targets_data):
                issues.append(
                    f"Targets index size mismatch: index={len(targets_index)}, data={len(targets_data)}"
                )

            if len(training_feedback_index) != len(training_feedback_data):
                issues.append(
                    f"Training feedback index size mismatch: index={len(training_feedback_index)}, data={len(training_feedback_data)}"
                )

        integrity_status = {
            "is_valid": len(issues) == 0,
            "issues": issues,
            "total_issues": len(issues),
            "data_version": self.data_version,
            "timestamp": datetime.now().isoformat(),
        }

        if issues:
            slogger.log_error(
                error_type="DataIntegrityViolation",
                message=f"Data integrity issues found: {len(issues)} issues",
                stack_trace=str(issues),
            )
        else:
            slogger.log_business_event(
                event_type="data_integrity_check",
                description="Data integrity validation passed",
                data={"version": self.data_version},
            )

        return integrity_status

    def create_data_backup(self) -> str:
        """Veri yedekleme"""
        global \
            personnel_data, \
            daily_records_data, \
            targets_data, \
            performance_data, \
            training_feedback_data
        try:
            with backup_lock:
                backup_filename = (
                    f"data_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                )
                backup_path = os.path.join(os.path.dirname(__file__), backup_filename)

                backup_data = {
                    "personnel_data": personnel_data,
                    "daily_records_data": daily_records_data,
                    "targets_data": targets_data,
                    "performance_data": performance_data,
                    "training_feedback_data": training_feedback_data,
                    "data_version": self.data_version,
                    "backup_timestamp": datetime.now().isoformat(),
                    "metadata": {
                        "personnel_count": len(personnel_data),
                        "daily_records_count": len(daily_records_data),
                        "targets_count": len(targets_data),
                        "training_feedback_count": len(training_feedback_data),
                    },
                }

                with open(backup_path, "wb") as f:
                    pickle.dump(backup_data, f)

                self.last_backup = time.time()

                slogger.log_business_event(
                    event_type="data_backup_created",
                    description=f"Data backup created: {backup_filename}",
                    data={
                        "file": backup_filename,
                        "version": self.data_version,
                        "size_bytes": os.path.getsize(backup_path),
                    },
                )

                return backup_path

        except Exception as e:
            slogger.log_error(
                error_type="BackupCreationError",
                message=f"Failed to create backup: {str(e)}",
                stack_trace=str(e),
            )
            raise e

    def auto_backup_check(self):
        """Otomatik yedekleme kontrol√º"""
        if time.time() - self.last_backup > self.backup_interval:
            try:
                self.create_data_backup()
            except Exception as e:
                slogger.log_error(
                    error_type="AutoBackupError",
                    message=f"Auto backup failed: {str(e)}",
                )

    def list_backup_files(self) -> List[Dict]:
        """Backup dosyalarƒ±nƒ± listele"""
        backup_files = []
        try:
            backend_dir = os.path.dirname(__file__)

            for filename in os.listdir(backend_dir):
                if filename.startswith("data_backup_") and filename.endswith(".pkl"):
                    file_path = os.path.join(backend_dir, filename)
                    file_stat = os.stat(file_path)

                    backup_files.append(
                        {
                            "filename": filename,
                            "size_bytes": file_stat.st_size,
                            "created_time": datetime.fromtimestamp(file_stat.st_ctime),
                            "age_hours": round(
                                (time.time() - file_stat.st_ctime) / 3600, 1
                            ),
                        }
                    )

            # En yeni backup'lar √∂nce
            backup_files.sort(key=lambda x: x["created_time"], reverse=True)

        except Exception as e:
            slogger.log_error(
                error_type="ListBackupFilesError",
                message=f"Failed to list backup files: {str(e)}",
            )

        return backup_files


# Data sync manager instance
sync_manager = DataSyncManager()

# Request counter global deƒüi≈üken (doc_manager hazƒ±r olmadan √∂nce)
global_request_counter = 0


def rebuild_all_indexes():
    """T√ºm index'leri yeniden olu≈ütur - thread-safe"""
    with data_lock:
        rebuild_personnel_index()
        rebuild_daily_records_index()
        rebuild_targets_index()
        rebuild_training_feedback_index()


def safe_data_operation(operation_name: str, operation_func, *args, **kwargs):
    """Thread-safe veri operasyonu wrapper"""
    try:
        with sync_manager.transaction(operation_name):
            result = operation_func(*args, **kwargs)

            # Otomatik backup kontrol√º
            sync_manager.auto_backup_check()

            return result

    except Exception as e:
        slogger.log_error(
            error_type="SafeDataOperationError",
            message=f"Safe operation failed: {operation_name}",
            stack_trace=str(e),
        )
        raise e


# FastAPI app instance with configuration
app = FastAPI(
    title=settings.app_name,
    description=settings.description,
    version=settings.app_version,
    docs_url="/docs" if not config_manager.is_production() else None,
    redoc_url="/redoc" if not config_manager.is_production() else None,
    debug=config_manager.is_debug_mode(),
)


# üìä REQUEST/RESPONSE MONITORING MIDDLEWARE
@app.middleware("http")
async def monitoring_middleware(request: Request, call_next):
    """Her API √ßaƒürƒ±sƒ±nƒ± izler ve performans metrics'lerini toplar"""
    start_time = time.time()

    # Request bilgilerini logla
    method = request.method
    url = str(request.url)
    endpoint = request.url.path

    # User bilgisini al (eƒüer authentication varsa)
    user_id = None
    try:
        # Authorization header'dan user bilgisini √ßƒ±kar
        auth_header = request.headers.get("authorization")
        if auth_header and auth_header.startswith("Bearer "):
            token = auth_header.split(" ")[1]
            if token in VALID_API_KEYS:
                user_id = VALID_API_KEYS[token]["name"]
    except:
        pass

    # Request'i logla
    slogger.log_request(method, endpoint, user_id)

    # Request counter'ƒ± artƒ±r (Category 10 i√ßin)
    if doc_manager:
        doc_manager.request_counter += 1
    else:
        global global_request_counter
        global_request_counter += 1

    # Response'u al
    try:
        response = await call_next(request)
        process_time = time.time() - start_time

        # Response'u logla
        slogger.log_response(endpoint, response.status_code, process_time, user_id)

        # Response header'a process time ekle
        response.headers["X-Process-Time"] = str(round(process_time * 1000, 2)) + "ms"

        return response

    except Exception as e:
        process_time = time.time() - start_time

        # Hata durumunu logla
        slogger.log_error(
            error_type=type(e).__name__,
            message=str(e),
            endpoint=endpoint,
            user_id=user_id,
            stack_trace=str(e),
        )

        raise e


# üîí AUTH ENFORCEMENT MIDDLEWARE (simple): require Bearer for API routes except auth and public assets
@app.middleware("http")
async def auth_enforcement_middleware(request: Request, call_next):
    path = request.url.path
    method = request.method.upper()
    # Allow root, static assets and styles, and docs in dev
    public_prefixes = [
        "/",
        "/static/",
        "/styles.css",
        "/professional-styles.css",
        "/styles-improvements.css",
        "/app.js",
        "/test",
        "/test_buttons.html",
        "/debug_console.html",
        "/docs",
        "/redoc",
        "/api/health",  # Health endpoint should be public for monitoring
    ]
    if any(path == p or path.startswith(p) for p in public_prefixes):
        return await call_next(request)
    # Allow auth endpoints without token
    if path.startswith("/api/auth/"):
        return await call_next(request)
    # Only enforce for API routes
    if path.startswith("/api/"):
        auth_header = request.headers.get("authorization") or request.headers.get(
            "Authorization"
        )
        if not auth_header or not auth_header.lower().startswith("bearer "):
            return Response(status_code=401)
        token = auth_header.split(" ")[-1].strip()
        if token not in VALID_API_KEYS:
            return Response(status_code=401)
    return await call_next(request)


# CORS middleware - Dynamic origins based on environment
app.add_middleware(
    CORSMiddleware,
    allow_origins=config_manager.get_cors_origins(),
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Static files - Frontend dosyalarƒ±nƒ± serve et
frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
if os.path.exists(frontend_path):
    app.mount("/static", StaticFiles(directory=frontend_path), name="static")

# Security
security = HTTPBearer()

# üîí G√úVENLƒ∞K ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞
# API anahtarlarƒ± - Production'da demo anahtarlarƒ± devre dƒ±≈üƒ±
if not config_manager.is_production():
    VALID_API_KEYS = {
        "demo-key-123": {"role": "admin", "name": "Demo Admin"},
        "readonly-key-456": {"role": "readonly", "name": "Demo Readonly"},
    }
else:
    VALID_API_KEYS = {}

# Admin credentials from environment (.env)
ADMIN_USERNAME = os.getenv("ADMIN_USERNAME", "")
ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "")

# Password hashing context (bcrypt)
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# Basit in-memory login deneme takibi: {username: [timestamps]}
_login_attempts = defaultdict(list)

# --- Simple SQLite-based user store ---
# Use the same DATA_DIR to keep DB file in a persistent location
DB_PATH = os.path.join(DATA_DIR, "auth.db")


def get_db_conn():
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


def init_auth_db():
    try:
        os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    except Exception:
        pass
    with get_db_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username TEXT NOT NULL UNIQUE,
                password_hash TEXT NOT NULL,
                role TEXT NOT NULL DEFAULT 'admin',
                is_active INTEGER NOT NULL DEFAULT 1,
                created_at TEXT NOT NULL
            )
            """
        )
        # Persistent session store for auth tokens
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS sessions (
                token TEXT PRIMARY KEY,
                user_id INTEGER NOT NULL,
                username TEXT NOT NULL,
                role TEXT NOT NULL,
                exp REAL NOT NULL,
                created_at TEXT NOT NULL,
                FOREIGN KEY(user_id) REFERENCES users(id)
            )
            """
        )
        conn.commit()


def get_user_by_username(username: str) -> Optional[dict]:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("SELECT * FROM users WHERE username = ? LIMIT 1", (username,))
            row = cur.fetchone()
            if not row:
                return None
            return dict(row)
    except Exception as e:
        slogger.log_error("DBError", f"get_user_by_username failed: {e}")
        return None


def get_user_by_id(user_id: int) -> Optional[dict]:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("SELECT * FROM users WHERE id = ? LIMIT 1", (user_id,))
            row = cur.fetchone()
            if not row:
                return None
            return dict(row)
    except Exception as e:
        slogger.log_error("DBError", f"get_user_by_id failed: {e}")
        return None


def list_users_db() -> List[dict]:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute(
                "SELECT id, username, role, is_active, created_at FROM users ORDER BY id ASC"
            )
            rows = cur.fetchall()
            return [dict(r) for r in rows]
    except Exception as e:
        slogger.log_error("DBError", f"list_users_db failed: {e}")
        return []


def create_user_db(
    username: str, password: str, role: str = "admin", is_active: bool = True
) -> dict:
    if not username or not password:
        raise HTTPException(status_code=400, detail="username ve password zorunlu")
    if role not in ("admin", "readonly", "user"):
        role = "admin"
    phash = pwd_context.hash(password)
    now = datetime.now().isoformat()
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute(
                "INSERT INTO users (username, password_hash, role, is_active, created_at) VALUES (?, ?, ?, ?, ?)",
                (username, phash, role, 1 if is_active else 0, now),
            )
            conn.commit()
            new_id = cur.lastrowid
            return {
                "id": new_id,
                "username": username,
                "role": role,
                "is_active": 1 if is_active else 0,
                "created_at": now,
            }
    except sqlite3.IntegrityError:
        raise HTTPException(status_code=400, detail="Bu kullanƒ±cƒ± adƒ± zaten mevcut")
    except Exception as e:
        slogger.log_error("DBError", f"create_user_db failed: {e}")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± olu≈üturma hatasƒ±")


def update_user_db(user_id: int, updates: dict) -> dict:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            # Fetch existing
            cur.execute("SELECT * FROM users WHERE id = ?", (user_id,))
            row = cur.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail="Kullanƒ±cƒ± bulunamadƒ±")
            user = dict(row)
            # Apply updates
            role = updates.get("role", user["role"]) or user["role"]
            is_active = updates.get("is_active")
            if is_active is None:
                is_active = user["is_active"]
            else:
                is_active = 1 if bool(is_active) else 0
            if "password" in updates and updates["password"]:
                phash = pwd_context.hash(str(updates["password"]))
                cur.execute(
                    "UPDATE users SET role = ?, is_active = ?, password_hash = ? WHERE id = ?",
                    (role, is_active, phash, user_id),
                )
            else:
                cur.execute(
                    "UPDATE users SET role = ?, is_active = ? WHERE id = ?",
                    (role, is_active, user_id),
                )
            conn.commit()
            return {
                "id": user_id,
                "username": user["username"],
                "role": role,
                "is_active": is_active,
                "created_at": user["created_at"],
            }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error("DBError", f"update_user_db failed: {e}")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± g√ºncelleme hatasƒ±")


def delete_user_db(user_id: int) -> dict:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("SELECT username FROM users WHERE id = ?", (user_id,))
            row = cur.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail="Kullanƒ±cƒ± bulunamadƒ±")
            username = row["username"]
            cur.execute("DELETE FROM users WHERE id = ?", (user_id,))
            conn.commit()
            return {"deleted_id": user_id, "username": username}
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error("DBError", f"delete_user_db failed: {e}")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± silme hatasƒ±")


def count_users_db() -> int:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(1) AS c FROM users")
            row = cur.fetchone()
            return int(row["c"]) if row else 0
    except Exception:
        return 0


# --- Session helpers (persistent tokens) ---
def _session_insert(token: str, user_id: int, username: str, role: str, exp_ts: float):
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute(
                "INSERT OR REPLACE INTO sessions (token, user_id, username, role, exp, created_at) VALUES (?, ?, ?, ?, ?, ?)",
                (
                    token,
                    user_id,
                    username,
                    role,
                    float(exp_ts),
                    datetime.now().isoformat(),
                ),
            )
            conn.commit()
    except Exception as e:
        slogger.log_error("DBError", f"session insert failed: {e}")


def _session_get(token: str) -> Optional[dict]:
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("SELECT * FROM sessions WHERE token = ? LIMIT 1", (token,))
            row = cur.fetchone()
            return dict(row) if row else None
    except Exception:
        return None


def _session_delete(token: str):
    try:
        with get_db_conn() as conn:
            cur = conn.cursor()
            cur.execute("DELETE FROM sessions WHERE token = ?", (token,))
            conn.commit()
    except Exception:
        pass


@app.on_event("startup")
async def startup_init_users():
    try:
        init_auth_db()
        if count_users_db() == 0:
            # Bootstrap admin from env if provided; else create dev default in debug mode
            if ADMIN_USERNAME and ADMIN_PASSWORD:
                create_user_db(
                    ADMIN_USERNAME, ADMIN_PASSWORD, role="admin", is_active=True
                )
                logger.info("Admin user bootstrapped from environment")
            elif config_manager.is_debug_mode():
                create_user_db("admin", "admin", role="admin", is_active=True)
                logger.warning(
                    "Default admin created (admin/admin) for development. Change immediately!"
                )
            else:
                logger.error(
                    "No users exist and ADMIN credentials not provided. Authentication will fail until a user is created."
                )
    except Exception as e:
        logger.error(f"User DB startup init failed: {e}")


def issue_token(username: str, role: str = "admin", user_id: int | None = None) -> str:
    """Create a new random token with TTL and register it in VALID_API_KEYS."""
    token = secrets.token_urlsafe(32)
    ttl_minutes = settings.security.access_token_expire_minutes
    expires_at = time.time() + (ttl_minutes * 60)
    VALID_API_KEYS[token] = {
        "role": role,
        "name": username,
        "id": user_id,
        "exp": expires_at,
    }
    # Persist session so tokens survive restarts (when DATA_DIR is on a volume)
    try:
        if user_id is None:
            # Try to resolve user id if not provided
            user = get_user_by_username(username)
            if user:
                user_id = int(user.get("id"))
        if user_id is not None:
            _session_insert(token, int(user_id), username, role, expires_at)
    except Exception:
        pass
    return token


# Admin yetkisi gerektiren endpoint'ler
ADMIN_ENDPOINTS = [
    "/api/personnel",  # POST/PUT/DELETE
    "/api/targets",  # POST/PUT/DELETE
    "/api/performance-records",  # POST/PUT/DELETE
]


async def verify_token(credentials: Optional[str] = Depends(security)) -> dict:
    """API token doƒürulama"""
    if not credentials:
        raise HTTPException(
            status_code=401,
            detail="Authentication required",
            headers={"WWW-Authenticate": "Bearer"},
        )

    token = credentials.credentials
    # Fast path: in-memory cache
    info = VALID_API_KEYS.get(token)
    if not info:
        # Slow path: check persistent session store
        sess = _session_get(token)
        if not sess:
            raise HTTPException(
                status_code=401,
                detail="Invalid authentication token",
                headers={"WWW-Authenticate": "Bearer"},
            )
        info = {
            "role": sess.get("role", "admin"),
            "name": sess.get("username"),
            "id": sess.get("user_id"),
            "exp": sess.get("exp"),
        }
        # rehydrate in-memory cache for performance
        VALID_API_KEYS[token] = info
    exp = info.get("exp")
    if exp and time.time() > exp:
        # Expired: kaldƒ±r ve 401
        try:
            VALID_API_KEYS.pop(token, None)
            _session_delete(token)
        finally:
            raise HTTPException(
                status_code=401,
                detail="Token s√ºresi doldu",
                headers={"WWW-Authenticate": "Bearer"},
            )

    return info


# Optional logout endpoint to allow clients to invalidate tokens explicitly
@app.post("/api/auth/logout")
async def logout(credentials: Optional[str] = Depends(security)):
    try:
        token = credentials.credentials if credentials else None
        if token:
            VALID_API_KEYS.pop(token, None)
            _session_delete(token)
        return {
            "success": True,
            "data": {"logged_out": True},
            "timestamp": datetime.now(),
        }
    except Exception:
        return {
            "success": True,
            "data": {"logged_out": True},
            "timestamp": datetime.now(),
        }


async def verify_admin_access(user: dict = Depends(verify_token)) -> dict:
    """Admin yetkisi kontrol√º"""
    if user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    return user


# === AUTH ENDPOINTS ===
@app.post("/api/auth/login")
async def login(payload: dict):
    """DB-backed login with bcrypt. Bootstraps admin on first run if configured."""
    try:
        username = str(payload.get("username", "")).strip()
        password = str(payload.get("password", "")).strip()
        if not username or not password:
            raise HTTPException(
                status_code=400, detail="Kullanƒ±cƒ± adƒ± ve ≈üifre zorunlu"
            )
        # ƒ∞stek √ºzerine kaldƒ±rƒ±ldƒ±: minimum parola uzunluƒüu kontrol√º yapƒ±lmƒ±yor

        # Brute-force/lockout: belirli zaman penceresinde √ßok deneme
        window_sec = settings.security.lockout_duration_minutes * 60
        max_attempts = settings.security.max_login_attempts
        now_ts = time.time()
        recent = [t for t in _login_attempts[username] if now_ts - t < window_sec]
        _login_attempts[username] = recent
        if len(recent) >= max_attempts:
            raise HTTPException(
                status_code=429,
                detail="√áok fazla deneme. L√ºtfen daha sonra tekrar deneyin.",
            )

        user = get_user_by_username(username)
        if user:
            if int(user.get("is_active", 1)) != 1:
                raise HTTPException(status_code=403, detail="Kullanƒ±cƒ± pasif")
            if not pwd_context.verify(password, user.get("password_hash", "")):
                _login_attempts[username].append(now_ts)
                raise HTTPException(
                    status_code=401, detail="Ge√ßersiz kullanƒ±cƒ± adƒ± veya ≈üifre"
                )
            role = user.get("role", "admin")
            tok = issue_token(username, role=role, user_id=int(user.get("id")))
            return {
                "success": True,
                "data": {
                    "token": tok,
                    "user": {"name": username, "role": role, "id": user.get("id")},
                },
                "timestamp": datetime.now(),
            }

        # Fallback: if no users exist but env admin provided, accept and create
        if (
            count_users_db() == 0
            and ADMIN_USERNAME
            and ADMIN_PASSWORD
            and username == ADMIN_USERNAME
            and password == ADMIN_PASSWORD
        ):
            created = create_user_db(username, password, role="admin", is_active=True)
            tok = issue_token(username, role="admin", user_id=int(created.get("id")))
            return {
                "success": True,
                "data": {
                    "token": tok,
                    "user": {
                        "name": username,
                        "role": "admin",
                        "id": created.get("id"),
                    },
                },
                "timestamp": datetime.now(),
            }

        _login_attempts[username].append(now_ts)
        raise HTTPException(status_code=401, detail="Ge√ßersiz kullanƒ±cƒ± adƒ± veya ≈üifre")
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error("AuthLoginError", str(e), endpoint="/api/auth/login")
        raise HTTPException(status_code=500, detail="Giri≈ü sƒ±rasƒ±nda hata olu≈ütu")


@app.get("/api/auth/users")
async def list_users(user: dict = Depends(verify_admin_access)):
    try:
        return {"success": True, "data": list_users_db(), "timestamp": datetime.now()}
    except Exception as e:
        slogger.log_error("AuthUsersListError", str(e), endpoint="/api/auth/users")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± listesi alƒ±namadƒ±")


@app.post("/api/auth/users")
async def create_user(payload: dict, user: dict = Depends(verify_admin_access)):
    try:
        username = (payload.get("username") or "").strip()
        password = (payload.get("password") or "").strip()
        role = (payload.get("role") or "admin").strip()
        is_active = bool(payload.get("is_active", True))
        created = create_user_db(username, password, role=role, is_active=is_active)
        return {
            "success": True,
            "data": created,
            "message": "Kullanƒ±cƒ± olu≈üturuldu",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error("AuthUserCreateError", str(e), endpoint="/api/auth/users")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± olu≈üturma hatasƒ±")


@app.put("/api/auth/users/{user_id}")
async def update_user(
    user_id: int, payload: dict, user: dict = Depends(verify_admin_access)
):
    try:
        updated = update_user_db(user_id, payload or {})
        return {
            "success": True,
            "data": updated,
            "message": "Kullanƒ±cƒ± g√ºncellendi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "AuthUserUpdateError", str(e), endpoint=f"/api/auth/users/{user_id}"
        )
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± g√ºncelleme hatasƒ±")


@app.delete("/api/auth/users/{user_id}")
async def delete_user(user_id: int, user: dict = Depends(verify_admin_access)):
    try:
        deleted = delete_user_db(user_id)
        return {
            "success": True,
            "data": deleted,
            "message": "Kullanƒ±cƒ± silindi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "AuthUserDeleteError", str(e), endpoint=f"/api/auth/users/{user_id}"
        )
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± silme hatasƒ±")


@app.post("/api/auth/change-password")
async def change_password(payload: dict, user: dict = Depends(verify_token)):
    try:
        user_id = user.get("id")
        username = user.get("name")
        if not user_id and username:
            u = get_user_by_username(username)
            user_id = u.get("id") if u else None
        if not user_id:
            raise HTTPException(status_code=400, detail="Mevcut kullanƒ±cƒ± bulunamadƒ±")
        old_pw = (payload.get("old_password") or "").strip()
        new_pw = (payload.get("new_password") or "").strip()
        if not old_pw or not new_pw:
            raise HTTPException(status_code=400, detail="Eski ve yeni ≈üifre zorunlu")
        urow = get_user_by_id(int(user_id))
        if not urow:
            raise HTTPException(status_code=404, detail="Kullanƒ±cƒ± bulunamadƒ±")
        if not pwd_context.verify(old_pw, urow.get("password_hash", "")):
            raise HTTPException(status_code=401, detail="Eski ≈üifre hatalƒ±")
        update_user_db(int(user_id), {"password": new_pw})
        return {
            "success": True,
            "data": {"id": user_id},
            "message": "≈ûifre g√ºncellendi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "AuthChangePasswordError", str(e), endpoint="/api/auth/change-password"
        )
        raise HTTPException(status_code=500, detail="≈ûifre deƒüi≈ütirme hatasƒ±")


@app.get("/api/auth/me")
async def auth_me(user: dict = Depends(verify_token)):
    """Returns current user based on bearer token."""
    try:
        return {"success": True, "data": {"user": user}, "timestamp": datetime.now()}
    except Exception as e:
        slogger.log_error("AuthMeError", str(e), endpoint="/api/auth/me")
        raise HTTPException(status_code=500, detail="Kullanƒ±cƒ± bilgisi alƒ±namadƒ±")


# Input sanitization fonksiyonu
def sanitize_string(value: str) -> str:
    """String'i XSS saldƒ±rƒ±larƒ±na kar≈üƒ± temizle"""
    if not isinstance(value, str):
        return value

    # HTML karakterlerini escape et
    import html

    sanitized = html.escape(value)

    # Script taglerini kaldƒ±r
    import re

    sanitized = re.sub(
        r"<script[^>]*>.*?</script>", "", sanitized, flags=re.IGNORECASE | re.DOTALL
    )

    return sanitized.strip()


# üîí RATE LIMITING - API k√∂t√ºye kullanƒ±m korumasƒ±
rate_limit_storage = defaultdict(list)  # {ip: [timestamp, timestamp, ...]}
RATE_LIMIT_REQUESTS = 100  # dakikada maksimum istek
RATE_LIMIT_WINDOW = 60  # saniye (1 dakika)


async def check_rate_limit(request: Request):
    """Rate limiting kontrol√º"""
    client_ip = request.client.host
    current_time = time.time()

    # Eski istekleri temizle (1 dakikadan eskiler)
    rate_limit_storage[client_ip] = [
        timestamp
        for timestamp in rate_limit_storage[client_ip]
        if current_time - timestamp < RATE_LIMIT_WINDOW
    ]

    # Mevcut istek sayƒ±sƒ±nƒ± kontrol et
    if len(rate_limit_storage[client_ip]) >= RATE_LIMIT_REQUESTS:
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded. Maximum {RATE_LIMIT_REQUESTS} requests per minute allowed.",
        )

    # Yeni isteƒüi kaydet
    rate_limit_storage[client_ip].append(current_time)


# üöÄ PERFORMANS ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞ - INDEX'LER
# ID bazlƒ± hƒ±zlƒ± eri≈üim i√ßin dictionary index'leri
personnel_index = {}  # {id: personnel_object}
daily_records_index = {}  # {id: record_object}
targets_index = {}  # {id: target_object}
training_feedback_index = {}  # {id: training_feedback_object}
after_hours_index = {}  # {id: after_hours_object}


# Index'leri g√ºncelleme fonksiyonlarƒ±
def rebuild_personnel_index():
    """Personnel verilerinin index'ini yeniden olu≈ütur"""
    global personnel_index
    personnel_index = {p["id"]: p for p in personnel_data}


def rebuild_daily_records_index():
    """Daily records verilerinin index'ini yeniden olu≈ütur"""
    global daily_records_index
    daily_records_index = {r["id"]: r for r in daily_records_data}


def rebuild_targets_index():
    """Targets verilerinin index'ini yeniden olu≈ütur"""
    global targets_index
    targets_index = {t["id"]: t for t in targets_data}


def rebuild_training_feedback_index():
    """Training feedback verilerinin index'ini yeniden olu≈ütur"""
    global training_feedback_index
    training_feedback_index = {tf["id"]: tf for tf in training_feedback_data}


def rebuild_after_hours_index():
    """After-hours verilerinin index'ini yeniden olu≈ütur"""
    global after_hours_index
    after_hours_index = {ah["id"]: ah for ah in after_hours_data}


# Ba≈ülangƒ±√ßta index'leri olu≈ütur
_loaded = load_data_from_disk()
rebuild_personnel_index()
rebuild_daily_records_index()
rebuild_targets_index()
rebuild_training_feedback_index()
rebuild_after_hours_index()

# Max ID cache'leri - performans i√ßin
_max_personnel_id = max([p.get("id", 0) for p in personnel_data], default=0)
_max_daily_record_id = max([r.get("id", 0) for r in daily_records_data], default=0)
_max_target_id = max([t.get("id", 0) for t in targets_data], default=0)
_max_training_feedback_id = max(
    [tf.get("id", 0) for tf in training_feedback_data], default=0
)
_max_after_hours_id = max([ah.get("id", 0) for ah in after_hours_data], default=0)


# Helper: filter only Warning/Cut-like entries from training_feedback_data
def _is_warning_cut(rec: dict) -> bool:
    t = (rec or {}).get("warning_interruption_type", "").lower()
    return t in {"uyari", "kesinti"}


def _tf_get_warning_cut_records(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
) -> List[dict]:
    with data_lock:
        items = [r for r in training_feedback_data if _is_warning_cut(r)]
        if start_date or end_date:
            tmp = []
            for r in items:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            items = tmp
        if personnel_id:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            items = [r for r in items if r.get("personnel_id") == pid]
        return copy.deepcopy(items)


@app.get("/api/warnings-cuts")
async def list_warnings_cuts(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """UYARI/KESƒ∞NTƒ∞ kayƒ±tlarƒ±nƒ± d√∂nd√ºr√ºr (training_feedback_data i√ßinden filtrelenir)."""
    try:
        rows = _tf_get_warning_cut_records(start_date, end_date, personnel_id)
        return {
            "success": True,
            "data": rows,
            "total": len(rows),
            "timestamp": datetime.now(),
        }
    except Exception as e:
        slogger.log_error(
            "WarningsCutsListError", str(e), endpoint="/api/warnings-cuts"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti listesi hatasƒ±: {str(e)}"
        )


@app.post("/api/warnings-cuts")
async def create_warning_cut(payload: dict):
    """Yeni UYARI/KESƒ∞NTƒ∞ kaydƒ± ekler (training_feedback_data'ya)."""
    try:
        required = ["date", "personnel_id", "warning_interruption_type"]
        for k in required:
            if k not in payload or payload[k] in (None, ""):
                raise HTTPException(status_code=400, detail=f"{k} alanƒ± gerekli")
        t = str(payload.get("warning_interruption_type", "")).lower()
        if t not in {"uyari", "kesinti"}:
            raise HTTPException(
                status_code=400,
                detail="warning_interruption_type 'uyari' veya 'kesinti' olmalƒ±",
            )
        count = payload.get("warning_interruption_count", 1)
        try:
            count = int(count)
        except Exception:
            count = 1
        if count < 1:
            count = 1
        global _max_training_feedback_id
        _max_training_feedback_id += 1
        rec = {
            "id": _max_training_feedback_id,
            "date": payload["date"],
            "personnel_id": int(payload["personnel_id"]),
            "warning_interruption_type": t,
            "warning_interruption_subject": payload.get(
                "warning_interruption_subject", ""
            ),
            "warning_interruption_count": count,
            # keep other training fields empty to avoid mixing
            "feedback_count": 0,
            "feedback_subject": "",
            "general_training_count": 0,
            "general_training_subject": "",
            "personal_training_count": 0,
            "personal_training_subject": "",
            "notes": payload.get("notes", ""),
        }
        with data_lock:
            training_feedback_data.append(rec)
            training_feedback_index[rec["id"]] = rec
        return {
            "success": True,
            "data": rec,
            "message": "Kayƒ±t eklendi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "WarningsCutsCreateError", str(e), endpoint="/api/warnings-cuts"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti ekleme hatasƒ±: {str(e)}"
        )


@app.put("/api/warnings-cuts/{rec_id}")
async def update_warning_cut(rec_id: int, payload: dict):
    """Mevcut UYARI/KESƒ∞NTƒ∞ kaydƒ±nƒ± g√ºnceller."""
    try:
        with data_lock:
            rec = training_feedback_index.get(rec_id)
            if not rec or not _is_warning_cut(rec):
                raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
            # Update allowed fields
            if "date" in payload:
                rec["date"] = payload["date"]
            if "personnel_id" in payload:
                rec["personnel_id"] = int(payload["personnel_id"])
            if "warning_interruption_type" in payload:
                t = str(payload.get("warning_interruption_type", "")).lower()
                if t not in {"uyari", "kesinti"}:
                    raise HTTPException(
                        status_code=400,
                        detail="warning_interruption_type 'uyari' veya 'kesinti' olmalƒ±",
                    )
                rec["warning_interruption_type"] = t
            if "warning_interruption_subject" in payload:
                rec["warning_interruption_subject"] = payload.get(
                    "warning_interruption_subject", ""
                )
            if "warning_interruption_count" in payload:
                try:
                    c = int(payload.get("warning_interruption_count", 1))
                except Exception:
                    c = 1
                rec["warning_interruption_count"] = max(1, c)
        return {
            "success": True,
            "data": rec,
            "message": "Kayƒ±t g√ºncellendi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "WarningsCutsUpdateError", str(e), endpoint=f"/api/warnings-cuts/{rec_id}"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti g√ºncelleme hatasƒ±: {str(e)}"
        )


@app.delete("/api/warnings-cuts/{rec_id}")
async def delete_warning_cut(rec_id: int):
    """UYARI/KESƒ∞NTƒ∞ kaydƒ±nƒ± siler."""
    try:
        with data_lock:
            rec = training_feedback_index.get(rec_id)
            if not rec or not _is_warning_cut(rec):
                raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
            # remove from list
            for i, r in enumerate(training_feedback_data):
                if r.get("id") == rec_id:
                    training_feedback_data.pop(i)
                    break
            training_feedback_index.pop(rec_id, None)
        return {
            "success": True,
            "data": {"deleted_id": rec_id},
            "message": "Kayƒ±t silindi",
            "timestamp": datetime.now(),
        }
    except HTTPException:
        raise
    except Exception as e:
        slogger.log_error(
            "WarningsCutsDeleteError", str(e), endpoint=f"/api/warnings-cuts/{rec_id}"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti silme hatasƒ±: {str(e)}"
        )


@app.get("/api/warnings-cuts/summary")
async def warnings_cuts_summary(
    start_date: Optional[str] = None, end_date: Optional[str] = None
):
    """Uyarƒ±/Kesinti √∂zetini (personel bazlƒ± toplamlar ve t√ºr bazlƒ±) d√∂nd√ºr√ºr."""
    try:
        rows = _tf_get_warning_cut_records(start_date, end_date)
        # Aggregate
        per_person = defaultdict(lambda: {"uyari": 0, "kesinti": 0})
        type_totals = {"uyari": 0, "kesinti": 0}
        daily_trend = defaultdict(lambda: {"uyari": 0, "kesinti": 0})
        for r in rows:
            pid = r.get("personnel_id")
            t = (r.get("warning_interruption_type") or "").lower()
            c = r.get("warning_interruption_count")
            if c is None:
                c = r.get("feedback_count", 1)
            try:
                c = int(c)
            except Exception:
                c = 1
            per_person[pid][t] += c
            type_totals[t] += c
            d = r.get("date") or ""
            if d:
                daily_trend[d][t] += c
        # Enrich with personnel name
        result_rows = []
        for pid, counts in per_person.items():
            person = personnel_index.get(pid)
            result_rows.append(
                {
                    "personnel_id": pid,
                    "personnel_name": person.get("name") if person else f"#{pid}",
                    "uyari": counts.get("uyari", 0),
                    "kesinti": counts.get("kesinti", 0),
                    "toplam": counts.get("uyari", 0) + counts.get("kesinti", 0),
                }
            )
        # Sort by toplam desc
        result_rows.sort(key=lambda x: x["toplam"], reverse=True)
        # Convert defaultdicts to plain dicts for JSON serialization
        daily_trend_dict = {d: dict(v) for d, v in daily_trend.items()}
        return {
            "success": True,
            "data": {
                "per_person": result_rows,
                "type_totals": dict(type_totals),
                "daily_trend": daily_trend_dict,
            },
            "timestamp": datetime.now(),
        }
    except Exception as e:
        slogger.log_error(
            "WarningsCutsSummaryError", str(e), endpoint="/api/warnings-cuts/summary"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti √∂zeti hatasƒ±: {str(e)}"
        )


@app.get("/api/warnings-cuts/export")
async def export_warnings_cuts(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    format: Optional[str] = "excel",
):
    """Uyarƒ±/Kesinti kayƒ±tlarƒ±nƒ± Excel veya CSV olarak dƒ±≈üa aktarƒ±r."""
    try:
        rows = _tf_get_warning_cut_records(start_date, end_date)
        # Build DataFrame
        df_rows = []
        for r in rows:
            person = personnel_index.get(r.get("personnel_id"))
            df_rows.append(
                {
                    "Tarih": r.get("date"),
                    "Personel": person.get("name") if person else r.get("personnel_id"),
                    "T√ºr": (r.get("warning_interruption_type") or "").capitalize(),
                    "Adet": r.get("warning_interruption_count")
                    or r.get("feedback_count", 1),
                    "Konu": r.get("warning_interruption_subject")
                    or r.get("feedback_subject", ""),
                }
            )
        df = pd.DataFrame(df_rows)
        # Prefer configured export path; fallback to DATA_DIR/exports
        exports_dir = settings.export_path or os.path.join(DATA_DIR, "exports")
        try:
            os.makedirs(exports_dir, exist_ok=True)
        except Exception:
            # Last resort: backend-local exports
            exports_dir = os.path.join(os.path.dirname(__file__), "exports")
            os.makedirs(exports_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        if (format or "").lower() == "csv":
            fname = (
                f"warnings_cuts_{start_date or 'all'}_to_{end_date or 'all'}_{ts}.csv"
            )
            fpath = os.path.join(exports_dir, fname)
            df.to_csv(fpath, index=False, encoding="utf-8-sig")
        else:
            fname = (
                f"warnings_cuts_{start_date or 'all'}_to_{end_date or 'all'}_{ts}.xlsx"
            )
            fpath = os.path.join(exports_dir, fname)
            with pd.ExcelWriter(fpath, engine="xlsxwriter") as writer:
                df.to_excel(writer, index=False, sheet_name="UyariKesinti")
        return FileResponse(fpath, filename=fname)
    except Exception as e:
        slogger.log_error(
            "WarningsCutsExportError", str(e), endpoint="/api/warnings-cuts/export"
        )
        raise HTTPException(
            status_code=500, detail=f"Uyarƒ±/Kesinti export hatasƒ±: {str(e)}"
        )


@app.get("/")
async def root():
    """Ana sayfa - Frontend index.html dosyasƒ±nƒ± serve et"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    index_path = os.path.join(frontend_path, "index.html")

    if os.path.exists(index_path):
        resp = FileResponse(index_path)
        # HTML i√ßin cache‚Äôi kapat
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp
    else:
        # Fallback: API bilgileri
        return {
            "success": True,
            "data": {
                "message": "üöÄ Personel Takip Paneli API",
                "version": "1.0.0",
                "status": "active",
                "features": [
                    "‚úÖ Personel y√∂netimi",
                    "‚úÖ G√ºnl√ºk veri giri≈üi",
                    "‚úÖ Excel import/export",
                    "‚úÖ Analytics dashboard",
                ],
            },
            "timestamp": datetime.now().isoformat(),
        }


@app.get("/test")
async def test_debug():
    """Test debug sayfasƒ±"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    test_path = os.path.join(frontend_path, "test-debug.html")

    if os.path.exists(test_path):
        return FileResponse(test_path)
    else:
        return {"error": "Test file not found"}


@app.get("/styles.css")
async def get_styles():
    """CSS dosyasƒ±nƒ± serve et"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    css_path = os.path.join(frontend_path, "styles.css")
    if os.path.exists(css_path):
        resp = FileResponse(css_path, media_type="text/css")
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp
    raise HTTPException(status_code=404, detail="CSS file not found")


@app.get("/professional-styles.css")
async def get_professional_styles():
    """Professional CSS dosyasƒ±nƒ± serve et"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    css_path = os.path.join(frontend_path, "professional-styles.css")
    if os.path.exists(css_path):
        resp = FileResponse(css_path, media_type="text/css")
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp
    raise HTTPException(status_code=404, detail="Professional CSS file not found")


@app.get("/styles-improvements.css")
async def get_styles_improvements():
    """Styles improvements CSS dosyasƒ±nƒ± serve et"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    css_path = os.path.join(frontend_path, "styles-improvements.css")
    if os.path.exists(css_path):
        resp = FileResponse(css_path, media_type="text/css")
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp
    raise HTTPException(
        status_code=404, detail="Styles improvements CSS file not found"
    )


@app.get("/app.js")
async def get_app_js():
    """JavaScript dosyasƒ±nƒ± serve et (cache kapalƒ±)"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    js_path = os.path.join(frontend_path, "app.js")
    if os.path.exists(js_path):
        resp = FileResponse(js_path, media_type="application/javascript")
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp
    raise HTTPException(status_code=404, detail="JavaScript file not found")


@app.get("/static/app-test.js")
async def get_app_test_js():
    """Frontend JavaScript dosyasƒ± - Test versiyonu"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    js_path = os.path.join(frontend_path, "app-test.js")

    if os.path.exists(js_path):
        return FileResponse(js_path, media_type="application/javascript")
    else:
        # Fallback to regular app.js
        js_path = os.path.join(frontend_path, "app.js")
        if os.path.exists(js_path):
            return FileResponse(js_path, media_type="application/javascript")
        else:
            return Response(
                "console.log('JavaScript not found');",
                media_type="application/javascript",
            )


# Duplicate /app.js route removed to avoid ambiguity


@app.get("/static/professional-styles.css")
async def get_professional_styles_css():
    """Profesyonel CSS stillerini serve et"""
    try:
        css_path = os.path.join(
            os.path.dirname(__file__), "..", "frontend", "professional-styles.css"
        )
        if os.path.exists(css_path):
            return FileResponse(css_path, media_type="text/css")
        else:
            # Eƒüer dosya yoksa bo≈ü CSS d√∂nd√ºr
            return Response(
                "/* Professional styles not found */", media_type="text/css"
            )
    except Exception as e:
        return Response(
            "/* Error loading professional styles */", media_type="text/css"
        )


@app.get("/static/styles.css")
async def get_styles_css():
    """Frontend CSS dosyasƒ±"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    css_path = os.path.join(frontend_path, "styles.css")

    if os.path.exists(css_path):
        return FileResponse(css_path, media_type="text/css")
    else:
        raise HTTPException(status_code=404, detail="styles.css not found")


@app.get("/test_buttons.html")
async def get_test_buttons():
    """Test buttons HTML dosyasƒ±"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    test_path = os.path.join(frontend_path, "test_buttons.html")

    if os.path.exists(test_path):
        return FileResponse(test_path, media_type="text/html")
    else:
        raise HTTPException(status_code=404, detail="test_buttons.html not found")


@app.get("/debug_console.html")
async def get_debug_console():
    """Debug console HTML dosyasƒ±"""
    frontend_path = os.path.join(os.path.dirname(__file__), "..", "frontend")
    debug_path = os.path.join(frontend_path, "debug_console.html")

    if os.path.exists(debug_path):
        return FileResponse(debug_path, media_type="text/html")
    else:
        raise HTTPException(status_code=404, detail="debug_console.html not found")


@app.get("/api/health")
async def health_check():
    """Geli≈ümi≈ü sistem durumu kontrol√º - üìä Monitoring bilgileri dahil"""
    try:
        # Sistem ba≈ülangƒ±√ß zamanƒ± (uygulama start time)
        start_time = datetime.now() - timedelta(hours=1, minutes=30)  # Placeholder
        uptime_seconds = (datetime.now() - start_time).total_seconds()
        uptime_formatted = (
            f"{int(uptime_seconds // 3600)}h {int((uptime_seconds % 3600) // 60)}m"
        )

        # Performance stats hesaplama
        endpoint_stats = {}
        for endpoint, response_times in request_stats.items():
            if response_times:
                endpoint_stats[endpoint] = {
                    "total_requests": len(response_times),
                    "avg_response_time_ms": round(
                        sum(response_times) * 1000 / len(response_times), 2
                    ),
                    "min_response_time_ms": round(min(response_times) * 1000, 2),
                    "max_response_time_ms": round(max(response_times) * 1000, 2),
                }

        # Memory ve data stats
        memory_stats = {
            "personnel_count": len(personnel_data),
            "daily_records_count": len(daily_records_data),
            "targets_count": len(targets_data),
            "performance_records_count": len(performance_data),
            "index_sizes": {
                "personnel_index": len(personnel_index),
                "daily_records_index": len(daily_records_index),
                "targets_index": len(targets_index),
            },
        }

        # Log file kontrol√º
        log_file_exists = os.path.exists("personel_takip.log")
        log_file_size = 0
        if log_file_exists:
            try:
                log_file_size = os.path.getsize("personel_takip.log")
            except:
                log_file_size = 0

        # Rate limiting stats
        rate_limit_stats = {
            "active_ips": len(rate_limit_storage),
            "total_tracked_requests": sum(
                len(requests) for requests in rate_limit_storage.values()
            ),
        }

        # üîÑ Data sync stats
        data_sync_stats = {
            "data_version": sync_manager.data_version,
            "last_backup_age_seconds": int(time.time() - sync_manager.last_backup),
            "backup_interval_seconds": sync_manager.backup_interval,
            "data_integrity": sync_manager.validate_data_integrity()["is_valid"],
            "thread_locks_active": data_lock._count
            if hasattr(data_lock, "_count")
            else 0,
        }

        health_data = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "uptime": uptime_formatted,
        }
        # Dev/Prod ayrƒ±mƒ±: prod'da minimal yanƒ±t
        if not config_manager.is_production():
            health_data.update(
                {
                    "system": {
                        "database": "in-memory",
                        "logging": "active" if log_file_exists else "inactive",
                        "authentication": "active",
                        "rate_limiting": "active",
                    },
                    "performance": {
                        "endpoint_stats": endpoint_stats,
                        "total_endpoints_tracked": len(endpoint_stats),
                    },
                    "data": memory_stats,
                    "security": {
                        "rate_limiting": rate_limit_stats,
                        "valid_api_keys": len(VALID_API_KEYS),
                    },
                    "data_sync": data_sync_stats,
                    "logs": {
                        "file_exists": log_file_exists,
                        "file_size_bytes": log_file_size,
                    },
                }
            )

        # Business event olarak logla
        slogger.log_business_event(
            event_type="health_check",
            description="System health check performed",
            data={"status": "healthy"},
        )

        return {"success": True, "data": health_data, "timestamp": datetime.now()}

    except Exception as e:
        # Error olarak logla
        slogger.log_error(
            error_type="HealthCheckError", message=str(e), endpoint="/api/health"
        )

        print(f"‚ùå Health check hatasƒ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Health check hatasƒ±: {str(e)}")


# üìä MONƒ∞TORƒ∞NG ENDPOINTS - Kategori 6
@app.get("/api/monitoring/stats")
async def get_monitoring_stats():
    """Detaylƒ± sistem performans istatistikleri"""
    try:
        # Endpoint bazlƒ± detaylƒ± istatistikler
        detailed_stats = {}
        for endpoint, response_times in request_stats.items():
            if response_times:
                # Son 10 request'in ortalamasƒ±
                recent_times = (
                    response_times[-10:]
                    if len(response_times) >= 10
                    else response_times
                )

                detailed_stats[endpoint] = {
                    "total_requests": len(response_times),
                    "avg_response_time_ms": round(
                        sum(response_times) * 1000 / len(response_times), 2
                    ),
                    "recent_avg_response_time_ms": round(
                        sum(recent_times) * 1000 / len(recent_times), 2
                    ),
                    "min_response_time_ms": round(min(response_times) * 1000, 2),
                    "max_response_time_ms": round(max(response_times) * 1000, 2),
                    "p95_response_time_ms": round(
                        sorted(response_times)[int(len(response_times) * 0.95)] * 1000,
                        2,
                    )
                    if len(response_times) > 1
                    else 0,
                }

        # Sistem resource kullanƒ±mƒ±
        system_resources = {
            "memory_usage": {
                "personnel_records": len(personnel_data),
                "daily_records": len(daily_records_data),
                "target_records": len(targets_data),
                "performance_records": len(performance_data),
                "index_memory": {
                    "personnel_index_size": len(personnel_index),
                    "daily_records_index_size": len(daily_records_index),
                    "targets_index_size": len(targets_index),
                },
            },
            "rate_limiting": {
                "tracked_ips": len(rate_limit_storage),
                "total_requests_in_window": sum(
                    len(requests) for requests in rate_limit_storage.values()
                ),
                "max_requests_per_ip": max(
                    [len(requests) for requests in rate_limit_storage.values()]
                )
                if rate_limit_storage
                else 0,
            },
        }

        # Top 5 en yava≈ü endpoint'ler
        slowest_endpoints = []
        for endpoint, stats in detailed_stats.items():
            slowest_endpoints.append(
                {"endpoint": endpoint, "avg_time_ms": stats["avg_response_time_ms"]}
            )
        slowest_endpoints.sort(key=lambda x: x["avg_time_ms"], reverse=True)

        return {
            "success": True,
            "data": {
                "endpoint_performance": detailed_stats,
                "system_resources": system_resources,
                "top_slowest_endpoints": slowest_endpoints[:5],
                "summary": {
                    "total_tracked_endpoints": len(detailed_stats),
                    "total_requests": sum(
                        stats["total_requests"] for stats in detailed_stats.values()
                    ),
                    "overall_avg_response_time_ms": round(
                        sum(sum(times) for times in request_stats.values())
                        * 1000
                        / sum(len(times) for times in request_stats.values()),
                        2,
                    )
                    if request_stats
                    else 0,
                },
            },
            "timestamp": datetime.now(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="MonitoringStatsError",
            message=str(e),
            endpoint="/api/monitoring/stats",
        )
        raise HTTPException(
            status_code=500, detail=f"Monitoring stats hatasƒ±: {str(e)}"
        )


@app.get("/api/monitoring/logs")
async def get_recent_logs(limit: int = 50):
    """Son N adet log kaydƒ±nƒ± getirir"""
    try:
        logs = []
        log_file_path = "personel_takip.log"

        if os.path.exists(log_file_path):
            try:
                with open(log_file_path, "r", encoding="utf-8") as f:
                    all_lines = f.readlines()
                    # Son N satƒ±rƒ± al
                    recent_lines = (
                        all_lines[-limit:] if len(all_lines) > limit else all_lines
                    )

                    for line in recent_lines:
                        if line.strip():
                            logs.append(line.strip())

            except UnicodeDecodeError:
                # UTF-8 ba≈üarƒ±sƒ±z olursa latin-1 dene
                with open(log_file_path, "r", encoding="latin-1") as f:
                    all_lines = f.readlines()
                    recent_lines = (
                        all_lines[-limit:] if len(all_lines) > limit else all_lines
                    )

                    for line in recent_lines:
                        if line.strip():
                            logs.append(line.strip())

        return {
            "success": True,
            "data": {
                "logs": logs,
                "total_returned": len(logs),
                "log_file_exists": os.path.exists(log_file_path),
            },
            "timestamp": datetime.now(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="LogRetrievalError",
            message=str(e),
            endpoint="/api/monitoring/logs",
        )
        raise HTTPException(status_code=500, detail=f"Log retrieval hatasƒ±: {str(e)}")


# ÔøΩ VERƒ∞ SENKRONƒ∞ZASYON ENDPOINTS - Kategori 7
@app.get("/api/sync/integrity-check")
async def check_data_integrity():
    """Veri b√ºt√ºnl√ºƒü√º kontrol√º"""
    try:
        integrity_report = sync_manager.validate_data_integrity()

        return {"success": True, "data": integrity_report, "timestamp": datetime.now()}

    except Exception as e:
        slogger.log_error(
            error_type="IntegrityCheckError",
            message=str(e),
            endpoint="/api/sync/integrity-check",
        )
        raise HTTPException(status_code=500, detail=f"Integrity check hatasƒ±: {str(e)}")


@app.post("/api/sync/create-backup")
async def create_manual_backup(user: dict = Depends(verify_admin_access)):
    """Manuel veri yedekleme - Admin yetkisi gerekli"""
    try:
        backup_path = sync_manager.create_data_backup()
        backup_filename = os.path.basename(backup_path)
        backup_size = os.path.getsize(backup_path)

        return {
            "success": True,
            "data": {
                "backup_file": backup_filename,
                "backup_path": backup_path,
                "file_size_bytes": backup_size,
                "file_size_mb": round(backup_size / (1024 * 1024), 2),
                "data_version": sync_manager.data_version,
                "created_by": user["name"],
            },
            "message": f"Backup ba≈üarƒ±yla olu≈üturuldu: {backup_filename}",
            "timestamp": datetime.now(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="ManualBackupError",
            message=str(e),
            endpoint="/api/sync/create-backup",
            user_id=user["name"],
        )
        raise HTTPException(
            status_code=500, detail=f"Backup olu≈üturma hatasƒ±: {str(e)}"
        )


@app.get("/api/sync/data-version")
async def get_data_version():
    """Mevcut veri versiyonu bilgisi"""
    try:
        with data_lock:
            version_info = {
                "data_version": sync_manager.data_version,
                "last_backup": sync_manager.last_backup,
                "backup_age_seconds": int(time.time() - sync_manager.last_backup),
                "data_counts": {
                    "personnel": len(personnel_data),
                    "daily_records": len(daily_records_data),
                    "targets": len(targets_data),
                    "performance": len(performance_data),
                },
                "index_counts": {
                    "personnel_index": len(personnel_index),
                    "daily_records_index": len(daily_records_index),
                    "targets_index": len(targets_index),
                },
            }

        return {"success": True, "data": version_info, "timestamp": datetime.now()}

    except Exception as e:
        slogger.log_error(
            error_type="DataVersionError",
            message=str(e),
            endpoint="/api/sync/data-version",
        )
        raise HTTPException(status_code=500, detail=f"Data version hatasƒ±: {str(e)}")


@app.post("/api/sync/rebuild-indexes")
async def rebuild_indexes_endpoint(user: dict = Depends(verify_admin_access)):
    """Index'leri yeniden olu≈ütur - Admin yetkisi gerekli"""
    try:
        with sync_manager.transaction("rebuild_indexes"):
            old_counts = {
                "personnel_index": len(personnel_index),
                "daily_records_index": len(daily_records_index),
                "targets_index": len(targets_index),
            }

            rebuild_all_indexes()

            new_counts = {
                "personnel_index": len(personnel_index),
                "daily_records_index": len(daily_records_index),
                "targets_index": len(targets_index),
            }

            slogger.log_business_event(
                event_type="indexes_rebuilt",
                description="All indexes rebuilt successfully",
                data={
                    "old_counts": old_counts,
                    "new_counts": new_counts,
                    "rebuilder": user["name"],
                },
                user_id=user["name"],
            )

        return {
            "success": True,
            "data": {
                "old_counts": old_counts,
                "new_counts": new_counts,
                "rebuilt_by": user["name"],
            },
            "message": "Index'ler ba≈üarƒ±yla yeniden olu≈üturuldu",
            "timestamp": datetime.now(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="IndexRebuildError",
            message=str(e),
            endpoint="/api/sync/rebuild-indexes",
            user_id=user["name"],
        )
        raise HTTPException(status_code=500, detail=f"Index rebuild hatasƒ±: {str(e)}")


@app.get("/api/sync/backup-list")
async def list_backups():
    """Mevcut backup dosyalarƒ±nƒ± listele"""
    try:
        backup_files = []
        backend_dir = os.path.dirname(__file__)

        for filename in os.listdir(backend_dir):
            if filename.startswith("data_backup_") and filename.endswith(".pkl"):
                file_path = os.path.join(backend_dir, filename)
                file_stat = os.stat(file_path)

                backup_files.append(
                    {
                        "filename": filename,
                        "size_bytes": file_stat.st_size,
                        "size_mb": round(file_stat.st_size / (1024 * 1024), 2),
                        "created_time": datetime.fromtimestamp(
                            file_stat.st_ctime
                        ).isoformat(),
                        "age_hours": round(
                            (time.time() - file_stat.st_ctime) / 3600, 1
                        ),
                    }
                )

        # En yeni backup'lar √∂nce
        backup_files.sort(key=lambda x: x["created_time"], reverse=True)

        return {
            "success": True,
            "data": {
                "backups": backup_files,
                "total_backups": len(backup_files),
                "total_size_mb": round(
                    sum(b["size_bytes"] for b in backup_files) / (1024 * 1024), 2
                ),
            },
            "timestamp": datetime.now(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="BackupListError",
            message=str(e),
            endpoint="/api/sync/backup-list",
        )
        raise HTTPException(status_code=500, detail=f"Backup listesi hatasƒ±: {str(e)}")


# ÔøΩüë• PERSONEL ENDPOINTS
@app.get("/api/personnel")
async def get_personnel():
    """T√ºm personel listesini d√∂nd√ºr√ºr"""
    try:
        return {
            "success": True,
            "data": personnel_data,
            "total": len(personnel_data),
            "timestamp": datetime.now(),
        }
    except Exception as e:
        print(f"‚ùå Personel listesi getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Personel listesi getirme hatasƒ±: {str(e)}"
        )


@app.get("/api/personnel/{personnel_id}")
async def get_personnel_by_id(personnel_id: int):
    """Belirli bir personelin bilgilerini d√∂nd√ºr√ºr"""
    try:
        # üöÄ PERFORMANS: Index'den O(1) eri≈üim
        personnel = personnel_index.get(personnel_id)
        if not personnel:
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")

        return {"success": True, "data": personnel, "timestamp": datetime.now()}
    except HTTPException:
        raise  # HTTP hatalarƒ±nƒ± yeniden fƒ±rlat
    except Exception as e:
        print(f"‚ùå Personel getirme hatasƒ± (ID: {personnel_id}): {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Personel getirme hatasƒ±: {str(e)}"
        )


@app.post("/api/personnel")
async def create_personnel(personnel: dict):
    """Yeni personel ekler"""
    try:
        print(f"üì• Gelen personel verisi: {personnel}")

        # Gerekli alanlarƒ± kontrol et
        if not personnel.get("name") or not personnel["name"].strip():
            raise HTTPException(status_code=400, detail="Personel adƒ± gerekli")

        if not personnel.get("username") or not personnel["username"].strip():
            raise HTTPException(status_code=400, detail="Kullanƒ±cƒ± adƒ± gerekli")

        if not personnel.get("hire_date"):
            raise HTTPException(status_code=400, detail="ƒ∞≈üe giri≈ü tarihi gerekli")

        if not personnel.get("team"):
            raise HTTPException(status_code=400, detail="Ekip se√ßimi gerekli")

        # Kullanƒ±cƒ± adƒ± benzersizlik kontrol√º
        for existing in personnel_data:
            if (
                existing.get("username", "").lower()
                == personnel["username"].strip().lower()
            ):
                raise HTTPException(
                    status_code=400, detail="Bu kullanƒ±cƒ± adƒ± zaten kullanƒ±lƒ±yor"
                )

        # Global max ID'yi g√ºncelle
        global _max_personnel_id
        _max_personnel_id += 1
        new_id = _max_personnel_id

        # Yeni personel objesi olu≈ütur
        new_personnel = {
            "id": new_id,
            "name": personnel["name"].strip(),
            "username": personnel["username"].strip(),
            "email": personnel.get("email", "").strip(),
            "reference": personnel.get("reference", "").strip(),
            "hire_date": personnel["hire_date"],
            "team": personnel["team"],
            "promotion_date": personnel.get("promotion_date", ""),
            "department": "Genel",  # Mevcut sistem uyumluluƒüu i√ßin
            "position": "√áalƒ±≈üan",  # Mevcut sistem uyumluluƒüu i√ßin
            "phone": "",  # Mevcut sistem uyumluluƒüu i√ßin
            "daily_targets": {
                "uye_adedi": 100,
                "whatsapp_adedi": 50,
                "cihaz_adedi": 30,
                "whatsapp_cevapsiz": 5,
            },
            "status": "active",
        }

        # Thread-safe veri ekleme
        with data_lock:
            personnel_data.append(new_personnel)
            personnel_index[new_id] = new_personnel

        print(f"‚úÖ Personel eklendi: {new_personnel}")
        print(f"üìä Toplam personel sayƒ±sƒ±: {len(personnel_data)}")

        return {
            "success": True,
            "message": "Personel ba≈üarƒ±yla eklendi",
            "data": new_personnel,
            "timestamp": datetime.now(),
        }

    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"‚ùå Personel ekleme hatasƒ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Personel ekleme hatasƒ±: {str(e)}")


# @app.put("/api/personnel/{personnel_id}")
# async def update_personnel_admin(personnel_id: int, personnel: dict, user: dict = Depends(verify_admin_access), request: Request = None):
#     """Personel g√ºnceller - üîí Admin yetkisi gerekli"""
#     # üîí Rate limiting kontrol√º
#     if request:
#         await check_rate_limit(request)
#
#     try:
#         print(f"üìù Personel g√ºncelleme isteƒüi: {personnel_id} (User: {user['name']})")
#
#         # üîí Input sanitization
#         if "name" in personnel:
#             personnel["name"] = sanitize_string(personnel["name"])
#         if "department" in personnel:
#             personnel["department"] = sanitize_string(personnel["department"])
#         if "position" in personnel:
#             personnel["position"] = sanitize_string(personnel["position"])
#         if "email" in personnel:
#             personnel["email"] = sanitize_string(personnel["email"])
#         if "phone" in personnel:
#             personnel["phone"] = sanitize_string(personnel["phone"])
#             personnel["phone"] = sanitize_string(personnel["phone"])
#
#         # üîç VERƒ∞ DOƒûRULAMA
#         # Email doƒürulama (varsa)
#         if "email" in personnel and personnel["email"]:
#             import re
#             email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
#             if not re.match(email_pattern, personnel["email"]):
#                 raise HTTPException(status_code=400, detail="Ge√ßersiz email formatƒ±")
#
#         # Telefon doƒürulama (varsa)
#         if "phone" in personnel and personnel["phone"]:
#             phone_clean = re.sub(r'[^\d]', '', personnel["phone"])
#             if len(phone_clean) < 10 or len(phone_clean) > 11:
#                 raise HTTPException(status_code=400, detail="Ge√ßersiz telefon formatƒ±")
#
#         # ƒ∞sim doƒürulama (varsa)
#         if "name" in personnel and personnel["name"]:
#             if len(personnel["name"].strip()) < 2:
#                 raise HTTPException(status_code=400, detail="ƒ∞sim en az 2 karakter olmalƒ±")
#
#         # üöÄ PERFORMANS: Index'den O(1) eri≈üim
#         existing_personnel = personnel_index.get(personnel_id)
#         if not existing_personnel:
#             raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")
#
#         # Mevcut personeli g√ºncelle
#         personnel["id"] = personnel_id
#         personnel["status"] = existing_personnel.get("status", "active")
#         # hire_date koru
#         if "hire_date" not in personnel:
#             personnel["hire_date"] = existing_personnel.get("hire_date", str(date.today()))
#
#         # üöÄ PERFORMANS: Hem liste hem index'i g√ºncelle
#         for i, p in enumerate(personnel_data):
#             if p["id"] == personnel_id:
#                 personnel_data[i] = personnel
#                 break
#         personnel_index[personnel_id] = personnel
#
#         return {
#             "success": True,
#             "message": "Personel ba≈üarƒ±yla g√ºncellendi",
#             "data": personnel,
#             "timestamp": datetime.now()
#         }
#     except HTTPException:
#         raise  # HTTP hatalarƒ±nƒ± yeniden fƒ±rlat
#     except Exception as e:
#         print(f"‚ùå Personel g√ºncelleme hatasƒ± (ID: {personnel_id}): {str(e)}")
#         raise HTTPException(status_code=500, detail=f"Personel g√ºncelleme hatasƒ±: {str(e)}")


@app.delete("/api/personnel/{personnel_id}")
async def delete_personnel(personnel_id: int, request: Request = None):
    """Personel siler - Ge√ßici olarak admin yetkisi kaldƒ±rƒ±ldƒ±"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(f"üóëÔ∏è Personel silme isteƒüi: {personnel_id}")

        # üöÄ PERFORMANS: Index'den O(1) kontrol
        personnel_to_delete = personnel_index.get(personnel_id)
        if not personnel_to_delete:
            print(f"‚ùå Personel bulunamadƒ±: {personnel_id}")
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")

        print(f" Mevcut personel sayƒ±sƒ±: {len(personnel_data)}")

        # Listeden sil
        for i, p in enumerate(personnel_data):
            if p["id"] == personnel_id:
                deleted = personnel_data.pop(i)
                break

        # üöÄ PERFORMANS: Index'den sil
        del personnel_index[personnel_id]

        # ƒ∞lgili daily records'larƒ± da sil (opsiyonel - veri b√ºt√ºnl√ºƒü√º i√ßin)
        daily_records_data[:] = [
            r for r in daily_records_data if r["personnel_id"] != personnel_id
        ]
        rebuild_daily_records_index()

        # ƒ∞lgili targets'larƒ± da sil
        targets_data[:] = [t for t in targets_data if t["personnel_id"] != personnel_id]
        rebuild_targets_index()

        print(f"‚úÖ Personel silindi: {personnel_to_delete.get('name')}")

        # Business event logla
        slogger.log_business_event(
            event_type="personnel_deleted",
            description=f"Personnel deleted: {personnel_to_delete.get('name')}",
            data={
                "personnel_id": personnel_id,
                "name": personnel_to_delete.get("name"),
            },
            user_id="system",
        )

        return {
            "success": True,
            "message": f"Personel {personnel_to_delete.get('name', 'Unknown')} ba≈üarƒ±yla silindi",
            "data": {"deleted_personnel": personnel_to_delete},
            "timestamp": datetime.now(),
        }

    except HTTPException:
        raise  # HTTP hatalarƒ±nƒ± yeniden fƒ±rlat
    except Exception as e:
        print(f"‚ùå Personel silme hatasƒ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Personel silme hatasƒ±: {str(e)}")


# Duplicate get_personnel_by_id route removed; single canonical definition kept above


@app.put("/api/personnel/{personnel_id}")
async def update_personnel(personnel_id: int, personnel: dict, request: Request = None):
    """Personel bilgilerini g√ºnceller"""
    # Ge√ßici olarak rate limiting kaldƒ±rƒ±ldƒ±
    # if request:
    #     await check_rate_limit(request)

    try:
        print(f"üìù Personel g√ºncelleme isteƒüi: {personnel_id}")
        print(f"üì• G√ºncelleme verisi: {personnel}")

        # Personeli bul
        personnel_to_update = personnel_index.get(personnel_id)
        if not personnel_to_update:
            print(f"‚ùå Personel bulunamadƒ±: {personnel_id}")
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")

        # Gerekli alanlarƒ± kontrol et
        if not personnel.get("name") or not personnel["name"].strip():
            raise HTTPException(status_code=400, detail="Personel adƒ± gerekli")

        if not personnel.get("username") or not personnel["username"].strip():
            raise HTTPException(status_code=400, detail="Kullanƒ±cƒ± adƒ± gerekli")

        if not personnel.get("hire_date"):
            raise HTTPException(status_code=400, detail="ƒ∞≈üe giri≈ü tarihi gerekli")

        if not personnel.get("team"):
            raise HTTPException(status_code=400, detail="Ekip se√ßimi gerekli")

        # Kullanƒ±cƒ± adƒ± benzersizlik kontrol√º (kendisi hari√ß)
        for existing in personnel_data:
            if (
                existing.get("username", "").lower()
                == personnel["username"].strip().lower()
                and existing["id"] != personnel_id
            ):
                raise HTTPException(
                    status_code=400, detail="Bu kullanƒ±cƒ± adƒ± zaten kullanƒ±lƒ±yor"
                )

        # Personel bilgilerini g√ºncelle
        updated_personnel = {
            **personnel_to_update,  # Mevcut bilgileri koru
            "name": personnel["name"].strip(),
            "username": personnel["username"].strip(),
            "email": personnel.get("email", "").strip(),
            "reference": personnel.get("reference", "").strip(),
            "hire_date": personnel["hire_date"],
            "team": personnel["team"],
            "promotion_date": personnel.get("promotion_date", ""),
        }

        # Thread-safe g√ºncelleme
        with data_lock:
            # Listede g√ºncelle
            for i, p in enumerate(personnel_data):
                if p["id"] == personnel_id:
                    personnel_data[i] = updated_personnel
                    break

            # Index'te g√ºncelle
            personnel_index[personnel_id] = updated_personnel

        print(f"‚úÖ Personel g√ºncellendi: {updated_personnel}")

        # Business event logla
        slogger.log_business_event(
            event_type="personnel_updated",
            description=f"Personnel updated: {updated_personnel.get('name')}",
            data={"personnel_id": personnel_id, "name": updated_personnel.get("name")},
            user_id="system",
        )

        return {
            "success": True,
            "message": "Personel ba≈üarƒ±yla g√ºncellendi",
            "data": updated_personnel,
            "timestamp": datetime.now(),
        }

    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"‚ùå Personel g√ºncelleme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Personel g√ºncelleme hatasƒ±: {str(e)}"
        )


# üìä G√úNL√úK KAYIT ENDPOINTS
@app.get("/api/daily-records")
async def get_daily_records(
    date_filter: Optional[str] = None,
    personnel_id: Optional[int] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
):
    """G√ºnl√ºk kayƒ±tlarƒ± d√∂nd√ºr√ºr"""
    try:
        filtered_records = daily_records_data

        # Tek tarih e≈üle≈ümesi (geri uyumluluk)
        if date_filter:
            filtered_records = [r for r in filtered_records if r["date"] == date_filter]

        # Tarih aralƒ±ƒüƒ± filtresi (YYYY-MM-DD string kar≈üƒ±la≈ütƒ±rmasƒ± yeterli)
        if start_date or end_date:
            tmp = []
            for r in filtered_records:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            filtered_records = tmp

        if personnel_id:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            filtered_records = [r for r in filtered_records if r["personnel_id"] == pid]

        return {
            "success": True,
            "data": filtered_records,
            "total": len(filtered_records),
            "filters": {
                "date": date_filter,
                "start_date": start_date,
                "end_date": end_date,
                "personnel_id": personnel_id,
            },
            "timestamp": datetime.now(),
        }
    except Exception as e:
        print(f"‚ùå G√ºnl√ºk kayƒ±tlarƒ± getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"G√ºnl√ºk kayƒ±tlarƒ± getirme hatasƒ±: {str(e)}"
        )


@app.get("/api/daily-records/{record_id}")
async def get_daily_record(record_id: int):
    """Tek g√ºnl√ºk kaydƒ± d√∂nd√ºr√ºr"""
    try:
        for r in daily_records_data:
            if r["id"] == record_id:
                return {"success": True, "data": r, "timestamp": datetime.now()}
        raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå G√ºnl√ºk kayƒ±t getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"G√ºnl√ºk kayƒ±t getirme hatasƒ±: {str(e)}"
        )


@app.post("/api/daily-records")
async def create_daily_record(record: dict):
    """Yeni √ßaƒürƒ± puanƒ± kaydƒ± ekler"""
    try:
        print(f"üìù Yeni √ßaƒürƒ± puanƒ± kaydƒ± ekleme isteƒüi alƒ±ndƒ±: {record}")

        # üöÄ PERFORMANS: Cache'den max ID + 1
        global _max_daily_record_id
        _max_daily_record_id += 1
        new_id = _max_daily_record_id

        # üîç VERƒ∞ DOƒûRULAMA
        # Gerekli alanlarƒ± kontrol et
        if "personnel_id" not in record or record["personnel_id"] is None:
            raise HTTPException(status_code=400, detail="personnel_id gerekli")
        if "call_number" not in record or not record["call_number"]:
            raise HTTPException(status_code=400, detail="call_number gerekli")
        if "score" not in record or record["score"] is None:
            raise HTTPException(status_code=400, detail="score gerekli")

        # Personel ID doƒürulama - var mƒ±?
        try:
            personnel_id = int(record["personnel_id"])
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="personnel_id sayƒ± olmalƒ±")

        # üöÄ PERFORMANS: Index'den O(1) kontrol
        personnel_exists = personnel_id in personnel_index
        if not personnel_exists:
            raise HTTPException(
                status_code=400, detail="Ge√ßersiz personnel_id - personel bulunamadƒ±"
            )

        # Score doƒürulama (0-100 arasƒ±)
        try:
            score = int(record["score"])
            if score < 0 or score > 100:
                raise HTTPException(
                    status_code=400, detail="Score 0-100 arasƒ±nda olmalƒ±"
                )
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="Score sayƒ± olmalƒ±")

        # Call number doƒürulama
        call_number = str(record["call_number"]).strip()
        if len(call_number) < 3:
            raise HTTPException(
                status_code=400, detail="Call number en az 3 karakter olmalƒ±"
            )

        # Tarih doƒürulama (varsa)
        record_date = record.get("date", str(date.today()))
        try:
            datetime.strptime(record_date, "%Y-%m-%d")
        except ValueError:
            raise HTTPException(
                status_code=400, detail="Ge√ßersiz tarih formatƒ± (YYYY-MM-DD olmalƒ±)"
            )

        new_record = {
            "id": new_id,
            "date": record_date,
            "personnel_id": personnel_id,
            "call_number": call_number,
            "score": score,
            "notes": record.get("notes", "").strip(),
        }

        daily_records_data.append(new_record)
        # üöÄ PERFORMANS: Index'i g√ºncelle
        daily_records_index[new_id] = new_record

        print(f"‚úÖ Yeni √ßaƒürƒ± puanƒ± kaydƒ± eklendi: {new_record}")

        return {
            "success": True,
            "data": new_record,
            "message": "√áaƒürƒ± puanƒ± kaydƒ± ba≈üarƒ±yla eklendi",
            "timestamp": datetime.now(),
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå √áaƒürƒ± puanƒ± kaydƒ± ekleme hatasƒ±: {e}")
        raise HTTPException(status_code=500, detail="Kayƒ±t eklenemedi")


@app.put("/api/daily-records/{record_id}")
async def update_daily_record(record_id: int, record: dict):
    """√áaƒürƒ± puanƒ± kaydƒ±nƒ± g√ºnceller"""
    try:
        print(f"üìù √áaƒürƒ± puanƒ± g√ºncelleme isteƒüi: ID={record_id}, Data={record}")

        for i, r in enumerate(daily_records_data):
            if r["id"] == record_id:
                # √áaƒürƒ± puanƒ± sistemi i√ßin gerekli alanlarƒ± kontrol et
                updated_record = {
                    "id": record_id,
                    "date": record.get("date", r["date"]),
                    "personnel_id": record.get("personnel_id", r["personnel_id"]),
                    "call_number": record.get("call_number", r["call_number"]),
                    "score": int(record.get("score", r["score"])),
                    "notes": record.get("notes", r["notes"]),
                }

                daily_records_data[i] = updated_record

                print(f"‚úÖ √áaƒürƒ± puanƒ± kaydƒ± ba≈üarƒ±yla g√ºncellendi: ID={record_id}")

                return {
                    "success": True,
                    "message": "√áaƒürƒ± puanƒ± kaydƒ± ba≈üarƒ±yla g√ºncellendi",
                    "data": updated_record,
                    "timestamp": datetime.now(),
                }

        print(f"‚ùå Kayƒ±t bulunamadƒ±: ID={record_id}")
        raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")

    except Exception as e:
        print(f"‚ùå Kayƒ±t g√ºncelleme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Kayƒ±t g√ºncelleme hatasƒ±: {str(e)}"
        )


@app.delete("/api/daily-records/{record_id}")
async def delete_daily_record(record_id: int):
    """G√ºnl√ºk kayƒ±t siler"""
    for i, r in enumerate(daily_records_data):
        if r["id"] == record_id:
            deleted = daily_records_data.pop(i)
            return {
                "success": True,
                "data": {"id": record_id},
                "message": "G√ºnl√ºk kayƒ±t ba≈üarƒ±yla silindi",
                "timestamp": datetime.now(),
            }

    raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")


# üéØ HEDEF ENDPOINTS
@app.get("/api/targets")
async def get_targets():
    """T√ºm hedef listesini d√∂nd√ºr√ºr"""
    try:
        return {
            "success": True,
            "data": targets_data,
            "total": len(targets_data),
            "timestamp": datetime.now(),
        }
    except Exception as e:
        print(f"‚ùå Hedefler getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Hedefler getirme hatasƒ±: {str(e)}"
        )


@app.post("/api/targets")
async def create_target(
    target: dict, user: dict = Depends(verify_admin_access), request: Request = None
):
    """Yeni hedef ekler - üîí Admin yetkisi gerekli"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(f"üì• Gelen hedef verisi: {target} (User: {user['name']})")

        # üîí Input sanitization
        if "target_type" in target:
            target["target_type"] = sanitize_string(target["target_type"])

        # üöÄ PERFORMANS: Cache'den max ID + 1
        global _max_target_id
        _max_target_id += 1
        new_id = _max_target_id

        # üîç VERƒ∞ DOƒûRULAMA
        # Gerekli alanlarƒ± kontrol et
        if "personnel_id" not in target or target["personnel_id"] is None:
            raise HTTPException(status_code=400, detail="personnel_id gerekli")
        if "target_value" not in target or target["target_value"] is None:
            raise HTTPException(status_code=400, detail="target_value gerekli")

        # Personel ID doƒürulama
        try:
            personnel_id = int(target["personnel_id"])
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="personnel_id sayƒ± olmalƒ±")

        # üöÄ PERFORMANS: Index'den O(1) kontrol
        personnel_exists = personnel_id in personnel_index
        if not personnel_exists:
            raise HTTPException(
                status_code=400, detail="Ge√ßersiz personnel_id - personel bulunamadƒ±"
            )

        # Target value doƒürulama (pozitif sayƒ±)
        try:
            target_value = int(target["target_value"])
            if target_value <= 0:
                raise HTTPException(
                    status_code=400, detail="target_value pozitif bir sayƒ± olmalƒ±"
                )
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="target_value sayƒ± olmalƒ±")

        # Target type doƒürulama
        valid_target_types = [
            "uye_adedi",
            "whatsapp_adedi",
            "cihaz_adedi",
            "whatsapp_cevapsiz",
        ]
        target_type = target.get("target_type", "uye_adedi")
        if target_type not in valid_target_types:
            raise HTTPException(
                status_code=400,
                detail=f"target_type ≈üunlardan biri olmalƒ±: {', '.join(valid_target_types)}",
            )

        # Tarih doƒürulama
        start_date = target.get("start_date", str(date.today()))
        end_date = target.get("end_date", str(date.today()))

        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            if end_dt < start_dt:
                raise HTTPException(
                    status_code=400, detail="end_date start_date'den √∂nce olamaz"
                )
        except ValueError as e:
            raise HTTPException(
                status_code=400,
                detail=f"Ge√ßersiz tarih formatƒ± (YYYY-MM-DD olmalƒ±): {str(e)}",
            )

        new_target = {
            "id": new_id,
            "personnel_id": personnel_id,
            "target_type": target_type,
            "target_value": target_value,
            "start_date": start_date,
            "end_date": end_date,
        }

        targets_data.append(new_target)
        # üöÄ PERFORMANS: Index'i g√ºncelle
        targets_index[new_id] = new_target

        print(f"‚úÖ Hedef eklendi: {new_target}")

        return {
            "success": True,
            "message": "Hedef ba≈üarƒ±yla eklendi",
            "data": new_target,
            "timestamp": datetime.now(),
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Hedef ekleme hatasƒ±: {e}")
        raise HTTPException(status_code=500, detail="Hedef eklenemedi")


@app.put("/api/targets/{target_id}")
async def update_target(
    target_id: int,
    target: dict,
    user: dict = Depends(verify_admin_access),
    request: Request = None,
):
    """Hedef g√ºnceller - üîí Admin yetkisi gerekli"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(
            f"üìù Hedef g√ºncelleme isteƒüi: ID={target_id}, Data={target} (User: {user['name']})"
        )

        # üîí Input sanitization
        if "target_type" in target:
            target["target_type"] = sanitize_string(target["target_type"])

        for i, t in enumerate(targets_data):
            if t["id"] == target_id:
                updated_target = {
                    "id": target_id,
                    "personnel_id": target.get("personnel_id", t["personnel_id"]),
                    "target_type": target.get("target_type", t["target_type"]),
                    "target_value": int(target.get("target_value", t["target_value"])),
                    "start_date": target.get("start_date", t["start_date"]),
                    "end_date": target.get("end_date", t["end_date"]),
                }

                targets_data[i] = updated_target

                print(f"‚úÖ Hedef ba≈üarƒ±yla g√ºncellendi: ID={target_id}")

                return {
                    "success": True,
                    "message": "Hedef ba≈üarƒ±yla g√ºncellendi",
                    "data": updated_target,
                    "timestamp": datetime.now(),
                }

        print(f"‚ùå Hedef bulunamadƒ±: ID={target_id}")
        raise HTTPException(status_code=404, detail="Hedef bulunamadƒ±")

    except Exception as e:
        print(f"‚ùå Hedef g√ºncelleme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Hedef g√ºncelleme hatasƒ±: {str(e)}"
        )


@app.delete("/api/targets/{target_id}")
async def delete_target(
    target_id: int, user: dict = Depends(verify_admin_access), request: Request = None
):
    """Hedef siler - üîí Admin yetkisi gerekli"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(f"üóëÔ∏è Hedef silme isteƒüi: {target_id} (User: {user['name']})")

        for i, t in enumerate(targets_data):
            if t["id"] == target_id:
                deleted = targets_data.pop(i)
                # üöÄ PERFORMANS: Index'den sil
                if target_id in targets_index:
                    del targets_index[target_id]
                return {
                    "success": True,
                    "data": {"id": target_id},
                    "message": "Hedef ba≈üarƒ±yla silindi",
                    "timestamp": datetime.now(),
                }

        raise HTTPException(status_code=404, detail="Hedef bulunamadƒ±")

    except HTTPException:
        raise  # HTTP hatalarƒ±nƒ± yeniden fƒ±rlat
    except Exception as e:
        print(f"‚ùå Hedef silme hatasƒ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Hedef silme hatasƒ±: {str(e)}")


# üìà ANALƒ∞Tƒ∞K ENDPOINTS
@app.get("/api/analytics/summary")
async def get_analytics_summary():
    """Genel analitik √∂zet"""
    try:
        today = str(date.today())
        today_records = [r for r in daily_records_data if r["date"] == today]

        # √áaƒürƒ± puanƒ± ortalama hesaplama
        total_score = 0
        if today_records:
            total_score = sum([r.get("score", 0) for r in today_records]) / len(
                today_records
            )

        # Departman bazlƒ± analiz
        dept_performance = {}
        for record in today_records:
            personnel = next(
                (p for p in personnel_data if p["id"] == record["personnel_id"]), None
            )
            if personnel and "department" in personnel:
                dept = personnel["department"]
                if dept not in dept_performance:
                    dept_performance[dept] = []
                dept_performance[dept].append(record.get("score", 0))

        for dept in dept_performance:
            if dept_performance[dept]:  # Bo≈ü liste kontrol√º
                dept_performance[dept] = round(
                    sum(dept_performance[dept]) / len(dept_performance[dept]), 2
                )
            else:
                dept_performance[dept] = 0

        return {
            "success": True,
            "data": {
                "summary": {
                    "total_personnel": len(personnel_data),
                    "active_personnel": len(
                        [p for p in personnel_data if p.get("status") == "active"]
                    ),
                    "today_average_score": round(total_score, 2),
                    "total_call_evaluations_today": len(today_records),
                    "departments": len(
                        set(
                            [
                                p.get("department", "Unknown")
                                for p in personnel_data
                                if "department" in p
                            ]
                        )
                    ),
                },
                "department_performance": dept_performance,
                "call_score_stats": {
                    "highest_score": max([r.get("score", 0) for r in today_records])
                    if today_records
                    else 0,
                    "lowest_score": min([r.get("score", 0) for r in today_records])
                    if today_records
                    else 0,
                    "total_calls_evaluated": len(today_records),
                },
            },
            "timestamp": datetime.now(),
        }

    except Exception as e:
        print(f"‚ùå Analytics summary hatasƒ±: {str(e)}")
        return {
            "success": False,
            "data": {
                "summary": {
                    "total_personnel": 0,
                    "active_personnel": 0,
                    "today_average_score": 0,
                    "total_call_evaluations_today": 0,
                    "departments": 0,
                }
            },
            "error": str(e),
            "timestamp": datetime.now(),
        }


@app.get("/api/analytics/performance-trend")
async def get_performance_trend():
    """Performans trend verilerini d√∂nd√ºr√ºr"""
    try:
        # Son 7 g√ºn√ºn √ßaƒürƒ± puanlarƒ±
        recent_data = []
        for i in range(7):
            check_date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            day_records = [r for r in daily_records_data if r["date"] == check_date]

            avg_score = 0
            if day_records:
                avg_score = sum([r.get("score", 0) for r in day_records]) / len(
                    day_records
                )

            recent_data.append(
                {
                    "date": check_date,
                    "average_score": round(avg_score, 2),
                    "total_evaluations": len(day_records),
                }
            )

        return {
            "success": True,
            "data": {"trend_data": recent_data, "period": "7_days"},
            "timestamp": datetime.now(),
        }

    except Exception as e:
        print(f"‚ùå Performance trend hatasƒ±: {str(e)}")
        return {"success": False, "error": str(e), "timestamp": datetime.now()}


# üéØ PERFORMANS TABLOSU ENDPOINTS
@app.get("/api/performance-records")
async def get_performance_records(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """T√ºm performans kayƒ±tlarƒ±nƒ± getir - Tarih aralƒ±ƒüƒ± ve personel filtresi ile"""
    try:
        print(
            f"üìä Performance records listeleniyor, toplam kayƒ±t: {len(performance_data)}"
        )
        print(
            f"üîç Filtreler - start_date: {start_date}, end_date: {end_date}, personnel_id: {personnel_id}"
        )

        # Filtreleme uygula
        filtered_records = performance_data.copy()

        # Tarih aralƒ±ƒüƒ± filtresi
        if start_date or end_date:
            filtered_by_date = []
            for record in filtered_records:
                record_date = record.get("date")
                if record_date:
                    # Tarih kar≈üƒ±la≈ütƒ±rmasƒ±
                    if start_date and record_date < start_date:
                        continue
                    if end_date and record_date > end_date:
                        continue
                    filtered_by_date.append(record)
            filtered_records = filtered_by_date
            print(f"üìÖ Tarih filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel filtresi
        if personnel_id:
            filtered_records = [
                r for r in filtered_records if r.get("personnel_id") == personnel_id
            ]
            print(f"üë§ Personel filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel isimlerini dahil et
        enriched_records = []
        for record in filtered_records:
            personnel = next(
                (p for p in personnel_data if p["id"] == record["personnel_id"]), None
            )
            enriched_record = record.copy()
            enriched_record["personnel_name"] = (
                personnel["name"] if personnel else "Bilinmeyen"
            )
            enriched_records.append(enriched_record)

        return {
            "success": True,
            "data": enriched_records,
            "timestamp": datetime.now(),
            "filters": {
                "start_date": start_date,
                "end_date": end_date,
                "personnel_id": personnel_id,
                "total_filtered": len(enriched_records),
                "total_available": len(performance_data),
            },
        }

    except Exception as e:
        print(f"‚ùå Performance records getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Performance records getirilemedi: {str(e)}"
        )


@app.post("/api/performance-records")
async def add_performance_record(request: Request):
    """Yeni performans kaydƒ± ekle - Admin yetkisi KALDIRILDI ge√ßici olarak"""
    try:
        print(f"üìù Performans kaydƒ± ekleme isteƒüi")
        # Request body'yi al - UTF-8 encoding ile g√ºvenli ≈üekilde
        body = await request.body()
        try:
            # √ñnce UTF-8 ile dene
            request_data = json.loads(body.decode("utf-8"))
        except UnicodeDecodeError:
            # UTF-8 ba≈üarƒ±sƒ±z olursa latin-1 ile dene
            try:
                request_data = json.loads(body.decode("latin-1"))
            except:
                # Son √ßare olarak errors='ignore' ile
                request_data = json.loads(body.decode("utf-8", errors="ignore"))

        print(f"üìù Gelen performans verisi: {request_data}")

        # Gerekli alanlarƒ± kontrol et - frontend compatibility i√ßin mapping
        date = request_data.get("date") or request_data.get("performance_date")
        personnel_id = request_data.get("personnel_id") or request_data.get(
            "performance_personnel_id"
        )
        member_count = request_data.get("member_count", 0)
        whatsapp_count = request_data.get("whatsapp_count", 0)
        device_count = request_data.get("device_count", 0)
        unanswered_count = request_data.get("unanswered_count", 0)
        knowledge_duel_result = request_data.get("knowledge_duel_result", "")
        reward_penalty = request_data.get("reward_penalty", "")

        if not personnel_id:
            raise HTTPException(status_code=400, detail="Personnel ID gerekli")

        if not date:
            raise HTTPException(status_code=400, detail="Tarih gerekli")

        # Personeli kontrol et
        try:
            personnel_id = int(personnel_id)
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="Personnel ID sayƒ± olmalƒ±")

        # üöÄ PERFORMANS: Index'den O(1) kontrol
        if personnel_id not in personnel_index:
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")

        personnel = personnel_index[personnel_id]

        # Yeni performans kaydƒ± olu≈ütur
        new_record = {
            "id": len(performance_data) + 1,
            "personnel_id": personnel_id,
            "personnel_name": personnel.get("name", "Bilinmeyen"),
            "member_count": int(member_count) if member_count else 0,
            "whatsapp_count": int(whatsapp_count) if whatsapp_count else 0,
            "device_count": int(device_count) if device_count else 0,
            "unanswered_count": int(unanswered_count) if unanswered_count else 0,
            "knowledge_duel_result": str(knowledge_duel_result).strip(),
            "reward_penalty": str(reward_penalty).strip(),
            "date": str(date),
            "created_at": datetime.now().isoformat(),
        }

        # Thread-safe veri ekleme
        with data_lock:
            performance_data.append(new_record)

        print(f"‚úÖ Performans kaydƒ± ba≈üarƒ±yla eklendi: {personnel['name']}")

        # Business event logla
        slogger.log_business_event(
            event_type="performance_record_created",
            description=f"Performance record created for {personnel['name']}",
            data={"personnel_id": personnel_id, "record_id": new_record["id"]},
        )

        return {
            "success": True,
            "data": new_record,
            "message": "Performans kaydƒ± ba≈üarƒ±yla eklendi",
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Performans kaydƒ± ekleme hatasƒ±: {str(e)}")
        slogger.log_error(
            error_type="PerformanceRecordCreationError",
            message=f"Performance record creation failed: {str(e)}",
            endpoint="/api/performance-records",
        )
        raise HTTPException(
            status_code=500, detail=f"Performans kaydƒ± eklenemedi: {str(e)}"
        )


@app.put("/api/performance-records/{record_id}")
async def update_performance_record(
    record_id: int, request: Request, user: dict = Depends(verify_admin_access)
):
    """Performans kaydƒ±nƒ± g√ºncelle - üîí Admin yetkisi gerekli"""
    # üîí Rate limiting kontrol√º
    await check_rate_limit(request)

    try:
        print(
            f"üìù Performans kaydƒ± g√ºncelleme isteƒüi: {record_id} (User: {user['name']})"
        )
        # Request body'yi al - UTF-8 encoding ile g√ºvenli ≈üekilde
        body = await request.body()
        try:
            # √ñnce UTF-8 ile dene
            request_data = json.loads(body.decode("utf-8"))
        except UnicodeDecodeError:
            # UTF-8 ba≈üarƒ±sƒ±z olursa latin-1 ile dene
            try:
                request_data = json.loads(body.decode("latin-1"))
            except:
                # Son √ßare olarak errors='ignore' ile
                request_data = json.loads(body.decode("utf-8", errors="ignore"))

        # Kaydƒ± bul
        record = next((r for r in performance_data if r["id"] == record_id), None)
        if not record:
            raise HTTPException(status_code=404, detail="Performans kaydƒ± bulunamadƒ±")

        # G√ºncelleme verilerini uygula
        if "personnel_id" in request_data:
            try:
                pid = int(request_data["personnel_id"])
                record["personnel_id"] = pid
                # personnel_name'i da g√ºncelle
                if pid in personnel_index:
                    record["personnel_name"] = personnel_index[pid].get(
                        "name", record.get("personnel_name", "Bilinmeyen")
                    )
            except Exception:
                pass
        if "member_count" in request_data:
            record["member_count"] = int(request_data["member_count"])
        if "whatsapp_count" in request_data:
            record["whatsapp_count"] = int(request_data["whatsapp_count"])
        if "device_count" in request_data:
            record["device_count"] = int(request_data["device_count"])
        if "unanswered_count" in request_data:
            record["unanswered_count"] = int(request_data["unanswered_count"])
        if "knowledge_duel_result" in request_data:
            record["knowledge_duel_result"] = request_data["knowledge_duel_result"]
        if "reward_penalty" in request_data:
            record["reward_penalty"] = request_data["reward_penalty"]
        # Tarih g√ºncellemesi (destekle)
        if "date" in request_data or "performance_date" in request_data:
            record["date"] = str(
                request_data.get("date")
                or request_data.get("performance_date")
                or record.get("date")
            )

        record["updated_at"] = datetime.now().isoformat()

        print(f"‚úÖ Performans kaydƒ± g√ºncellendi: ID={record_id}")

        return {
            "success": True,
            "data": record,
            "message": "Performans kaydƒ± ba≈üarƒ±yla g√ºncellendi",
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Performans kaydƒ± g√ºncelleme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Performans kaydƒ± g√ºncellenemedi: {str(e)}"
        )


@app.get("/api/performance-records/{record_id}")
async def get_performance_record(record_id: int, request: Request = None):
    """Tek performans kaydƒ±nƒ± getir"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(f"üìñ Performans kaydƒ± getirme isteƒüi: {record_id}")

        record = next((r for r in performance_data if r["id"] == record_id), None)
        if record is None:
            raise HTTPException(status_code=404, detail="Performans kaydƒ± bulunamadƒ±")

        return {
            "success": True,
            "data": record,
            "message": "Performans kaydƒ± ba≈üarƒ±yla getirildi",
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Performans kaydƒ± getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Performans kaydƒ± getirilemedi: {str(e)}"
        )


@app.delete("/api/performance-records/{record_id}")
async def delete_performance_record(record_id: int, request: Request = None):
    """Performans kaydƒ±nƒ± sil"""
    # üîí Rate limiting kontrol√º
    if request:
        await check_rate_limit(request)

    try:
        print(f"üóëÔ∏è Performans kaydƒ± silme isteƒüi: {record_id}")

        record_index = next(
            (i for i, r in enumerate(performance_data) if r["id"] == record_id), None
        )
        if record_index is None:
            raise HTTPException(status_code=404, detail="Performans kaydƒ± bulunamadƒ±")

        deleted_record = performance_data.pop(record_index)

        print(f"üóëÔ∏è Performans kaydƒ± silindi: ID={record_id}")

        return {
            "success": True,
            "data": {"id": record_id},
            "message": "Performans kaydƒ± ba≈üarƒ±yla silindi",
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Performans kaydƒ± silme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Performans kaydƒ± silinemedi: {str(e)}"
        )


@app.get("/api/export/personnel-excel")
async def export_personnel_to_excel():
    """Personel listesini Excel formatƒ±nda export yapar"""
    try:
        print(f"üìä Personel Excel export ba≈ülatƒ±lƒ±yor...")
        print(f"üë• Toplam personel sayƒ±sƒ±: {len(personnel_data)}")

        # Personel verilerini tablodaki sƒ±ralama ile d√ºzenle (As Ekip √ºstte, Paf Ekip altta)
        sorted_personnel = sorted(
            personnel_data,
            key=lambda x: (
                0 if x.get("team", "").lower() == "as ekip" else 1,  # As Ekip √∂nce
                x.get("name", "").lower(),  # Sonra alfabetik
            ),
        )

        # Excel i√ßin veri hazƒ±rlama
        excel_data = []
        for person in sorted_personnel:
            excel_data.append(
                {
                    "AD SOYAD": person.get("name", ""),
                    "KULLANICI ADI": person.get("username", ""),
                    "E-POSTA": person.get("email", ""),
                    "EKƒ∞P": person.get("team", ""),
                    "ƒ∞≈ûE Gƒ∞Rƒ∞≈û TARƒ∞Hƒ∞": person.get("hire_date", ""),
                    "REFERANS": person.get("reference", ""),
                    "TERFƒ∞ TARƒ∞Hƒ∞": person.get("promotion_date", ""),
                    "DEPARTMAN": person.get("department", "Genel"),
                    "POZƒ∞SYON": person.get("position", "√áalƒ±≈üan"),
                    "TELEFON": person.get("phone", ""),
                    "DURUM": person.get("status", "active"),
                }
            )

        # DataFrame olu≈ütur
        import pandas as pd

        df = pd.DataFrame(excel_data)

        # Excel dosyasƒ± i√ßin BytesIO buffer
        from io import BytesIO

        excel_buffer = BytesIO()

        # EN BASIT YOL: Sadece openpyxl engine kullan (xlsxwriter ile problem var)
        df.to_excel(
            excel_buffer, index=False, sheet_name="Personel Listesi", engine="openpyxl"
        )
        excel_buffer.seek(0)

        # Dosya adƒ±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"personel_listesi_{timestamp}.xlsx"

        print(f"‚úÖ Excel dosyasƒ± olu≈üturuldu: {filename}")
        print(f"üìä Toplam {len(excel_data)} personel export edildi")

        # Business event logla
        slogger.log_business_event(
            event_type="personnel_excel_export",
            description=f"Personnel data exported to Excel: {len(excel_data)} records",
            data={"filename": filename, "total_records": len(excel_data)},
            user_id="system",
        )

        # Response d√∂nd√ºr
        return Response(
            content=excel_buffer.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        print(f"‚ùå Personel Excel export hatasƒ±: {str(e)}")
        import traceback

        print(f"üîç Detaylƒ± hata: {traceback.format_exc()}")
        slogger.log_error(
            error_type="PersonnelExcelExportError",
            message=str(e),
            endpoint="/api/export/personnel-excel",
        )
        raise HTTPException(status_code=500, detail=f"Excel export hatasƒ±: {str(e)}")


# üìä DASHBOARD √ñZET EXCEL EXPORT
@app.get("/api/export/dashboard-summary-excel")
async def export_dashboard_summary_excel(
    start_date: Optional[str] = None, end_date: Optional[str] = None
):
    """Dashboard √∂zet tablosunu (ekip, personel, √ºye, whatsapp, cihaz, cevapsƒ±z, bilgi d√ºellosu toplamƒ±, √ßaƒürƒ± adedi, √ßaƒürƒ± puanƒ±)
    Excel olarak indirir. Tarih aralƒ±ƒüƒ± filtreleri performans ve √ßaƒürƒ± (daily_records) verilerine birlikte uygulanƒ±r.
    """
    try:
        print("üìä Dashboard summary Excel export ba≈ülƒ±yor...")
        print(f"üîç Filtreler - start_date: {start_date}, end_date: {end_date}")

        # Performans verilerini filtrele
        perf_filtered = performance_data.copy()
        if start_date or end_date:
            tmp = []
            for r in perf_filtered:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            perf_filtered = tmp
        print(f"üìà Performans kayƒ±tlarƒ± (filtreli): {len(perf_filtered)}")

        # √áaƒürƒ± (daily_records) verilerini filtrele
        calls_filtered = daily_records_data.copy()
        if start_date or end_date:
            tmp = []
            for r in calls_filtered:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            calls_filtered = tmp
        print(f"üìû √áaƒürƒ± kayƒ±tlarƒ± (filtreli): {len(calls_filtered)}")

        # Personel id -> ki≈üi bilgisi
        person_by_id = {p["id"]: p for p in personnel_data}

        # Performans agregasyonlarƒ±
        perfAgg: Dict[int, Dict[str, Any]] = {}

        def _as_num(x):
            try:
                if x is None or x == "":
                    return 0
                if isinstance(x, (int, float)):
                    if isinstance(x, float) and x != x:  # NaN
                        return 0
                    return x
                return float(str(x).strip())
            except Exception:
                return 0

        for r in perf_filtered:
            pid = r.get("personnel_id")
            if pid is None:
                continue
            if pid not in perfAgg:
                perfAgg[pid] = {
                    "member_count": 0,
                    "whatsapp_count": 0,
                    "device_count": 0,
                    "unanswered_count": 0,
                    "knowledge_duel_sum": 0.0,
                }
            agg = perfAgg[pid]
            agg["member_count"] += int(_as_num(r.get("member_count")))
            agg["whatsapp_count"] += int(_as_num(r.get("whatsapp_count")))
            agg["device_count"] += int(_as_num(r.get("device_count")))
            agg["unanswered_count"] += int(_as_num(r.get("unanswered_count")))
            agg["knowledge_duel_sum"] += float(_as_num(r.get("knowledge_duel_result")))

        # √áaƒürƒ± agregasyonlarƒ±
        callAgg: Dict[int, Dict[str, Any]] = {}
        for r in calls_filtered:
            pid = r.get("personnel_id")
            if pid is None:
                continue
            if pid not in callAgg:
                callAgg[pid] = {"call_count": 0, "score_sum": 0.0}
            callAgg[pid]["call_count"] += 1
            callAgg[pid]["score_sum"] += float(_as_num(r.get("score")))

        # Satƒ±rlarƒ± olu≈ütur
        rows: List[Dict[str, Any]] = []
        for p in personnel_data:
            pid = p["id"]
            pa = perfAgg.get(
                pid,
                {
                    "member_count": 0,
                    "whatsapp_count": 0,
                    "device_count": 0,
                    "unanswered_count": 0,
                    "knowledge_duel_sum": 0.0,
                },
            )
            ca = callAgg.get(pid, {"call_count": 0, "score_sum": 0.0})
            avg_score = (
                (ca["score_sum"] / ca["call_count"]) if ca["call_count"] else None
            )
            rows.append(
                {
                    "EKƒ∞P": p.get("team", ""),
                    "PERSONEL": p.get("name", "-"),
                    "√úYE ADEDƒ∞": pa["member_count"],
                    "WHATSAPP ADEDƒ∞": pa["whatsapp_count"],
                    "Cƒ∞HAZ ADEDƒ∞": pa["device_count"],
                    "WHATSAPP CEVAPSIZ ADEDƒ∞": pa["unanswered_count"],
                    "Bƒ∞LGƒ∞ D√úELLOSU": int(pa["knowledge_duel_sum"])
                    if pa["knowledge_duel_sum"] == int(pa["knowledge_duel_sum"])
                    else float(pa["knowledge_duel_sum"]),
                    "√áAƒûRI ADEDƒ∞": ca["call_count"],
                    "√áAƒûRI PUANI": round(avg_score, 2)
                    if isinstance(avg_score, (int, float))
                    else None,
                }
            )

        # Takƒ±m ve personele g√∂re sƒ±rala (As, Paf, sonra isim)
        def team_key(t: str) -> int:
            return {"As Ekip": 0, "Paf Ekip": 1}.get(t or "", 2)

        rows.sort(key=lambda r: (team_key(r.get("EKƒ∞P")), r.get("PERSONEL") or ""))

        # DataFrame -> Excel (tek sheet)
        import pandas as pd
        from io import BytesIO

        df = pd.DataFrame(rows)
        excel_buffer = BytesIO()
        with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Dashboard √ñzeti")

        excel_buffer.seek(0)

        # Dosya adƒ±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = "dashboard_ozet"
        if start_date or end_date:
            filename += "_"
            if start_date:
                filename += start_date
            if start_date and end_date:
                filename += "_to_"
            if end_date:
                filename += end_date
        filename += f"_{timestamp}.xlsx"

        print(f"‚úÖ Dashboard summary Excel olu≈üturuldu: {filename} ({len(rows)} satƒ±r)")

        slogger.log_business_event(
            event_type="dashboard_summary_excel_export",
            description=f"Dashboard summary exported to Excel: {len(rows)} rows",
            data={
                "filename": filename,
                "row_count": len(rows),
                "filters": {"start_date": start_date, "end_date": end_date},
            },
            user_id="system",
        )

        return Response(
            content=excel_buffer.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        print(f"‚ùå Dashboard summary Excel export hatasƒ±: {str(e)}")
        import traceback

        print(f"üîç Detaylƒ± hata: {traceback.format_exc()}")
        slogger.log_error(
            error_type="DashboardSummaryExcelExportError",
            message=str(e),
            endpoint="/api/export/dashboard-summary-excel",
        )
        raise HTTPException(
            status_code=500, detail=f"Dashboard Excel export hatasƒ±: {str(e)}"
        )


@app.get("/api/export/performance-excel")
async def export_performance_to_excel(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """Performans verilerini Excel formatƒ±nda export yapar - filtreli"""
    try:
        print(f"üìä Performans Excel export ba≈ülatƒ±lƒ±yor...")
        print(
            f"üîç Filtreler - start_date: {start_date}, end_date: {end_date}, personnel_id: {personnel_id}"
        )

        # Filtreleme uygula (get_performance_records endpoint'i ile aynƒ± mantƒ±k)
        filtered_records = performance_data.copy()

        # Tarih aralƒ±ƒüƒ± filtresi
        if start_date or end_date:
            filtered_by_date = []
            for record in filtered_records:
                record_date = record.get("date")
                if record_date:
                    # Tarih kar≈üƒ±la≈ütƒ±rmasƒ±
                    if start_date and record_date < start_date:
                        continue
                    if end_date and record_date > end_date:
                        continue
                    filtered_by_date.append(record)
            filtered_records = filtered_by_date
            print(f"üìÖ Tarih filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel filtresi
        if personnel_id:
            filtered_records = [
                r for r in filtered_records if r.get("personnel_id") == personnel_id
            ]
            print(f"üë§ Personel filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel isimlerini dahil et
        enriched_records = []
        for record in filtered_records:
            personnel = next(
                (p for p in personnel_data if p["id"] == record["personnel_id"]), None
            )
            enriched_record = record.copy()
            enriched_record["personnel_name"] = (
                personnel["name"] if personnel else "Bilinmeyen"
            )
            enriched_records.append(enriched_record)

        print(f"üìä Excel'e aktarƒ±lacak kayƒ±t sayƒ±sƒ±: {len(enriched_records)}")

        # Yardƒ±mcƒ± d√∂n√º≈üt√ºr√ºc√ºler: None/bo≈ü/string deƒüerleri g√ºvenle sayƒ±ya √ßevir
        def _as_int(val) -> int:
            try:
                if val is None or val == "":
                    return 0
                # True/False gibi bool'lar i√ßin int'e d√∂n√º≈üt√ºrme
                if isinstance(val, bool):
                    return int(val)
                if isinstance(val, (int,)):
                    return val
                if isinstance(val, float):
                    # NaN kontrol√º
                    if val != val:  # nan
                        return 0
                    return int(val)
                # string veya diƒüer tipler
                return int(float(str(val).strip()))
            except Exception:
                return 0

        def _as_float(val) -> float:
            try:
                if val is None or val == "":
                    return 0.0
                if isinstance(val, bool):
                    return float(val)
                if isinstance(val, (int, float)):
                    # NaN kontrol√º
                    if isinstance(val, float) and val != val:
                        return 0.0
                    return float(val)
                return float(str(val).strip())
            except Exception:
                return 0.0

        # Sheet 1: Personel Performans √ñzeti
        summary_by_personnel: dict[int, dict] = {}
        for record in enriched_records:
            pid = record["personnel_id"]
            if pid not in summary_by_personnel:
                summary_by_personnel[pid] = {
                    "PERSONEL": record["personnel_name"],
                    "TOPLAM √úYE": 0,
                    "TOPLAM WHATSAPP": 0,
                    "TOPLAM Cƒ∞HAZ": 0,
                    "TOPLAM CEVAPSIZ": 0,
                    "ORTALAMA Bƒ∞LGƒ∞ D√úELLOSU": 0,
                    "KAYIT SAYISI": 0,
                    "toplam_bilgi_puani": 0.0,
                }

            summary_by_personnel[pid]["TOPLAM √úYE"] += _as_int(
                record.get("member_count")
            )
            summary_by_personnel[pid]["TOPLAM WHATSAPP"] += _as_int(
                record.get("whatsapp_count")
            )
            summary_by_personnel[pid]["TOPLAM Cƒ∞HAZ"] += _as_int(
                record.get("device_count")
            )
            summary_by_personnel[pid]["TOPLAM CEVAPSIZ"] += _as_int(
                record.get("unanswered_count")
            )
            summary_by_personnel[pid]["toplam_bilgi_puani"] += _as_float(
                record.get("knowledge_duel_result")
            )
            summary_by_personnel[pid]["KAYIT SAYISI"] += 1

        # Ortalama bilgi d√ºellosu hesapla
        for _pid, summary in summary_by_personnel.items():
            if summary["KAYIT SAYISI"] > 0:
                summary["ORTALAMA Bƒ∞LGƒ∞ D√úELLOSU"] = round(
                    (summary["toplam_bilgi_puani"] or 0.0) / summary["KAYIT SAYISI"], 1
                )
            # Ge√ßici alanƒ± sil
            if "toplam_bilgi_puani" in summary:
                del summary["toplam_bilgi_puani"]

        # Sheet 2: Detaylƒ± Performans Kayƒ±tlarƒ±
        detailed_data = []
        for record in enriched_records:
            detailed_data.append(
                {
                    "TARƒ∞H": record.get("date", ""),
                    "PERSONEL": record["personnel_name"],
                    "√úYE ADEDƒ∞": record.get("member_count", 0),
                    "WHATSAPP ADEDƒ∞": record.get("whatsapp_count", 0),
                    "Cƒ∞HAZ ADEDƒ∞": record.get("device_count", 0),
                    "CEVAPSIZ ADEDƒ∞": record.get("unanswered_count", 0),
                    "Bƒ∞LGƒ∞ D√úELLOSU": record.get("knowledge_duel_result", 0),
                    "√ñD√úL/CEZA": record.get("reward_penalty", ""),
                    "NOTLAR": record.get("notes", ""),
                }
            )

        # DataFrame olu≈ütur
        import pandas as pd

        summary_df = pd.DataFrame(list(summary_by_personnel.values()))
        detailed_df = pd.DataFrame(detailed_data)
        # Tamamen bo≈ü durumlarda bile ge√ßerli sayfalar olu≈üturabilmek i√ßin kolonlarƒ± garanti et
        if summary_df.empty:
            summary_df = pd.DataFrame(
                columns=[
                    "PERSONEL",
                    "TOPLAM √úYE",
                    "TOPLAM WHATSAPP",
                    "TOPLAM Cƒ∞HAZ",
                    "TOPLAM CEVAPSIZ",
                    "ORTALAMA Bƒ∞LGƒ∞ D√úELLOSU",
                    "KAYIT SAYISI",
                ]
            )
        if detailed_df.empty:
            detailed_df = pd.DataFrame(
                columns=[
                    "TARƒ∞H",
                    "PERSONEL",
                    "√úYE ADEDƒ∞",
                    "WHATSAPP ADEDƒ∞",
                    "Cƒ∞HAZ ADEDƒ∞",
                    "CEVAPSIZ ADEDƒ∞",
                    "Bƒ∞LGƒ∞ D√úELLOSU",
                    "√ñD√úL/CEZA",
                    "NOTLAR",
                ]
            )

        # Excel dosyasƒ± i√ßin BytesIO buffer
        from io import BytesIO

        excel_buffer = BytesIO()

        # Excel writer ile multiple sheets
        with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
            # Sheet 1: √ñzet
            summary_df.to_excel(
                writer, index=False, sheet_name="Personel Performans √ñzeti"
            )

            # Sheet 2: Detay
            detailed_df.to_excel(writer, index=False, sheet_name="Detaylƒ± Kayƒ±tlar")

        excel_buffer.seek(0)

        # Dosya adƒ± olu≈ütur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performans_raporu"
        if start_date or end_date:
            filename += "_"
            if start_date:
                filename += start_date
            if start_date and end_date:
                filename += "_to_"
            if end_date:
                filename += end_date
        filename += f"_{timestamp}.xlsx"

        print(f"‚úÖ Performans Excel dosyasƒ± olu≈üturuldu: {filename}")
        print(
            f"üìä √ñzet: {len(summary_by_personnel)} personel, Detay: {len(detailed_data)} kayƒ±t"
        )

        # Business event logla
        slogger.log_business_event(
            event_type="performance_excel_export",
            description=f"Performance data exported to Excel: {len(detailed_data)} records, {len(summary_by_personnel)} personnel",
            data={
                "filename": filename,
                "total_records": len(detailed_data),
                "personnel_count": len(summary_by_personnel),
                "filters": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "personnel_id": personnel_id,
                },
            },
            user_id="system",
        )

        # Response d√∂nd√ºr
        return Response(
            content=excel_buffer.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        print(f"‚ùå Performans Excel export hatasƒ±: {str(e)}")
        import traceback

        print(f"üîç Detaylƒ± hata: {traceback.format_exc()}")
        slogger.log_error(
            error_type="PerformanceExcelExportError",
            message=str(e),
            endpoint="/api/export/performance-excel",
        )
        raise HTTPException(
            status_code=500, detail=f"Performans Excel export hatasƒ±: {str(e)}"
        )


# üì• EXCEL EXPORT ENDPOINT
@app.get("/api/export/excel")
async def export_to_excel(start_date: str = None, end_date: str = None):
    """Tarih aralƒ±ƒüƒ±na g√∂re Excel formatƒ±nda export yapar"""
    try:
        # Timestamp olu≈ütur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Filename'de tarih aralƒ±ƒüƒ±nƒ± g√∂ster
        if start_date and end_date:
            # Her iki tarih de varsa aralƒ±ƒüƒ± g√∂ster
            filename = (
                f"personel_takip_export_{start_date}_to_{end_date}_{timestamp}.xlsx"
            )
        elif start_date:
            # Sadece ba≈ülangƒ±√ß tarihi varsa
            filename = f"personel_takip_export_from_{start_date}_{timestamp}.xlsx"
        elif end_date:
            # Sadece biti≈ü tarihi varsa
            filename = f"personel_takip_export_until_{end_date}_{timestamp}.xlsx"
        else:
            # Tarih filtresi yoksa normal
            filename = f"personel_takip_export_all_data_{timestamp}.xlsx"

        # √áaƒürƒ± puanlarƒ± verilerini filtrele
        filtered_records = daily_records_data.copy()

        print(f"üîç Gelen parametreler - start_date: {start_date}, end_date: {end_date}")
        print(f"üìã Toplam g√ºnl√ºk kayƒ±t sayƒ±sƒ±: {len(daily_records_data)}")

        # Eƒüer parametreler varsa filtreleme yap
        if start_date or end_date:
            original_count = len(filtered_records)

            if start_date and end_date:
                # Her iki tarih de varsa aralƒ±k filtresi
                filtered_records = [
                    record
                    for record in daily_records_data
                    if start_date <= record["date"] <= end_date
                ]
                print(f"üìÖ Tarih aralƒ±ƒüƒ± filtresi uygulandƒ±: {start_date} - {end_date}")
            elif start_date:
                # Sadece ba≈ülangƒ±√ß tarihi varsa, o tarihten sonraki kayƒ±tlar
                filtered_records = [
                    record
                    for record in daily_records_data
                    if record["date"] >= start_date
                ]
                print(f"ÔøΩ Ba≈ülangƒ±√ß tarih filtresi uygulandƒ±: {start_date} ve sonrasƒ±")
            elif end_date:
                # Sadece biti≈ü tarihi varsa, o tarihe kadar olan kayƒ±tlar
                filtered_records = [
                    record
                    for record in daily_records_data
                    if record["date"] <= end_date
                ]
                print(f"üìÖ Biti≈ü tarih filtresi uygulandƒ±: {end_date} ve √∂ncesi")

            print(
                f"üìä Filtreleme √∂ncesi: {original_count}, sonrasƒ±: {len(filtered_records)}"
            )

            # Filtrelenen kayƒ±tlarƒ±n detaylarƒ±nƒ± logla
            for record in filtered_records:
                print(
                    f"  üìù Kayƒ±t: ID={record['id']}, Tarih={record['date']}, Personel={record['personnel_id']}"
                )
        else:
            print("üìÖ Tarih filtresi yok, t√ºm kayƒ±tlar dahil edilecek")

        # Excel writer olu≈ütur
        with pd.ExcelWriter(filename, engine="xlsxwriter") as writer:
            # 1. SHEET: Personel √áaƒürƒ± √ñzeti
            personnel_summary = []
            for person in personnel_data:
                person_records = [
                    r for r in filtered_records if r["personnel_id"] == person["id"]
                ]

                if person_records:
                    scores = [r["score"] for r in person_records]
                    avg_score = sum(scores) / len(scores)
                    summary = {
                        "Personel": person["name"],
                        "Dinlenilen_Cagri_Adedi": len(person_records),
                        "Cagri_Puan_Ortalamasi": round(avg_score, 1),
                        "En_Yuksek_Puan": max(scores),
                        "En_Dusuk_Puan": min(scores),
                        "Toplam_Puan": sum(scores),
                    }
                else:
                    summary = {
                        "Personel": person["name"],
                        "Dinlenilen_Cagri_Adedi": 0,
                        "Cagri_Puan_Ortalamasi": 0,
                        "En_Yuksek_Puan": 0,
                        "En_Dusuk_Puan": 0,
                        "Toplam_Puan": 0,
                    }
                personnel_summary.append(summary)

            summary_df = pd.DataFrame(personnel_summary)
            summary_df.to_excel(writer, sheet_name="Personel √áaƒürƒ± √ñzeti", index=False)

            # 2. SHEET: Dinlenilen √áaƒürƒ±lar
            records_for_export = []
            for record in filtered_records:
                # Personel adƒ±nƒ± bul
                person = next(
                    (p for p in personnel_data if p["id"] == record["personnel_id"]),
                    None,
                )
                person_name = person["name"] if person else "Bilinmeyen"

                export_record = {
                    "Tarih": record["date"],
                    "Personel": person_name,
                    "Dinlenilen_Cagri_Numarasi": record["call_number"],
                    "Puan": record["score"],
                    "Geribildiri": record["notes"] or "",
                    "Kayit_ID": record["id"],
                }
                records_for_export.append(export_record)

            records_df = pd.DataFrame(records_for_export)
            records_df.to_excel(writer, sheet_name="Dinlenilen √áaƒürƒ±lar", index=False)

            # Worksheet formatlamasƒ±
            workbook = writer.book

            # Header formatƒ±
            header_format = workbook.add_format(
                {
                    "bold": True,
                    "bg_color": "#4472C4",
                    "font_color": "white",
                    "align": "center",
                    "valign": "vcenter",
                }
            )

            # Sheet formatlarƒ±nƒ± uygula
            for sheet_name in ["Personel √áaƒürƒ± √ñzeti", "Dinlenilen √áaƒürƒ±lar"]:
                worksheet = writer.sheets[sheet_name]

                # Header formatƒ±nƒ± uygula
                for col_num, value in enumerate(
                    summary_df.columns
                    if sheet_name == "Personel √áaƒürƒ± √ñzeti"
                    else records_df.columns
                ):
                    worksheet.write(0, col_num, value, header_format)

                # S√ºtun geni≈üliklerini ayarla
                worksheet.set_column("A:F", 20)

        print(f"‚úÖ Excel export olu≈üturuldu: {filename}")
        print(f"üìÑ Personel √∂zeti sayƒ±sƒ±: {len(personnel_summary)}")
        print(f"üìÑ √áaƒürƒ± kayƒ±tlarƒ± sayƒ±sƒ±: {len(records_for_export)}")

        return FileResponse(
            filename,
            filename=filename,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    except Exception as e:
        print(f"‚ùå Excel export hatasƒ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Excel export hatasƒ±: {str(e)}")


# üìö Eƒûƒ∞Tƒ∞M-GERƒ∞Bƒ∞LDƒ∞Rƒ∞M-UYARI-KESƒ∞NTƒ∞ ENDPOINTS
@app.get("/api/training-feedback")
async def get_training_feedback_records(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """Eƒüitim-Geribildirim-Uyarƒ±-Kesinti kayƒ±tlarƒ±nƒ± getir - Tarih aralƒ±ƒüƒ± ve personel filtresi ile"""
    try:
        print(
            f"üìö Training feedback records listeleniyor, toplam kayƒ±t: {len(training_feedback_data)}"
        )
        print(
            f"üîç Filtreler - start_date: {start_date}, end_date: {end_date}, personnel_id: {personnel_id}"
        )

        # Filtreleme uygula
        filtered_records = training_feedback_data.copy()

        # Tarih aralƒ±ƒüƒ± filtresi
        if start_date or end_date:
            filtered_by_date = []
            for record in filtered_records:
                record_date = record.get("date")
                if record_date:
                    # Tarih kar≈üƒ±la≈ütƒ±rmasƒ±
                    if start_date and record_date < start_date:
                        continue
                    if end_date and record_date > end_date:
                        continue
                    filtered_by_date.append(record)
            filtered_records = filtered_by_date
            print(f"üìÖ Tarih filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel filtresi
        if personnel_id:
            filtered_records = [
                r for r in filtered_records if r.get("personnel_id") == personnel_id
            ]
            print(f"üë§ Personel filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(filtered_records)}")

        # Personel isimlerini dahil et
        enriched_records = []
        for record in filtered_records:
            personnel = next(
                (p for p in personnel_data if p["id"] == record["personnel_id"]), None
            )
            enriched_record = record.copy()
            enriched_record["personnel_name"] = (
                personnel["name"] if personnel else "Bilinmeyen"
            )
            enriched_records.append(enriched_record)

        return {
            "success": True,
            "data": enriched_records,
            "timestamp": datetime.now(),
            "filters": {
                "start_date": start_date,
                "end_date": end_date,
                "personnel_id": personnel_id,
                "total_filtered": len(enriched_records),
                "total_available": len(training_feedback_data),
            },
        }

    except Exception as e:
        print(f"‚ùå Training feedback records getirme hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Training feedback records getirilemedi: {str(e)}"
        )


@app.get("/api/training-feedback/summary")
async def get_training_feedback_summary(
    start_date: Optional[str] = None, end_date: Optional[str] = None
):
    """Eƒüitim-Geribildirim-Uyarƒ±-Kesinti √∂zet verilerini getir (√ºst tablo i√ßin)"""
    try:
        print(f"üìä Training feedback summary verisi hesaplanƒ±yor...")

        # Filtreleme uygula
        filtered_records = training_feedback_data.copy()

        # Tarih aralƒ±ƒüƒ± filtresi
        if start_date or end_date:
            filtered_by_date = []
            for record in filtered_records:
                record_date = record.get("date")
                if record_date:
                    if start_date and record_date < start_date:
                        continue
                    if end_date and record_date > end_date:
                        continue
                    filtered_by_date.append(record)
            filtered_records = filtered_by_date

        # Personel bazƒ±nda toplama
        summary_by_personnel = {}
        for record in filtered_records:
            personnel_id = record["personnel_id"]

            # Personel adƒ±nƒ± personnel_data'dan bul
            personnel_name = "Bilinmeyen Personel"
            for personnel in personnel_data:
                if personnel["id"] == personnel_id:
                    personnel_name = personnel["name"]
                    break

            if personnel_id not in summary_by_personnel:
                summary_by_personnel[personnel_id] = {
                    "personnel_id": personnel_id,
                    "personnel_name": personnel_name,
                    "warning_count": 0,
                    "interruption_count": 0,
                    "feedback_count": 0,
                    "general_training_count": 0,
                    "personal_training_count": 0,
                    "warning_details": [],
                    "interruption_details": [],
                    "feedback_details": [],
                    "general_training_details": [],
                    "personal_training_details": [],
                }

            # Sayƒ±larƒ± topla
            if record.get("warning_interruption_type") == "uyari":
                # Yeni: her kayƒ±t i√ßin "warning_interruption_count" alanƒ±nƒ± topla (yoksa 1)
                try:
                    wcnt = int(record.get("warning_interruption_count", 1) or 1)
                except Exception:
                    wcnt = 1
                summary_by_personnel[personnel_id]["warning_count"] += wcnt
                if record.get("warning_interruption_subject"):
                    summary_by_personnel[personnel_id]["warning_details"].append(
                        {
                            "date": record["date"],
                            "subject": record["warning_interruption_subject"],
                            "count": wcnt,
                        }
                    )
            elif record.get("warning_interruption_type") == "kesinti":
                try:
                    icnt = int(record.get("warning_interruption_count", 1) or 1)
                except Exception:
                    icnt = 1
                summary_by_personnel[personnel_id]["interruption_count"] += icnt
                if record.get("warning_interruption_subject"):
                    summary_by_personnel[personnel_id]["interruption_details"].append(
                        {
                            "date": record["date"],
                            "subject": record["warning_interruption_subject"],
                            "count": icnt,
                        }
                    )

            if record.get("feedback_count", 0) > 0:
                summary_by_personnel[personnel_id]["feedback_count"] += record[
                    "feedback_count"
                ]
                if record.get("feedback_subject"):
                    summary_by_personnel[personnel_id]["feedback_details"].append(
                        {
                            "date": record["date"],
                            "count": record["feedback_count"],
                            "subject": record["feedback_subject"],
                        }
                    )

            if record.get("general_training_count", 0) > 0:
                summary_by_personnel[personnel_id]["general_training_count"] += record[
                    "general_training_count"
                ]
                if record.get("general_training_subject"):
                    summary_by_personnel[personnel_id][
                        "general_training_details"
                    ].append(
                        {
                            "date": record["date"],
                            "count": record["general_training_count"],
                            "subject": record["general_training_subject"],
                        }
                    )

            if record.get("personal_training_count", 0) > 0:
                summary_by_personnel[personnel_id]["personal_training_count"] += record[
                    "personal_training_count"
                ]
                if record.get("personal_training_subject"):
                    summary_by_personnel[personnel_id][
                        "personal_training_details"
                    ].append(
                        {
                            "date": record["date"],
                            "count": record["personal_training_count"],
                            "subject": record["personal_training_subject"],
                        }
                    )

        return {
            "success": True,
            "data": list(summary_by_personnel.values()),
            "timestamp": datetime.now(),
            "filters": {
                "start_date": start_date,
                "end_date": end_date,
                "total_personnel": len(summary_by_personnel),
                "total_records": len(filtered_records),
            },
        }

    except Exception as e:
        print(f"‚ùå Training feedback summary hatasƒ±: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Training feedback summary getirilemedi: {str(e)}"
        )


# üìå MESAI SONRASI √áALI≈ûMA ENDPOINTS
@app.get("/api/after-hours")
async def get_after_hours(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """Mesai sonrasƒ± √ßalƒ±≈üma kayƒ±tlarƒ±nƒ± getir (tarih/personel filtreli)"""
    try:
        filtered = after_hours_data.copy()
        # Date range filter (string compare YYYY-MM-DD)
        if start_date or end_date:
            tmp = []
            for r in filtered:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            filtered = tmp
        # Personnel filter
        if personnel_id:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            filtered = [r for r in filtered if r.get("personnel_id") == pid]
        # Enrich with personnel_name
        enriched = []
        for r in filtered:
            p = personnel_index.get(r.get("personnel_id"))
            rr = r.copy()
            rr["personnel_name"] = p.get("name") if p else "Bilinmeyen"
            enriched.append(rr)
        return {
            "success": True,
            "data": enriched,
            "filters": {
                "start_date": start_date,
                "end_date": end_date,
                "personnel_id": personnel_id,
            },
            "total": len(enriched),
            "timestamp": datetime.now(),
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± kayƒ±tlarƒ± getirilemedi: {str(e)}"
        )


@app.get("/api/after-hours/summary")
async def get_after_hours_summary(
    start_date: Optional[str] = None, end_date: Optional[str] = None
):
    """Mesai sonrasƒ± √ßalƒ±≈üma √∂zet verileri (√ºst tablo i√ßin toplamlar)"""
    try:
        # Reuse listing filter without personnel filter
        filtered = after_hours_data.copy()
        if start_date or end_date:
            tmp = []
            for r in filtered:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            filtered = tmp
        totals = {
            "total_call_count": 0,
            "total_talk_duration": 0,
            "total_member_count": 0,
            "record_count": len(filtered),
        }
        for r in filtered:
            totals["total_call_count"] += int(r.get("call_count", 0) or 0)
            totals["total_talk_duration"] += int(r.get("talk_duration", 0) or 0)
            totals["total_member_count"] += int(r.get("member_count", 0) or 0)
        return {"success": True, "data": totals, "timestamp": datetime.now()}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± √∂zet getirilemedi: {str(e)}"
        )


@app.post("/api/after-hours")
async def add_after_hours(record: dict):
    """Mesai sonrasƒ± √ßalƒ±≈üma kaydƒ± ekle"""
    try:
        date_val = record.get("date")
        personnel_id = record.get("personnel_id")
        if not date_val or not personnel_id:
            raise HTTPException(status_code=400, detail="Tarih ve personel zorunludur")
        try:
            pid = int(personnel_id)
        except Exception:
            raise HTTPException(status_code=400, detail="Personel ID sayƒ± olmalƒ±")
        if pid not in personnel_index:
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")
        global _max_after_hours_id
        _max_after_hours_id += 1
        new_id = _max_after_hours_id
        new_rec = {
            "id": new_id,
            "date": str(date_val),
            "personnel_id": pid,
            "call_count": int(record.get("call_count", 0) or 0),
            "talk_duration": int(record.get("talk_duration", 0) or 0),
            "member_count": int(record.get("member_count", 0) or 0),
            "notes": str(record.get("notes", "")),
        }
        with data_lock:
            after_hours_data.append(new_rec)
            after_hours_index[new_id] = new_rec
        return {
            "success": True,
            "data": new_rec,
            "message": "Kayƒ±t eklendi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± kayƒ±t eklenemedi: {str(e)}"
        )


@app.put("/api/after-hours/{record_id}")
async def update_after_hours(record_id: int, record: dict):
    """Mesai sonrasƒ± √ßalƒ±≈üma kaydƒ± g√ºncelle"""
    try:
        existing = after_hours_index.get(record_id)
        if not existing:
            raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
        with data_lock:
            if "date" in record:
                existing["date"] = str(record.get("date") or existing.get("date"))
            if "personnel_id" in record:
                try:
                    pid = int(record.get("personnel_id"))
                    if pid in personnel_index:
                        existing["personnel_id"] = pid
                except Exception:
                    pass
            for k in ("call_count", "talk_duration", "member_count"):
                if k in record:
                    try:
                        existing[k] = int(record.get(k) or 0)
                    except Exception:
                        existing[k] = 0
            if "notes" in record:
                existing["notes"] = str(record.get("notes") or "")
        return {
            "success": True,
            "data": existing,
            "message": "Kayƒ±t g√ºncellendi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± kayƒ±t g√ºncellenemedi: {str(e)}"
        )


@app.delete("/api/after-hours/{record_id}")
async def delete_after_hours(record_id: int):
    """Mesai sonrasƒ± √ßalƒ±≈üma kaydƒ± sil"""
    try:
        with data_lock:
            idx = next(
                (i for i, r in enumerate(after_hours_data) if r["id"] == record_id),
                None,
            )
            if idx is None:
                raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
            deleted = after_hours_data.pop(idx)
            if record_id in after_hours_index:
                del after_hours_index[record_id]
        return {
            "success": True,
            "data": {"id": record_id},
            "message": "Kayƒ±t silindi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± kayƒ±t silinemedi: {str(e)}"
        )


@app.get("/api/export/after-hours-excel")
async def export_after_hours_excel(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """Mesai sonrasƒ± √ßalƒ±≈üma verilerini Excel olarak indir"""
    try:
        # Filter
        filtered = after_hours_data.copy()
        if start_date or end_date:
            tmp = []
            for r in filtered:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            filtered = tmp
        if personnel_id:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            filtered = [r for r in filtered if r.get("personnel_id") == pid]
        # Enrich
        enriched = []
        for r in filtered:
            p = personnel_index.get(r.get("personnel_id"))
            e = r.copy()
            e["personnel_name"] = p.get("name") if p else "Bilinmeyen"
            enriched.append(e)
        # DataFrames
        import pandas as pd

        # Summary (Toplamlar) to match UI table headers, grouped by personel
        summary_rows = []
        by_personnel = {}
        for r in enriched:
            pid = r.get("personnel_id")
            name = r.get("personnel_name", "Bilinmeyen")
            if pid not in by_personnel:
                by_personnel[pid] = {
                    "Personel": name,
                    "Arama Adedi": 0,
                    "Konu≈üma S√ºresi (dk)": 0,
                    "√úye Adedi": 0,
                    "Kayƒ±t Sayƒ±sƒ±": 0,
                }
            by_personnel[pid]["Arama Adedi"] += int(r.get("call_count", 0) or 0)
            by_personnel[pid]["Konu≈üma S√ºresi (dk)"] += int(
                r.get("talk_duration", 0) or 0
            )
            by_personnel[pid]["√úye Adedi"] += int(r.get("member_count", 0) or 0)
            by_personnel[pid]["Kayƒ±t Sayƒ±sƒ±"] += 1
        summary_rows.extend(by_personnel.values())
        # Add a general total row at the end for convenience
        gen_totals = {
            "Personel": "Genel Toplam",
            "Arama Adedi": sum(r["Arama Adedi"] for r in summary_rows)
            if summary_rows
            else 0,
            "Konu≈üma S√ºresi (dk)": sum(r["Konu≈üma S√ºresi (dk)"] for r in summary_rows)
            if summary_rows
            else 0,
            "√úye Adedi": sum(r["√úye Adedi"] for r in summary_rows)
            if summary_rows
            else 0,
            "Kayƒ±t Sayƒ±sƒ±": sum(r["Kayƒ±t Sayƒ±sƒ±"] for r in summary_rows)
            if summary_rows
            else 0,
        }
        if summary_rows:
            summary_rows.append(gen_totals)
        else:
            # If no data, still provide a single total row with zeros
            summary_rows = [gen_totals]

        summary_df = pd.DataFrame(
            summary_rows,
            columns=[
                "Personel",
                "Arama Adedi",
                "Konu≈üma S√ºresi (dk)",
                "√úye Adedi",
                "Kayƒ±t Sayƒ±sƒ±",
            ],
        )

        # Detailed sheet stays as detailed list
        detailed = [
            {
                "TARƒ∞H": r.get("date", ""),
                "PERSONEL": r.get("personnel_name", ""),
                "ARAMA ADEDƒ∞": r.get("call_count", 0),
                "KONU≈ûMA S√úRESƒ∞ (dk)": r.get("talk_duration", 0),
                "√úYE ADEDƒ∞": r.get("member_count", 0),
                "NOTLAR": r.get("notes", ""),
            }
            for r in enriched
        ]
        detail_df = pd.DataFrame(detailed)
        from io import BytesIO

        excel_buffer = BytesIO()
        with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
            summary_df.to_excel(writer, index=False, sheet_name="√ñzet")
            detail_df.to_excel(writer, index=False, sheet_name="Detaylar")
        excel_buffer.seek(0)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"mesai_sonrasi_{timestamp_str}.xlsx"
        return Response(
            content=excel_buffer.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Mesai sonrasƒ± Excel export hatasƒ±: {str(e)}"
        )


# üóìÔ∏è ATTENDANCE / LEAVE OVERRIDES (in-memory)
# Allows explicitly setting attendance value for a person on a date: 1 (full), 0.5 (half-day), 0 (absent)
# Initialize in-memory stores
attendance_overrides_data = []  # List[dict]
attendance_overrides_index = {}
_max_attendance_override_id = 0


@app.get("/api/attendance")
async def list_attendance_overrides(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    try:
        recs = list(attendance_overrides_data)
        # date range filter (string compare)
        if start_date or end_date:
            tmp = []
            for r in recs:
                d = r.get("date")
                if not d:
                    continue
                if start_date and d < start_date:
                    continue
                if end_date and d > end_date:
                    continue
                tmp.append(r)
            recs = tmp
        # personnel filter
        if personnel_id is not None:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            recs = [r for r in recs if r.get("personnel_id") == pid]
        # enrich
        enriched = []
        for r in recs:
            p = next(
                (pp for pp in personnel_data if pp["id"] == r.get("personnel_id")), None
            )
            e = dict(r)
            e["personnel_name"] = p.get("name") if p else "Bilinmeyen"
            enriched.append(e)
        return {
            "success": True,
            "data": enriched,
            "total": len(enriched),
            "timestamp": datetime.now(),
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Attendance overrides getirilemedi: {str(e)}"
        )


@app.post("/api/attendance")
async def add_attendance_override(record: dict):
    try:
        date_val = record.get("date")
        personnel_id = record.get("personnel_id")
        value = record.get("value")  # expected 0, 0.5 or 1
        if date_val is None or personnel_id is None or value is None:
            raise HTTPException(
                status_code=400, detail="date, personnel_id ve value zorunludur"
            )
        try:
            pid = int(personnel_id)
        except Exception:
            raise HTTPException(status_code=400, detail="personnel_id sayƒ± olmalƒ±")
        if pid not in personnel_index:
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")
        try:
            v = float(value)
        except Exception:
            raise HTTPException(status_code=400, detail="value 0, 0.5 veya 1 olmalƒ±")
        if v not in (0.0, 0.5, 1.0):
            raise HTTPException(status_code=400, detail="value 0, 0.5 veya 1 olmalƒ±")
        # Idempotent behavior: if an override for same (personnel_id, date) exists, update it instead of creating duplicate
        with data_lock:
            existing = next(
                (
                    r
                    for r in attendance_overrides_data
                    if r.get("personnel_id") == pid and r.get("date") == str(date_val)
                ),
                None,
            )
            if existing:
                existing["value"] = v
                if "leave_type" in record:
                    existing["leave_type"] = str(record.get("leave_type", ""))
                if "period" in record:
                    existing["period"] = str(record.get("period", ""))
                if "notes" in record:
                    existing["notes"] = str(record.get("notes", ""))
                attendance_overrides_index[existing["id"]] = existing
                return {
                    "success": True,
                    "data": existing,
                    "message": "Attendance override g√ºncellendi (var olan kayƒ±t)",
                    "timestamp": datetime.now(),
                }

            global _max_attendance_override_id
            _max_attendance_override_id += 1
            new_id = _max_attendance_override_id
            # Keep backward-compatible fields if sent, but they are optional from UI now
            rec = {
                "id": new_id,
                "date": str(date_val),
                "personnel_id": pid,
                "value": v,
                "leave_type": str(record.get("leave_type", "")),  # optional
                "period": str(record.get("period", "")),  # optional
                "notes": str(record.get("notes", "")),
            }
            attendance_overrides_data.append(rec)
            attendance_overrides_index[new_id] = rec
        return {
            "success": True,
            "data": rec,
            "message": "Attendance override eklendi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Attendance override eklenemedi: {str(e)}"
        )


@app.put("/api/attendance/{record_id}")
async def update_attendance_override(record_id: int, record: dict):
    try:
        existing = attendance_overrides_index.get(record_id)
        if not existing:
            raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
        with data_lock:
            if "date" in record:
                existing["date"] = str(record.get("date") or existing.get("date"))
            if "personnel_id" in record:
                try:
                    pid = int(record.get("personnel_id"))
                    if pid in personnel_index:
                        existing["personnel_id"] = pid
                except Exception:
                    pass
            if "value" in record:
                try:
                    v = float(record.get("value"))
                    if v in (0.0, 0.5, 1.0):
                        existing["value"] = v
                except Exception:
                    pass
            for k in ("leave_type", "period", "notes"):
                if k in record:
                    existing[k] = str(record.get(k) or "")
        return {
            "success": True,
            "data": existing,
            "message": "Attendance override g√ºncellendi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Attendance override g√ºncellenemedi: {str(e)}"
        )


@app.delete("/api/attendance/{record_id}")
async def delete_attendance_override(record_id: int):
    try:
        with data_lock:
            idx = next(
                (
                    i
                    for i, r in enumerate(attendance_overrides_data)
                    if r["id"] == record_id
                ),
                None,
            )
            if idx is None:
                raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
            attendance_overrides_data.pop(idx)
            if record_id in attendance_overrides_index:
                del attendance_overrides_index[record_id]
        return {
            "success": True,
            "data": {"id": record_id},
            "message": "Attendance override silindi",
            "timestamp": datetime.now(),
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Attendance override silinemedi: {str(e)}"
        )


@app.post("/api/training-feedback")
async def add_training_feedback_record(request: Request):
    """Yeni eƒüitim-geribildirim-uyarƒ±-kesinti kaydƒ± ekle"""
    try:
        print(f"üìù Training feedback kaydƒ± ekleme isteƒüi")
        # Request body'yi al - UTF-8 encoding ile g√ºvenli ≈üekilde
        body = await request.body()
        try:
            # √ñnce UTF-8 ile dene
            request_data = json.loads(body.decode("utf-8"))
        except UnicodeDecodeError:
            # UTF-8 ba≈üarƒ±sƒ±z olursa latin-1 ile dene
            try:
                request_data = json.loads(body.decode("latin-1"))
            except:
                # Son √ßare olarak errors='ignore' ile
                request_data = json.loads(body.decode("utf-8", errors="ignore"))

        print(f"üìù Gelen training feedback verisi: {request_data}")

        # Gerekli alanlarƒ± kontrol et
        date = (
            request_data.get("date")
            or request_data.get("training_date")
            or request_data.get("training_feedback_date")
        )
        personnel_id = (
            request_data.get("personnel_id")
            or request_data.get("training_personnel_id")
            or request_data.get("training_feedback_personnel_id")
        )
        warning_interruption_type = request_data.get(
            "warning_interruption_type"
        ) or request_data.get("warning_type", "")
        warning_interruption_subject = request_data.get(
            "warning_interruption_subject"
        ) or request_data.get("warning_subject", "")
        feedback_count = request_data.get("feedback_count", 0)
        feedback_subject = request_data.get("feedback_subject", "")
        general_training_count = request_data.get("general_training_count", 0)
        general_training_subject = request_data.get("general_training_subject", "")
        personal_training_count = request_data.get(
            "personal_training_count"
        ) or request_data.get("one_on_one_training_count", 0)
        personal_training_subject = request_data.get(
            "personal_training_subject"
        ) or request_data.get("one_on_one_training_subject", "")
        # Yeni: Uyarƒ±/Kesinti i√ßin adet desteƒüi
        try:
            warning_interruption_count = int(
                request_data.get("warning_interruption_count", 1) or 1
            )
        except Exception:
            warning_interruption_count = 1
        notes = request_data.get("notes", "")

        if not personnel_id:
            raise HTTPException(status_code=400, detail="Personnel ID gerekli")

        if not date:
            raise HTTPException(status_code=400, detail="Tarih gerekli")

        # Personeli kontrol et
        try:
            personnel_id = int(personnel_id)
        except (ValueError, TypeError):
            raise HTTPException(status_code=400, detail="Personnel ID sayƒ± olmalƒ±")

        # üöÄ PERFORMANS: Index'den O(1) kontrol
        if personnel_id not in personnel_index:
            raise HTTPException(status_code=404, detail="Personel bulunamadƒ±")

        personnel = personnel_index[personnel_id]

        # Yeni training feedback kaydƒ± olu≈ütur
        global _max_training_feedback_id
        _max_training_feedback_id += 1
        new_id = _max_training_feedback_id

        new_record = {
            "id": new_id,
            "personnel_id": personnel_id,
            "personnel_name": personnel.get("name", "Bilinmeyen"),
            "date": str(date),
            "warning_interruption_type": str(warning_interruption_type).strip(),
            "warning_interruption_subject": str(warning_interruption_subject).strip(),
            "warning_interruption_count": int(warning_interruption_count),
            "feedback_count": int(feedback_count) if feedback_count else 0,
            "feedback_subject": str(feedback_subject).strip(),
            "general_training_count": int(general_training_count)
            if general_training_count
            else 0,
            "general_training_subject": str(general_training_subject).strip(),
            "personal_training_count": int(personal_training_count)
            if personal_training_count
            else 0,
            "personal_training_subject": str(personal_training_subject).strip(),
            "notes": str(notes).strip(),
            "created_at": datetime.now().isoformat(),
        }

        # Thread-safe veri ekleme
        with data_lock:
            training_feedback_data.append(new_record)
            training_feedback_index[new_id] = new_record

        print(f"‚úÖ Training feedback kaydƒ± ba≈üarƒ±yla eklendi: {personnel['name']}")

        # Business event logla
        slogger.log_business_event(
            event_type="training_feedback_record_created",
            description=f"Training feedback record created for {personnel['name']}",
            data={"personnel_id": personnel_id, "record_id": new_record["id"]},
        )

        return {
            "success": True,
            "data": new_record,
            "message": "Eƒüitim-Geribildirim kaydƒ± ba≈üarƒ±yla eklendi",
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Training feedback kaydƒ± ekleme hatasƒ±: {str(e)}")
        slogger.log_error(
            error_type="TrainingFeedbackRecordCreationError",
            message=f"Training feedback record creation failed: {str(e)}",
            endpoint="/api/training-feedback",
        )
        raise HTTPException(
            status_code=500, detail=f"Training feedback kaydƒ± eklenemedi: {str(e)}"
        )


@app.delete("/api/training-feedback/{record_id}")
async def delete_training_feedback_record(record_id: int):
    """Eƒüitim-Geribildirim-Uyarƒ±-Kesinti kaydƒ±nƒ± sil"""
    try:
        print(f"üóëÔ∏è Training feedback kaydƒ± siliniyor: {record_id}")

        # Thread-safe silme i≈ülemi
        with data_lock:
            # Kaydƒ± bul
            record_to_delete = None
            for i, record in enumerate(training_feedback_data):
                if record["id"] == record_id:
                    record_to_delete = training_feedback_data.pop(i)
                    break

            if not record_to_delete:
                print(f"‚ùå Training feedback kaydƒ± bulunamadƒ±: {record_id}")
                raise HTTPException(
                    status_code=404,
                    detail=f"ID {record_id} ile training feedback kaydƒ± bulunamadƒ±",
                )

            # Personel bilgisini al
            personnel_name = "Bilinmeyen"
            for personnel in personnel_data:
                if personnel["id"] == record_to_delete["personnel_id"]:
                    personnel_name = personnel["name"]
                    break

            print(
                f"‚úÖ Training feedback kaydƒ± silindi: {personnel_name} (ID: {record_id})"
            )

            # Log business event
            slogger.log_business_event(
                event_type="training_feedback_record_deleted",
                description=f"Training feedback record deleted for {personnel_name}",
                data={
                    "personnel_id": record_to_delete["personnel_id"],
                    "record_id": record_id,
                },
            )

        return {
            "success": True,
            "message": f"Training feedback kaydƒ± ba≈üarƒ±yla silindi",
            "deleted_record": {
                "id": record_id,
                "personnel_name": personnel_name,
                "date": record_to_delete.get("date"),
            },
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Training feedback kaydƒ± silme hatasƒ±: {str(e)}")
        slogger.log_error(
            error_type="TrainingFeedbackRecordDeletionError",
            message=f"Training feedback record deletion failed: {str(e)}",
            endpoint=f"/api/training-feedback/{record_id}",
        )
        raise HTTPException(
            status_code=500, detail=f"Training feedback kaydƒ± silinemedi: {str(e)}"
        )


@app.put("/api/training-feedback/{record_id}")
async def update_training_feedback_record(record_id: int, record: dict):
    """Eƒüitim-Geribildirim-Uyarƒ±-Kesinti kaydƒ±nƒ± g√ºncelle"""
    try:
        print(f"‚úèÔ∏è Training feedback kaydƒ± g√ºncelleniyor: {record_id}")
        print(f"üìã Gelen veri: {record}")

        # Thread-safe g√ºncelleme i≈ülemi
        with data_lock:
            # Kaydƒ± bul
            record_to_update = None
            for existing_record in training_feedback_data:
                if existing_record["id"] == record_id:
                    record_to_update = existing_record
                    break

            if not record_to_update:
                print(f"‚ùå Training feedback kaydƒ± bulunamadƒ±: {record_id}")
                raise HTTPException(
                    status_code=404,
                    detail=f"ID {record_id} ile training feedback kaydƒ± bulunamadƒ±",
                )

            # Gelen alanlarƒ± mevcut yapƒ±yla uyumlayarak g√ºncelle
            # Tarih ve personel
            if "personnel_id" in record:
                try:
                    pid = int(record.get("personnel_id"))
                    record_to_update["personnel_id"] = pid
                    # isim g√ºncelle
                    if pid in personnel_index:
                        record_to_update["personnel_name"] = personnel_index[pid].get(
                            "name", record_to_update.get("personnel_name", "Bilinmeyen")
                        )
                except Exception:
                    pass
            if any(
                k in record for k in ["date", "training_feedback_date", "training_date"]
            ):
                record_to_update["date"] = str(
                    record.get("date")
                    or record.get("training_feedback_date")
                    or record.get("training_date")
                    or record_to_update.get("date")
                )

            # Uyarƒ±/Kesinti t√ºr√º ve konusu
            if any(k in record for k in ["warning_interruption_type", "warning_type"]):
                record_to_update["warning_interruption_type"] = (
                    record.get("warning_interruption_type")
                    or record.get("warning_type")
                    or record_to_update.get("warning_interruption_type", "")
                ).strip()
            if any(
                k in record for k in ["warning_interruption_subject", "warning_subject"]
            ):
                record_to_update["warning_interruption_subject"] = (
                    record.get("warning_interruption_subject")
                    or record.get("warning_subject")
                    or record_to_update.get("warning_interruption_subject", "")
                ).strip()
            if "warning_interruption_count" in record:
                try:
                    record_to_update["warning_interruption_count"] = int(
                        record.get("warning_interruption_count")
                        or record_to_update.get("warning_interruption_count", 1)
                    )
                except Exception:
                    pass

            # Geribildirim
            if "feedback_count" in record:
                try:
                    record_to_update["feedback_count"] = int(
                        record.get(
                            "feedback_count", record_to_update.get("feedback_count", 0)
                        )
                    )
                except Exception:
                    pass
            if any(k in record for k in ["feedback_subject", "feedback_topic"]):
                record_to_update["feedback_subject"] = (
                    record.get("feedback_subject")
                    or record.get("feedback_topic")
                    or record_to_update.get("feedback_subject", "")
                ).strip()

            # Genel eƒüitim
            if "general_training_count" in record:
                try:
                    record_to_update["general_training_count"] = int(
                        record.get(
                            "general_training_count",
                            record_to_update.get("general_training_count", 0),
                        )
                    )
                except Exception:
                    pass
            if any(
                k in record
                for k in ["general_training_subject", "general_training_topic"]
            ):
                record_to_update["general_training_subject"] = (
                    record.get("general_training_subject")
                    or record.get("general_training_topic")
                    or record_to_update.get("general_training_subject", "")
                ).strip()

            # Birebir eƒüitim
            if any(
                k in record
                for k in [
                    "personal_training_count",
                    "one_on_one_training_count",
                    "individual_training_count",
                ]
            ):
                try:
                    record_to_update["personal_training_count"] = int(
                        record.get("personal_training_count")
                        or record.get("one_on_one_training_count")
                        or record.get("individual_training_count")
                        or record_to_update.get("personal_training_count", 0)
                    )
                except Exception:
                    pass
            if any(
                k in record
                for k in [
                    "personal_training_subject",
                    "one_on_one_training_subject",
                    "individual_training_topic",
                ]
            ):
                record_to_update["personal_training_subject"] = (
                    record.get("personal_training_subject")
                    or record.get("one_on_one_training_subject")
                    or record.get("individual_training_topic")
                    or record_to_update.get("personal_training_subject", "")
                ).strip()

            # Notlar
            if "notes" in record:
                record_to_update["notes"] = str(record.get("notes")).strip()

            # Personel bilgisini al
            personnel_name = "Bilinmeyen"
            for personnel in personnel_data:
                if personnel["id"] == record_to_update["personnel_id"]:
                    personnel_name = personnel["name"]
                    break

            print(
                f"‚úÖ Training feedback kaydƒ± g√ºncellendi: {personnel_name} (ID: {record_id})"
            )

            # Log business event
            slogger.log_business_event(
                event_type="training_feedback_record_updated",
                description=f"Training feedback record updated for {personnel_name}",
                data={
                    "personnel_id": record_to_update["personnel_id"],
                    "record_id": record_id,
                },
            )

        return {
            "success": True,
            "message": f"Training feedback kaydƒ± ba≈üarƒ±yla g√ºncellendi",
            "updated_record": {
                "id": record_id,
                "personnel_name": personnel_name,
                "date": record_to_update["date"],
            },
            "timestamp": datetime.now(),
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Training feedback kaydƒ± g√ºncelleme hatasƒ±: {str(e)}")
        slogger.log_error(
            error_type="TrainingFeedbackRecordUpdateError",
            message=f"Training feedback record update failed: {str(e)}",
            endpoint=f"/api/training-feedback/{record_id}",
        )
        raise HTTPException(
            status_code=500, detail=f"Training feedback kaydƒ± g√ºncellenemedi: {str(e)}"
        )


@app.get("/api/training-feedback/{record_id}")
async def get_training_feedback_record(record_id: int):
    """Tek training feedback kaydƒ±nƒ± getir - Edit i≈ülemi i√ßin"""
    try:
        print(f"üìñ Training feedback kaydƒ± getiriliyor: {record_id}")

        # Thread-safe okuma i≈ülemi
        with data_lock:
            # Kaydƒ± bul
            record = None
            for existing_record in training_feedback_data:
                if existing_record["id"] == record_id:
                    record = existing_record
                    break

            if not record:
                print(f"‚ùå Training feedback kaydƒ± bulunamadƒ±: {record_id}")
                raise HTTPException(
                    status_code=404,
                    detail=f"ID {record_id} ile training feedback kaydƒ± bulunamadƒ±",
                )

            print(f"‚úÖ Training feedback kaydƒ± bulundu: {record}")

            return {
                "success": True,
                "data": record,
                "message": "Training feedback kaydƒ± ba≈üarƒ±yla getirildi",
                "timestamp": datetime.now(),
            }

    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"‚ùå Training feedback kaydƒ± getirme hatasƒ±: {str(e)}")
        slogger.log_error(
            error_type="TrainingFeedbackRecordRetrievalError",
            message=f"Training feedback record retrieval failed: {str(e)}",
            endpoint=f"/api/training-feedback/{record_id}",
        )
        raise HTTPException(
            status_code=500, detail=f"Training feedback kaydƒ± getirilemedi: {str(e)}"
        )


@app.get("/api/export/training-feedback-excel")
async def export_training_feedback_to_excel(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    personnel_id: Optional[int] = None,
):
    """Eƒüitimler verilerini Excel formatƒ±nda export yapar (Genel/Birebir Eƒüitimler ve Geribildirimler)."""
    try:
        print("üìä Training feedback Excel export ba≈ülatƒ±lƒ±yor‚Ä¶")
        print(
            f"üîç Filtreler - start_date: {start_date}, end_date: {end_date}, personnel_id: {personnel_id}"
        )

        # Kayƒ±tlarƒ± kopyala ve filtreleri uygula
        records = list(training_feedback_data)

        # Tarih aralƒ±ƒüƒ± filtresi (YYYY-MM-DD string kar≈üƒ±la≈ütƒ±rmasƒ± yeterli)
        if start_date or end_date:
            records = [
                r
                for r in records
                if (not start_date or (r.get("date") or "") >= start_date)
                and (not end_date or (r.get("date") or "") <= end_date)
            ]
            print(f"üìÖ Tarih filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(records)}")

        # Personel filtresi
        if personnel_id is not None:
            try:
                pid = int(personnel_id)
            except Exception:
                pid = personnel_id
            records = [r for r in records if r.get("personnel_id") == pid]
            print(f"üë§ Personel filtresi sonrasƒ± kayƒ±t sayƒ±sƒ±: {len(records)}")

        # Personel isimlerini ekle
        enriched_records = []
        for r in records:
            p = personnel_index.get(r.get("personnel_id"))
            enriched = dict(r)
            enriched["personnel_name"] = p.get("name") if p else "Bilinmeyen"
            enriched_records.append(enriched)

        print(f"üìä Excel'e aktarƒ±lacak kayƒ±t sayƒ±sƒ±: {len(enriched_records)}")

        # Sheet 1: Eƒüitimler √ñzeti (personel bazlƒ± toplamlar)
        summary_by_personnel: Dict[Any, Dict[str, Any]] = {}
        for r in enriched_records:
            pid = r.get("personnel_id")
            if pid not in summary_by_personnel:
                summary_by_personnel[pid] = {
                    "PERSONEL": r.get("personnel_name", "Bilinmeyen"),
                    "GENEL Eƒûƒ∞Tƒ∞M ADEDƒ∞": 0,
                    "Bƒ∞REBƒ∞R Eƒûƒ∞Tƒ∞M ADEDƒ∞": 0,
                    "GERƒ∞Bƒ∞LDƒ∞Rƒ∞M ADEDƒ∞": 0,
                }
            summary_by_personnel[pid]["GENEL Eƒûƒ∞Tƒ∞M ADEDƒ∞"] += int(
                r.get("general_training_count", 0) or 0
            )
            summary_by_personnel[pid]["Bƒ∞REBƒ∞R Eƒûƒ∞Tƒ∞M ADEDƒ∞"] += int(
                r.get("personal_training_count", 0) or 0
            )
            summary_by_personnel[pid]["GERƒ∞Bƒ∞LDƒ∞Rƒ∞M ADEDƒ∞"] += int(
                r.get("feedback_count", 0) or 0
            )

        # Sheet 2: Detaylƒ± Kayƒ±tlar
        detailed_rows: List[Dict[str, Any]] = []
        for r in enriched_records:
            detailed_rows.append(
                {
                    "TARƒ∞H": r.get("date", ""),
                    "PERSONEL": r.get("personnel_name", "Bilinmeyen"),
                    "GENEL Eƒûƒ∞Tƒ∞M ADEDƒ∞": r.get("general_training_count", 0),
                    "GENEL Eƒûƒ∞Tƒ∞M KONUSU": r.get("general_training_subject", ""),
                    "Bƒ∞REBƒ∞R Eƒûƒ∞Tƒ∞M ADEDƒ∞": r.get("personal_training_count", 0),
                    "Bƒ∞REBƒ∞R Eƒûƒ∞Tƒ∞M KONUSU": r.get("personal_training_subject", ""),
                    "GERƒ∞Bƒ∞LDƒ∞Rƒ∞M ADEDƒ∞": r.get("feedback_count", 0),
                    "GERƒ∞Bƒ∞LDƒ∞Rƒ∞M KONUSU": r.get("feedback_subject", ""),
                    "NOTLAR": r.get("notes", ""),
                }
            )

        # DataFrame olu≈ütur
        summary_df = (
            pd.DataFrame(list(summary_by_personnel.values()))
            if summary_by_personnel
            else pd.DataFrame(
                columns=[
                    "PERSONEL",
                    "GENEL Eƒûƒ∞Tƒ∞M ADEDƒ∞",
                    "Bƒ∞REBƒ∞R Eƒûƒ∞Tƒ∞M ADEDƒ∞",
                    "GERƒ∞Bƒ∞LDƒ∞Rƒ∞M ADEDƒ∞",
                ]
            )
        )
        detailed_df = pd.DataFrame(detailed_rows)

        # Excel yazƒ±mƒ±
        from io import BytesIO

        excel_buffer = BytesIO()
        with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
            summary_df.to_excel(writer, index=False, sheet_name="Eƒüitimler √ñzeti")
            detailed_df.to_excel(writer, index=False, sheet_name="Detaylƒ± Kayƒ±tlar")

        excel_buffer.seek(0)

        # Dosya adƒ±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = "egitimler_raporu"
        if start_date or end_date:
            filename += "_"
            if start_date:
                filename += start_date
            if start_date and end_date:
                filename += "_to_"
            if end_date:
                filename += end_date
        filename += f"_{timestamp}.xlsx"

        print(f"‚úÖ Training feedback Excel dosyasƒ± olu≈üturuldu: {filename}")
        print(
            f"üìä √ñzet: {len(summary_by_personnel)} personel, Detay: {len(detailed_rows)} kayƒ±t"
        )

        # Business event logla
        slogger.log_business_event(
            event_type="training_feedback_excel_export",
            description=(
                f"Training feedback data exported to Excel: {len(detailed_rows)} records, "
                f"{len(summary_by_personnel)} personnel"
            ),
            data={
                "filename": filename,
                "total_records": len(detailed_rows),
                "personnel_count": len(summary_by_personnel),
                "filters": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "personnel_id": personnel_id,
                },
            },
            user_id="system",
        )

        return Response(
            content=excel_buffer.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        import traceback

        print(f"‚ùå Training feedback Excel export hatasƒ±: {str(e)}")
        print(f"üîç Detaylƒ± hata: {traceback.format_exc()}")
        slogger.log_error(
            error_type="TrainingFeedbackExcelExportError",
            message=str(e),
            endpoint="/api/export/training-feedback-excel",
        )
        raise HTTPException(
            status_code=500, detail=f"Training feedback Excel export hatasƒ±: {str(e)}"
        )


# üß™ KATEGORI 9 - TESTING INFRASTRUCTURE ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞


class TestResult(BaseModel):
    """Test sonucu modeli"""

    test_name: str
    status: str  # "passed", "failed", "skipped"
    duration_ms: float
    message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    timestamp: str


class TestSuite(BaseModel):
    """Test suite modeli"""

    name: str
    tests: List[TestResult]
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    duration_ms: float
    success_rate: float
    timestamp: str


class TestManager:
    """Test y√∂netimi ve raporlama sistemi"""

    def __init__(self):
        self.test_results = []
        self.test_suites = {}
        self.mock_data_backup = None

    def create_test_data(self) -> Dict[str, List]:
        """Test i√ßin mock data olu≈ütur"""
        return {
            "personnel": [
                {
                    "id": 999,
                    "name": "Test User",
                    "department": "Test Dept",
                    "position": "Test Position",
                    "email": "test@test.com",
                    "phone": "555-TEST",
                    "hire_date": "2025-01-01",
                    "daily_targets": {
                        "uye_adedi": 10,
                        "whatsapp_adedi": 5,
                        "cihaz_adedi": 3,
                        "whatsapp_cevapsiz": 1,
                    },
                    "status": "active",
                }
            ],
            "daily_records": [
                {
                    "id": 999,
                    "date": "2025-08-06",
                    "personnel_id": 999,
                    "call_number": "TEST-001",
                    "score": 85,
                    "notes": "Test record",
                }
            ],
            "targets": [
                {
                    "id": 999,
                    "personnel_id": 999,
                    "target_type": "uye_adedi",
                    "target_value": 50,
                    "start_date": "2025-08-01",
                    "end_date": "2025-08-31",
                }
            ],
        }

    def backup_real_data(self):
        """Ger√ßek verileri yedekle"""
        global personnel_data, daily_records_data, targets_data
        self.mock_data_backup = {
            "personnel": copy.deepcopy(personnel_data),
            "daily_records": copy.deepcopy(daily_records_data),
            "targets": copy.deepcopy(targets_data),
        }

    def restore_real_data(self):
        """Ger√ßek verileri geri y√ºkle"""
        global personnel_data, daily_records_data, targets_data
        if self.mock_data_backup:
            personnel_data.clear()
            personnel_data.extend(self.mock_data_backup["personnel"])
            daily_records_data.clear()
            daily_records_data.extend(self.mock_data_backup["daily_records"])
            targets_data.clear()
            targets_data.extend(self.mock_data_backup["targets"])
            rebuild_all_indexes()

    def setup_test_environment(self):
        """Test ortamƒ±nƒ± hazƒ±rla"""
        self.backup_real_data()
        test_data = self.create_test_data()

        global personnel_data, daily_records_data, targets_data
        personnel_data.extend(test_data["personnel"])
        daily_records_data.extend(test_data["daily_records"])
        targets_data.extend(test_data["targets"])

        rebuild_all_indexes()

    def cleanup_test_environment(self):
        """Test ortamƒ±nƒ± temizle"""
        self.restore_real_data()

    async def run_endpoint_test(
        self,
        client,
        method: str,
        endpoint: str,
        data: Optional[Dict] = None,
        expected_status: int = 200,
        headers: Optional[Dict] = None,
    ) -> TestResult:
        """Tek endpoint test'i √ßalƒ±≈ütƒ±r"""
        start_time = time.time()
        test_name = f"{method} {endpoint}"

        try:
            if method.upper() == "GET":
                response = client.get(endpoint, headers=headers or {})
            elif method.upper() == "POST":
                response = client.post(endpoint, json=data or {}, headers=headers or {})
            elif method.upper() == "PUT":
                response = client.put(endpoint, json=data or {}, headers=headers or {})
            elif method.upper() == "DELETE":
                response = client.delete(endpoint, headers=headers or {})
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            duration = (time.time() - start_time) * 1000

            if response.status_code == expected_status:
                # Response formatƒ±nƒ± kontrol et
                if response.headers.get("content-type", "").startswith(
                    "application/json"
                ):
                    try:
                        json_data = response.json()
                        if isinstance(json_data, dict) and "success" in json_data:
                            return TestResult(
                                test_name=test_name,
                                status="passed",
                                duration_ms=round(duration, 2),
                                message=f"‚úÖ Status: {response.status_code}, Response format valid",
                                details={
                                    "status_code": response.status_code,
                                    "response_keys": list(json_data.keys()),
                                },
                                timestamp=datetime.now().isoformat(),
                            )
                        else:
                            return TestResult(
                                test_name=test_name,
                                status="failed",
                                duration_ms=round(duration, 2),
                                message=f"‚ùå Invalid response format - missing 'success' field",
                                details={
                                    "status_code": response.status_code,
                                    "response": str(json_data)[:200],
                                },
                                timestamp=datetime.now().isoformat(),
                            )
                    except Exception as e:
                        return TestResult(
                            test_name=test_name,
                            status="failed",
                            duration_ms=round(duration, 2),
                            message=f"‚ùå JSON parse error: {str(e)}",
                            details={"status_code": response.status_code},
                            timestamp=datetime.now().isoformat(),
                        )
                else:
                    # Non-JSON response (file, HTML etc.)
                    return TestResult(
                        test_name=test_name,
                        status="passed",
                        duration_ms=round(duration, 2),
                        message=f"‚úÖ Status: {response.status_code}, Non-JSON response",
                        details={
                            "status_code": response.status_code,
                            "content_type": response.headers.get("content-type"),
                        },
                        timestamp=datetime.now().isoformat(),
                    )
            else:
                return TestResult(
                    test_name=test_name,
                    status="failed",
                    duration_ms=round(duration, 2),
                    message=f"‚ùå Expected status {expected_status}, got {response.status_code}",
                    details={
                        "expected_status": expected_status,
                        "actual_status": response.status_code,
                        "response": response.text[:200],
                    },
                    timestamp=datetime.now().isoformat(),
                )

        except Exception as e:
            duration = (time.time() - start_time) * 1000
            return TestResult(
                test_name=test_name,
                status="failed",
                duration_ms=round(duration, 2),
                message=f"‚ùå Exception: {str(e)}",
                details={"error_type": type(e).__name__, "error_message": str(e)},
                timestamp=datetime.now().isoformat(),
            )

    async def run_data_validation_tests(self) -> List[TestResult]:
        """Veri doƒürulama testleri"""
        tests = []
        start_time = time.time()

        # Test 1: Data integrity
        try:
            integrity_result = sync_manager.validate_data_integrity()
            if integrity_result["is_valid"]:
                tests.append(
                    TestResult(
                        test_name="Data Integrity Check",
                        status="passed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message="‚úÖ Data integrity validation passed",
                        details=integrity_result,
                        timestamp=datetime.now().isoformat(),
                    )
                )
            else:
                tests.append(
                    TestResult(
                        test_name="Data Integrity Check",
                        status="failed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message=f"‚ùå Data integrity issues: {len(integrity_result['issues'])}",
                        details=integrity_result,
                        timestamp=datetime.now().isoformat(),
                    )
                )
        except Exception as e:
            tests.append(
                TestResult(
                    test_name="Data Integrity Check",
                    status="failed",
                    duration_ms=round((time.time() - start_time) * 1000, 2),
                    message=f"‚ùå Integrity check failed: {str(e)}",
                    details={"error": str(e)},
                    timestamp=datetime.now().isoformat(),
                )
            )

        # Test 2: Index consistency
        start_time = time.time()
        try:
            index_issues = []

            if len(personnel_index) != len(personnel_data):
                index_issues.append(
                    f"Personnel index mismatch: {len(personnel_index)} vs {len(personnel_data)}"
                )

            if len(daily_records_index) != len(daily_records_data):
                index_issues.append(
                    f"Daily records index mismatch: {len(daily_records_index)} vs {len(daily_records_data)}"
                )

            if len(targets_index) != len(targets_data):
                index_issues.append(
                    f"Targets index mismatch: {len(targets_index)} vs {len(targets_data)}"
                )

            if not index_issues:
                tests.append(
                    TestResult(
                        test_name="Index Consistency Check",
                        status="passed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message="‚úÖ All indexes are consistent",
                        details={
                            "personnel_index": len(personnel_index),
                            "daily_records_index": len(daily_records_index),
                            "targets_index": len(targets_index),
                        },
                        timestamp=datetime.now().isoformat(),
                    )
                )
            else:
                tests.append(
                    TestResult(
                        test_name="Index Consistency Check",
                        status="failed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message=f"‚ùå Index inconsistencies found",
                        details={"issues": index_issues},
                        timestamp=datetime.now().isoformat(),
                    )
                )
        except Exception as e:
            tests.append(
                TestResult(
                    test_name="Index Consistency Check",
                    status="failed",
                    duration_ms=round((time.time() - start_time) * 1000, 2),
                    message=f"‚ùå Index check failed: {str(e)}",
                    details={"error": str(e)},
                    timestamp=datetime.now().isoformat(),
                )
            )

        return tests

    async def run_performance_tests(self) -> List[TestResult]:
        """Performans testleri"""
        tests = []

        # Test 1: Response time test
        start_time = time.time()
        try:
            # Simulate multiple requests to test performance
            response_times = []
            for i in range(5):
                req_start = time.time()
                # Simulate a data operation
                _ = len(personnel_data) + len(daily_records_data)
                req_time = time.time() - req_start
                response_times.append(req_time * 1000)  # Convert to ms

            avg_response_time = sum(response_times) / len(response_times)
            max_response_time = max(response_times)

            if avg_response_time < 100:  # Less than 100ms average
                tests.append(
                    TestResult(
                        test_name="Average Response Time",
                        status="passed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message=f"‚úÖ Average response time: {avg_response_time:.2f}ms",
                        details={
                            "avg_ms": avg_response_time,
                            "max_ms": max_response_time,
                            "samples": len(response_times),
                        },
                        timestamp=datetime.now().isoformat(),
                    )
                )
            else:
                tests.append(
                    TestResult(
                        test_name="Average Response Time",
                        status="failed",
                        duration_ms=round((time.time() - start_time) * 1000, 2),
                        message=f"‚ùå Slow response time: {avg_response_time:.2f}ms",
                        details={
                            "avg_ms": avg_response_time,
                            "max_ms": max_response_time,
                            "threshold_ms": 100,
                        },
                        timestamp=datetime.now().isoformat(),
                    )
                )
        except Exception as e:
            tests.append(
                TestResult(
                    test_name="Average Response Time",
                    status="failed",
                    duration_ms=round((time.time() - start_time) * 1000, 2),
                    message=f"‚ùå Performance test failed: {str(e)}",
                    details={"error": str(e)},
                    timestamp=datetime.now().isoformat(),
                )
            )

        return tests

    def generate_test_report(
        self, suite_name: str, test_results: List[TestResult]
    ) -> TestSuite:
        """Test raporu olu≈ütur"""
        total_tests = len(test_results)
        passed_tests = sum(1 for t in test_results if t.status == "passed")
        failed_tests = sum(1 for t in test_results if t.status == "failed")
        skipped_tests = sum(1 for t in test_results if t.status == "skipped")

        total_duration = sum(t.duration_ms for t in test_results)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        return TestSuite(
            name=suite_name,
            tests=test_results,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            duration_ms=round(total_duration, 2),
            success_rate=round(success_rate, 2),
            timestamp=datetime.now().isoformat(),
        )


# Global test manager
test_manager = TestManager()


@app.get("/api/test/health", tags=["Testing"])
async def test_health_endpoint():
    """Test: Health endpoint functionality"""
    try:
        # Internal health check
        health_data = {
            "status": "testing",
            "test_mode": True,
            "timestamp": datetime.now().isoformat(),
            "basic_checks": {
                "data_loaded": len(personnel_data) > 0,
                "indexes_built": len(personnel_index) == len(personnel_data),
                "config_loaded": config_manager.settings.app_name
                == "Personel Takip API",
            },
        }

        return {
            "success": True,
            "data": health_data,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        slogger.log_error(
            error_type="TestHealthError",
            message=f"Test health check failed: {str(e)}",
            endpoint="/api/test/health",
        )
        raise HTTPException(
            status_code=500, detail=f"Test health check hatasƒ±: {str(e)}"
        )


@app.post("/api/test/run-suite", tags=["Testing"])
async def run_test_suite(
    suite_name: str = "full", user: dict = Depends(verify_admin_access)
):
    """Test suite'ini √ßalƒ±≈ütƒ±r - Admin yetkisi gerekli"""
    try:
        try:
            from fastapi.testclient import TestClient

            TEST_CLIENT_AVAILABLE = True
        except ImportError:
            TEST_CLIENT_AVAILABLE = False
            return {
                "success": False,
                "error": "TestClient not available - cannot run tests",
                "message": "Install fastapi[all] to enable testing features",
                "timestamp": datetime.now(),
            }

        slogger.log_business_event(
            event_type="test_suite_started",
            description=f"Test suite started: {suite_name}",
            data={"suite": suite_name, "user": user["name"]},
        )

        all_tests = []

        # Setup test environment
        test_manager.setup_test_environment()

        try:
            client = TestClient(app)

            if suite_name in ["full", "api"]:
                # API Endpoint Tests
                api_tests = [
                    ("GET", "/api/health", None, 200),
                    ("GET", "/api/personnel", None, 200),
                    ("GET", "/api/daily-records", None, 200),
                    ("GET", "/api/config/status", None, 200),
                    ("GET", "/api/config/summary", None, 200),
                    ("GET", "/api/analytics/summary", None, 200),
                ]

                for method, endpoint, data, expected_status in api_tests:
                    test_result = await test_manager.run_endpoint_test(
                        client, method, endpoint, data, expected_status
                    )
                    all_tests.append(test_result)

            if suite_name in ["full", "data"]:
                # Data Validation Tests
                data_tests = await test_manager.run_data_validation_tests()
                all_tests.extend(data_tests)

            if suite_name in ["full", "performance"]:
                # Performance Tests
                performance_tests = await test_manager.run_performance_tests()
                all_tests.extend(performance_tests)

        finally:
            # Cleanup test environment
            test_manager.cleanup_test_environment()

        # Generate report
        test_suite = test_manager.generate_test_report(suite_name, all_tests)

        # Store results
        test_manager.test_suites[suite_name] = test_suite

        slogger.log_business_event(
            event_type="test_suite_completed",
            description=f"Test suite completed: {suite_name}",
            data={
                "suite": suite_name,
                "total_tests": test_suite.total_tests,
                "passed": test_suite.passed_tests,
                "failed": test_suite.failed_tests,
                "success_rate": test_suite.success_rate,
                "duration_ms": test_suite.duration_ms,
                "user": user["name"],
            },
        )

        return {
            "success": True,
            "data": test_suite.dict(),
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="TestSuiteError",
            message=f"Test suite execution failed: {str(e)}",
            endpoint="/api/test/run-suite",
            user_id=user["name"],
        )
        raise HTTPException(status_code=500, detail=f"Test suite hatasƒ±: {str(e)}")


@app.get("/api/test/results", tags=["Testing"])
async def get_test_results():
    """Test sonu√ßlarƒ±nƒ± getir"""
    try:
        return {
            "success": True,
            "data": {
                "available_suites": list(test_manager.test_suites.keys()),
                "latest_results": {
                    name: suite.dict()
                    for name, suite in test_manager.test_suites.items()
                },
                "summary": {
                    "total_suites": len(test_manager.test_suites),
                    "last_run": max(
                        [suite.timestamp for suite in test_manager.test_suites.values()]
                    )
                    if test_manager.test_suites
                    else None,
                },
            },
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        slogger.log_error(
            error_type="TestResultsError",
            message=f"Failed to get test results: {str(e)}",
            endpoint="/api/test/results",
        )
        raise HTTPException(status_code=500, detail="Test sonu√ßlarƒ± alƒ±namadƒ±")


@app.get("/api/test/coverage", tags=["Testing"])
async def get_test_coverage():
    """Test kapsamƒ± analizi"""
    try:
        # Analyze test coverage
        total_endpoints = 0
        tested_endpoints = 0

        # Count total endpoints (approximate)
        api_endpoints = [
            "/api/personnel",
            "/api/daily-records",
            "/api/targets",
            "/api/analytics/summary",
            "/api/config/status",
            "/api/config/summary",
            "/api/health",
            "/api/monitoring/stats",
            "/api/sync/integrity-check",
            "/api/export/excel",
        ]

        total_endpoints = len(api_endpoints)

        # Check which endpoints have been tested
        tested_endpoints = 0
        if "full" in test_manager.test_suites or "api" in test_manager.test_suites:
            # Approximate based on common test patterns
            tested_endpoints = min(6, total_endpoints)  # API tests cover 6 endpoints

        coverage_percentage = (
            (tested_endpoints / total_endpoints * 100) if total_endpoints > 0 else 0
        )

        coverage_report = {
            "total_endpoints": total_endpoints,
            "tested_endpoints": tested_endpoints,
            "coverage_percentage": round(coverage_percentage, 2),
            "untested_endpoints": [ep for ep in api_endpoints[tested_endpoints:]],
            "test_categories": {
                "api_tests": "api" in test_manager.test_suites
                or "full" in test_manager.test_suites,
                "data_tests": "data" in test_manager.test_suites
                or "full" in test_manager.test_suites,
                "performance_tests": "performance" in test_manager.test_suites
                or "full" in test_manager.test_suites,
                "integration_tests": False,  # Not implemented yet
                "security_tests": False,  # Not implemented yet
            },
            "recommendations": [],
        }

        # Add recommendations
        if coverage_percentage < 80:
            coverage_report["recommendations"].append(
                "Test coverage is below 80%. Consider adding more endpoint tests."
            )

        if not coverage_report["test_categories"]["integration_tests"]:
            coverage_report["recommendations"].append(
                "Integration tests are not implemented. Consider adding end-to-end tests."
            )

        if not coverage_report["test_categories"]["security_tests"]:
            coverage_report["recommendations"].append(
                "Security tests are not implemented. Consider adding authentication and authorization tests."
            )

        return {
            "success": True,
            "data": coverage_report,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        slogger.log_error(
            error_type="TestCoverageError",
            message=f"Failed to analyze test coverage: {str(e)}",
            endpoint="/api/test/coverage",
        )
        raise HTTPException(status_code=500, detail="Test kapsamƒ± analizi ba≈üarƒ±sƒ±z")


# üìù KATEGORI 8 - CONFIGURATION MANAGEMENT ENDPOINTS


@app.get("/api/config/status", tags=["Configuration"])
async def get_configuration_status(user: dict = Depends(verify_admin_access)):
    """Get current configuration status and validation"""
    try:
        issues = config_manager.validate_config()

        return {
            "success": True,
            "data": {
                "environment": settings.environment,
                "version": settings.app_version,
                "debug_mode": config_manager.is_debug_mode(),
                "production_mode": config_manager.is_production(),
                "validation_issues": issues,
                "status": "healthy" if not issues else "needs_attention",
                "last_updated": datetime.now().isoformat(),
            },
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        slogger.log_error(
            error_type="ConfigurationStatusError",
            message=f"Failed to get configuration status: {str(e)}",
            endpoint="/api/config/status",
        )
        raise HTTPException(status_code=500, detail="Konfig√ºrasyon durumu alƒ±namadƒ±")


@app.get("/api/config/summary", tags=["Configuration"])
async def get_configuration_summary(user: dict = Depends(verify_admin_access)):
    """Get configuration summary for monitoring"""
    try:
        summary = config_manager.get_config_summary()

        return {
            "success": True,
            "data": summary,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        slogger.log_error(
            error_type="ConfigurationSummaryError",
            message=f"Failed to get configuration summary: {str(e)}",
            endpoint="/api/config/summary",
        )
        raise HTTPException(status_code=500, detail="Konfig√ºrasyon √∂zeti alƒ±namadƒ±")


@app.get("/api/config/health", tags=["Configuration"])
async def get_configuration_health(user: dict = Depends(verify_admin_access)):
    """Configuration saƒülƒ±k durumu"""
    try:
        # System health metrics
        performance_data = {}
        if PSUTIL_AVAILABLE:
            performance_data = {
                "memory_usage_percent": psutil.virtual_memory().percent,
                "cpu_usage_percent": psutil.cpu_percent(interval=1),
                "disk_usage_percent": psutil.disk_usage("/").percent
                if os.name != "nt"
                else psutil.disk_usage("C:").percent,
            }
        else:
            performance_data = {
                "memory_usage_percent": 0,  # Fallback
                "cpu_usage_percent": 0,  # Fallback
                "disk_usage_percent": 0,  # Fallback
            }

        health_data = {
            "overall_status": "healthy",
            "config_validation": config_manager.validate_config(),
            "environment_status": {
                "environment": config_manager.get_environment(),
                "debug_mode": config_manager.is_debug_mode(),
                "production_ready": config_manager.is_production(),
            },
            "performance": performance_data,
            "data_integrity": sync_manager.validate_data_integrity()["is_valid"],
            "services": {
                "rate_limiting": "active",
                "monitoring": "active",
                "data_sync": "active",
                "testing": "active",
            },
        }

        # Determine overall health
        issues = []
        if health_data["performance"]["memory_usage_percent"] > 90:
            issues.append("High memory usage")
        if health_data["performance"]["cpu_usage_percent"] > 80:
            issues.append("High CPU usage")
        if not health_data["data_integrity"]:
            issues.append("Data integrity issues")

        if issues:
            health_data["overall_status"] = (
                "warning" if len(issues) <= 2 else "critical"
            )
            health_data["issues"] = issues

        return {"success": True, "data": health_data, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå Configuration health error: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Configuration health error: {str(e)}"
        )


# üìö KATEGORI 10 - DOCUMENTATION & MAINTENANCE ƒ∞Yƒ∞LE≈ûTƒ∞RMESƒ∞


class DocumentationModule(BaseModel):
    """Dokumentasyon mod√ºl√º"""

    name: str
    description: str
    endpoints: List[str]
    examples: Optional[Dict[str, Any]] = None
    last_updated: str


class MaintenanceTask(BaseModel):
    """Bakƒ±m g√∂revi modeli"""

    task_id: str
    name: str
    description: str
    category: str  # "backup", "cleanup", "optimization", "monitoring"
    status: str  # "scheduled", "running", "completed", "failed"
    schedule: Optional[str] = None  # cron format
    last_run: Optional[str] = None
    next_run: Optional[str] = None
    duration_seconds: Optional[float] = None
    result: Optional[Dict[str, Any]] = None


class SystemStatus(BaseModel):
    """Sistem durum modeli"""

    uptime_seconds: float
    total_requests: int
    active_connections: int
    memory_usage: Dict[str, Any]
    disk_usage: Dict[str, Any]
    database_status: str
    last_backup: Optional[str] = None
    health_score: int  # 0-100


class DocumentationManager:
    """Dokumentasyon ve bakƒ±m y√∂netimi"""

    def __init__(self):
        self.modules = []
        self.maintenance_tasks = []
        self.system_start_time = time.time()
        self.request_counter = 0
        self.setup_documentation()
        self.setup_maintenance_tasks()

    def setup_documentation(self):
        """API dokumentasyonu yapƒ±landƒ±r"""
        self.modules = [
            DocumentationModule(
                name="Personnel Management",
                description="Personel y√∂netimi i√ßin CRUD operasyonlarƒ±",
                endpoints=["/api/personnel", "/api/personnel/{id}"],
                examples={
                    "create": {
                        "name": "Ahmet Yƒ±lmaz",
                        "department": "Satƒ±≈ü",
                        "position": "Satƒ±≈ü Temsilcisi",
                        "email": "ahmet@company.com",
                        "phone": "555-0123",
                    },
                    "update": {
                        "name": "Ahmet Yƒ±lmaz",
                        "department": "Satƒ±≈ü",
                        "position": "Kƒ±demli Satƒ±≈ü Temsilcisi",
                    },
                },
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="Daily Records",
                description="G√ºnl√ºk √ßaƒürƒ± puanlarƒ± kayƒ±t sistemi",
                endpoints=["/api/daily-records", "/api/daily-records/{id}"],
                examples={
                    "create": {
                        "personnel_id": 1,
                        "call_number": "CALL-001",
                        "score": 85,
                        "notes": "M√º≈üteri memnuniyeti y√ºksek",
                    }
                },
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="Performance Analytics",
                description="Performans analizi ve raporlama",
                endpoints=[
                    "/api/analytics/summary",
                    "/api/analytics/performance-trend",
                ],
                examples={"filter": {"date_filter": "2025-08-06", "personnel_id": 1}},
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="Excel Export",
                description="Verilerin Excel formatƒ±nda dƒ±≈üa aktarƒ±lmasƒ±",
                endpoints=["/api/export/excel"],
                examples={
                    "export": {"start_date": "2025-08-01", "end_date": "2025-08-31"}
                },
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="System Monitoring",
                description="Sistem izleme ve saƒülƒ±k kontrol√º",
                endpoints=["/api/monitoring/stats", "/api/monitoring/logs"],
                examples={"logs": {"limit": 50}},
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="Configuration Management",
                description="Sistem yapƒ±landƒ±rma y√∂netimi",
                endpoints=[
                    "/api/config/status",
                    "/api/config/health",
                    "/api/config/summary",
                ],
                examples={},
                last_updated=datetime.now().isoformat(),
            ),
            DocumentationModule(
                name="Testing Infrastructure",
                description="Otomatik test sistemi ve raporlama",
                endpoints=[
                    "/api/test/health",
                    "/api/test/run-suite",
                    "/api/test/results",
                ],
                examples={
                    "run_suite": {
                        "suite_name": "api"  # "api", "data", "performance", "full"
                    }
                },
                last_updated=datetime.now().isoformat(),
            ),
        ]

    def setup_maintenance_tasks(self):
        """Bakƒ±m g√∂revlerini yapƒ±landƒ±r"""
        self.maintenance_tasks = [
            MaintenanceTask(
                task_id="backup_daily",
                name="Daily Data Backup",
                description="G√ºnl√ºk veri yedekleme i≈ülemi",
                category="backup",
                status="scheduled",
                schedule="0 2 * * *",  # Her g√ºn saat 02:00
                last_run=None,
                next_run=self.calculate_next_run("0 2 * * *"),
            ),
            MaintenanceTask(
                task_id="cleanup_logs",
                name="Log Cleanup",
                description="Eski log dosyalarƒ±nƒ± temizleme",
                category="cleanup",
                status="scheduled",
                schedule="0 1 * * SUN",  # Her Pazar saat 01:00
                last_run=None,
                next_run=self.calculate_next_run("0 1 * * SUN"),
            ),
            MaintenanceTask(
                task_id="optimize_indexes",
                name="Index Optimization",
                description="Veri indekslerini optimize etme",
                category="optimization",
                status="scheduled",
                schedule="0 3 * * MON",  # Her Pazartesi saat 03:00
                last_run=None,
                next_run=self.calculate_next_run("0 3 * * MON"),
            ),
            MaintenanceTask(
                task_id="health_check",
                name="System Health Check",
                description="Sistem saƒülƒ±k kontrol√º",
                category="monitoring",
                status="scheduled",
                schedule="*/30 * * * *",  # Her 30 dakikada
                last_run=None,
                next_run=self.calculate_next_run("*/30 * * * *"),
            ),
            MaintenanceTask(
                task_id="performance_analysis",
                name="Performance Analysis",
                description="Sistem performans analizi",
                category="monitoring",
                status="scheduled",
                schedule="0 */6 * * *",  # Her 6 saatte
                last_run=None,
                next_run=self.calculate_next_run("0 */6 * * *"),
            ),
        ]

    def calculate_next_run(self, cron_expression: str) -> str:
        """Cron ifadesinden bir sonraki √ßalƒ±≈ütƒ±rma zamanƒ±nƒ± hesapla"""
        # Bu basit bir yakla≈üƒ±m - production'da croniter gibi bir lib kullanƒ±n
        now = datetime.now()

        if "*/30 * * * *" in cron_expression:  # Her 30 dakika
            next_run = now + timedelta(minutes=30)
        elif "0 */6 * * *" in cron_expression:  # Her 6 saat
            next_run = now + timedelta(hours=6)
        elif "0 2 * * *" in cron_expression:  # G√ºnl√ºk saat 02:00
            next_run = now.replace(hour=2, minute=0, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=1)
        elif "0 1 * * SUN" in cron_expression:  # Haftalƒ±k Pazar 01:00
            next_run = now + timedelta(days=7 - now.weekday())
            next_run = next_run.replace(hour=1, minute=0, second=0, microsecond=0)
        elif "0 3 * * MON" in cron_expression:  # Haftalƒ±k Pazartesi 03:00
            days_until_monday = (7 - now.weekday()) % 7
            if days_until_monday == 0 and now.hour >= 3:
                days_until_monday = 7
            next_run = now + timedelta(days=days_until_monday)
            next_run = next_run.replace(hour=3, minute=0, second=0, microsecond=0)
        else:
            next_run = now + timedelta(hours=24)

        return next_run.isoformat()

    def get_system_status(self) -> SystemStatus:
        """Sistem durumunu al"""
        uptime = time.time() - self.system_start_time

        # Memory usage
        if PSUTIL_AVAILABLE:
            memory = psutil.virtual_memory()
            memory_usage = {
                "total_mb": round(memory.total / 1024 / 1024, 2),
                "used_mb": round(memory.used / 1024 / 1024, 2),
                "available_mb": round(memory.available / 1024 / 1024, 2),
                "percent": round(memory.percent, 2),
            }

            # Disk usage
            disk = psutil.disk_usage("C:" if os.name == "nt" else "/")
            disk_usage = {
                "total_gb": round(disk.total / 1024 / 1024 / 1024, 2),
                "used_gb": round(disk.used / 1024 / 1024 / 1024, 2),
                "free_gb": round(disk.free / 1024 / 1024 / 1024, 2),
                "percent": round((disk.used / disk.total) * 100, 2),
            }
        else:
            # Fallback values when psutil is not available
            memory_usage = {
                "total_mb": 0,
                "used_mb": 0,
                "available_mb": 0,
                "percent": 0,
            }
            disk_usage = {"total_gb": 0, "used_gb": 0, "free_gb": 0, "percent": 0}

        # Health score calculation
        health_score = 100
        if memory_usage["percent"] > 80:
            health_score -= 20
        if disk_usage["percent"] > 80:
            health_score -= 15
        if not sync_manager.validate_data_integrity()["is_valid"]:
            health_score -= 25

        # Last backup info
        last_backup = None
        backup_files = sync_manager.list_backup_files()
        if backup_files:
            last_backup = max(backup_files, key=lambda x: x.get("created_at", ""))[
                "created_at"
            ]

        return SystemStatus(
            uptime_seconds=round(uptime, 2),
            total_requests=self.request_counter,
            active_connections=1,  # Simplified
            memory_usage=memory_usage,
            disk_usage=disk_usage,
            database_status="in_memory",
            last_backup=last_backup,
            health_score=max(0, health_score),
        )

    def run_maintenance_task(self, task_id: str) -> Dict[str, Any]:
        """Bakƒ±m g√∂revini √ßalƒ±≈ütƒ±r"""
        task = next((t for t in self.maintenance_tasks if t.task_id == task_id), None)
        if not task:
            raise ValueError(f"Task not found: {task_id}")

        start_time = time.time()
        task.status = "running"
        task.last_run = datetime.now().isoformat()

        result = {"success": False, "message": "", "details": {}}

        try:
            if task_id == "backup_daily":
                # Use the correct backup function from DataSyncManager
                backup_result = sync_manager.create_data_backup()
                result = {
                    "success": True,
                    "message": "Backup completed successfully",
                    "details": backup_result,
                }

            elif task_id == "cleanup_logs":
                # Log cleanup logic
                result = {
                    "success": True,
                    "message": "Log cleanup completed",
                    "details": {"logs_cleaned": 0},  # Simplified
                }

            elif task_id == "optimize_indexes":
                rebuild_all_indexes()
                result = {
                    "success": True,
                    "message": "Index optimization completed",
                    "details": {
                        "personnel_index_size": len(personnel_index),
                        "records_index_size": len(daily_records_index),
                        "targets_index_size": len(targets_index),
                    },
                }

            elif task_id == "health_check":
                health_status = self.get_system_status()
                result = {
                    "success": True,
                    "message": f"Health check completed - Score: {health_status.health_score}",
                    "details": health_status.dict(),
                }

            elif task_id == "performance_analysis":
                # Performance analysis logic
                result = {
                    "success": True,
                    "message": "Performance analysis completed",
                    "details": {
                        "avg_response_time_ms": 50,  # Simplified
                        "requests_per_minute": 100,
                        "error_rate_percent": 0.1,
                    },
                }

            task.status = "completed"

        except Exception as e:
            result = {
                "success": False,
                "message": f"Task failed: {str(e)}",
                "details": {"error": str(e)},
            }
            task.status = "failed"

        task.duration_seconds = round(time.time() - start_time, 2)
        task.result = result
        task.next_run = (
            self.calculate_next_run(task.schedule) if task.schedule else None
        )

        return result

    def generate_api_documentation(self) -> Dict[str, Any]:
        """API dokumentasyonu olu≈ütur"""
        return {
            "api_info": {
                "title": settings.app_name,
                "version": settings.app_version,
                "description": settings.description,
                "base_url": "/api",
                "authentication": "Bearer Token Required for Admin Operations",
            },
            "modules": [module.dict() for module in self.modules],
            "security": {
                "authentication_scheme": "HTTPBearer",
                "admin_endpoints": ADMIN_ENDPOINTS,
                "rate_limiting": {
                    "requests_per_minute": RATE_LIMIT_REQUESTS,
                    "window_seconds": RATE_LIMIT_WINDOW,
                },
            },
            "response_format": {
                "standard_response": {
                    "success": "boolean",
                    "data": "object/array",
                    "message": "string (optional)",
                    "timestamp": "ISO datetime",
                },
                "error_response": {"detail": "string", "status_code": "integer"},
            },
            "generated_at": datetime.now().isoformat(),
        }


# Global documentation manager
doc_manager = DocumentationManager()

# Update request counter if we had any before initialization
if "global_request_counter" in globals():
    doc_manager.request_counter = global_request_counter

# üìö DOCUMENTATION ENDPOINTS


@app.get("/api/docs/api", tags=["Documentation"])
async def get_api_documentation():
    """Complete API documentation"""
    try:
        documentation = doc_manager.generate_api_documentation()

        return {"success": True, "data": documentation, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå API documentation error: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"API documentation error: {str(e)}"
        )


@app.get("/api/docs/endpoints", tags=["Documentation"])
async def get_endpoint_documentation():
    """Endpoint listesi ve √∂rnekleri"""
    try:
        endpoints_info = {"total_endpoints": len(doc_manager.modules), "categories": {}}

        for module in doc_manager.modules:
            endpoints_info["categories"][module.name] = {
                "description": module.description,
                "endpoints": module.endpoints,
                "examples": module.examples,
                "last_updated": module.last_updated,
            }

        return {"success": True, "data": endpoints_info, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå Endpoint documentation error: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Endpoint documentation error: {str(e)}"
        )


@app.get("/api/docs/deployment", tags=["Documentation"])
async def get_deployment_guide():
    """Deployment guide ve kurulum talimatlarƒ±"""
    try:
        deployment_guide = {
            "requirements": {
                "python": "3.8+",
                "dependencies": [
                    "fastapi",
                    "uvicorn",
                    "pandas",
                    "xlsxwriter",
                    "pydantic-settings",
                    "python-dotenv",
                    "psutil",
                ],
            },
            "installation": {
                "steps": [
                    "1. Clone repository",
                    "2. Create virtual environment: python -m venv venv",
                    "3. Activate virtual environment: venv\\Scripts\\activate (Windows)",
                    "4. Install dependencies: pip install -r requirements.txt",
                    "5. Create .env file with configuration",
                    "6. Run server: python main.py",
                ]
            },
            "configuration": {
                "environment_variables": {
                    "APP_NAME": "Personel Takip API",
                    "APP_VERSION": "2.0.0",
                    "ENVIRONMENT": "development",
                    "DEBUG": "true",
                    "SERVER_HOST": "127.0.0.1",
                    "SERVER_PORT": "8002",
                }
            },
            "production_deployment": {
                "recommendations": [
                    "Use production WSGI server like Gunicorn",
                    "Set ENVIRONMENT=production",
                    "Disable debug mode",
                    "Use HTTPS",
                    "Set up proper logging",
                    "Configure monitoring",
                    "Use database instead of in-memory data",
                ]
            },
            "docker_deployment": {
                "dockerfile_example": """FROM python:3.9-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8002
CMD ["python", "main.py"]"""
            },
        }

        return {"success": True, "data": deployment_guide, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå Deployment guide error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Deployment guide error: {str(e)}")


# üîß MAINTENANCE ENDPOINTS


@app.get("/api/maintenance/status", tags=["Maintenance"])
async def get_system_status():
    """Sistem durum bilgisi"""
    try:
        status = doc_manager.get_system_status()

        return {"success": True, "data": status.dict(), "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå System status error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"System status error: {str(e)}")


@app.get("/api/maintenance/tasks", tags=["Maintenance"])
async def get_maintenance_tasks():
    """Bakƒ±m g√∂revleri listesi"""
    try:
        tasks_info = {
            "total_tasks": len(doc_manager.maintenance_tasks),
            "tasks": [task.dict() for task in doc_manager.maintenance_tasks],
            "categories": {
                "backup": [
                    t for t in doc_manager.maintenance_tasks if t.category == "backup"
                ],
                "cleanup": [
                    t for t in doc_manager.maintenance_tasks if t.category == "cleanup"
                ],
                "optimization": [
                    t
                    for t in doc_manager.maintenance_tasks
                    if t.category == "optimization"
                ],
                "monitoring": [
                    t
                    for t in doc_manager.maintenance_tasks
                    if t.category == "monitoring"
                ],
            },
        }

        return {"success": True, "data": tasks_info, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå Maintenance tasks error: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Maintenance tasks error: {str(e)}"
        )


@app.post("/api/maintenance/run-task/{task_id}", tags=["Maintenance"])
async def run_maintenance_task(task_id: str, user: dict = Depends(verify_admin_access)):
    """Bakƒ±m g√∂revini √ßalƒ±≈ütƒ±r - Admin yetkisi gerekli"""
    try:
        print(f"üîß Running maintenance task: {task_id} (User: {user['name']})")

        result = doc_manager.run_maintenance_task(task_id)

        return {
            "success": True,
            "data": result,
            "message": f"Maintenance task '{task_id}' completed",
            "timestamp": datetime.now(),
        }

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        print(f"‚ùå Maintenance task error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Maintenance task error: {str(e)}")


@app.get("/api/maintenance/health-report", tags=["Maintenance"])
async def get_health_report():
    """Kapsamlƒ± sistem saƒülƒ±k raporu"""
    try:
        system_status = doc_manager.get_system_status()

        # Additional health checks
        health_report = {
            "overall_health": "healthy"
            if system_status.health_score >= 80
            else "warning"
            if system_status.health_score >= 60
            else "critical",
            "system_status": system_status.dict(),
            "component_health": {
                "api_endpoints": "healthy",  # Could check endpoint response times
                "data_integrity": "healthy"
                if sync_manager.validate_data_integrity()["is_valid"]
                else "warning",
                "rate_limiting": "active",
                "monitoring": "active",
                "configuration": "healthy"
                if not config_manager.validate_config()
                else "warning",
            },
            "recommendations": [],
        }

        # Generate recommendations
        if system_status.memory_usage["percent"] > 80:
            health_report["recommendations"].append("Consider optimizing memory usage")

        if system_status.disk_usage["percent"] > 80:
            health_report["recommendations"].append("Clean up disk space")

        if system_status.health_score < 80:
            health_report["recommendations"].append("Review system health issues")

        if not system_status.last_backup:
            health_report["recommendations"].append("Create data backup")

        return {"success": True, "data": health_report, "timestamp": datetime.now()}

    except Exception as e:
        print(f"‚ùå Health report error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Health report error: {str(e)}")


if __name__ == "__main__":
    # Configuration validation on startup
    issues = config_manager.validate_config()
    if issues:
        print("‚ö†Ô∏è Configuration Issues Found:")
        for issue in issues:
            print(f"  - {issue}")
        print()

    # Show configuration summary
    summary = config_manager.get_config_summary()
    print("Configuration Summary:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    print()

    print("Starting FastAPI server...")
    uvicorn.run(
        app,
        host=settings.server.host,
        port=settings.server.port,
        reload=settings.server.reload and config_manager.is_debug_mode(),
        log_level=settings.server.log_level.lower(),
        workers=1 if config_manager.is_debug_mode() else settings.server.workers,
    )
